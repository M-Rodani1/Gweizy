{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u26a1 GPU Acceleration Setup\n",
    "\n",
    "### To Enable GPU in Google Colab:\n",
    "1. Click **Runtime** menu at the top\n",
    "2. Click **Change runtime type**\n",
    "3. Set **Hardware accelerator** to **GPU**\n",
    "4. Click **Save**\n",
    "5. The notebook will restart - run cells from the beginning\n",
    "\n",
    "### GPU Benefits:\n",
    "- **3-10x faster** model training\n",
    "- LightGBM with GPU acceleration\n",
    "- Faster feature engineering with CUDA\n",
    "- Better memory utilization for large datasets\n",
    "\n",
    "### What GPU Will You Get?\n",
    "- NVIDIA T4 (most common, 16GB VRAM)\n",
    "- NVIDIA A100 (premium tier, 40GB VRAM)\n",
    "- NVIDIA L4 (varies by availability)\n",
    "\n",
    "**Note:** Colab may limit GPU access during peak hours. Free tier gets occasional access.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gweizy Model Training Notebook\n",
    "\n",
    "Train all gas prediction models for Gweizy.\n",
    "\n",
    "## Instructions:\n",
    "1. Upload your `gas_data.db` file (from `backend/gas_data.db`)\n",
    "2. Run all cells\n",
    "3. Download the trained models zip file\n",
    "4. Extract to `backend/models/saved_models/` and push to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q scikit-learn pandas numpy joblib lightgbm xgboost matplotlib seaborn optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Configuration for Google Colab\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GPU ACCELERATION CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check CUDA availability\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"\\nCUDA Available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    GPU_AVAILABLE = True\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f  GPU NOT AVAILABLE\")\n",
    "    print(\"To enable GPU in Google Colab:\")\n",
    "    print(\"  1. Go to Runtime menu\")\n",
    "    print(\"  2. Click 'Change runtime type'\")\n",
    "    print(\"  3. Set 'Hardware accelerator' to 'GPU'\")\n",
    "    print(\"  4. Restart the notebook\")\n",
    "    GPU_AVAILABLE = False\n",
    "\n",
    "# TensorFlow GPU check\n",
    "tf_gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"\\nTensorFlow GPU devices: {len(tf_gpus)}\")\n",
    "if tf_gpus:\n",
    "    print(f\"  {tf_gpus[0]}\")\n",
    "\n",
    "# Set up GPU optimization flags for LightGBM and XGBoost\n",
    "GPU_PARAMS = {}\n",
    "if GPU_AVAILABLE:\n",
    "    GPU_PARAMS = {\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'device': 'gpu'\n",
    "    }\n",
    "    print(f\"\\n\u2713 GPU acceleration ENABLED for model training\")\n",
    "else:\n",
    "    GPU_PARAMS = {'device': 'cpu'}\n",
    "    print(f\"\\n\u2713 CPU mode (slower but will work)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Memory Optimization\n",
    "if GPU_AVAILABLE:\n",
    "    import torch\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # PyTorch GPU settings\n",
    "    torch.cuda.empty_cache()  # Clear any existing GPU memory\n",
    "    torch.backends.cudnn.benchmark = True  # Speed up training\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    \n",
    "    # TensorFlow GPU memory growth (prevent OOM)\n",
    "    for gpu in tf.config.list_physical_devices('GPU'):\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "    print(\"\u2713 GPU memory optimization enabled\")\n",
    "    print(f\"  - CUDA benchmarking: ON\")\n",
    "    print(f\"  - TensorFlow memory growth: ON\")\n",
    "    print(f\"  - GPU cache cleared\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your gas_data.db file\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"Upload your gas_data.db file from backend/gas_data.db\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "if 'gas_data.db' in uploaded:\n",
    "    print(f\"\\n\u2705 Uploaded gas_data.db ({len(uploaded['gas_data.db']) / 1024 / 1024:.1f} MB)\")\n",
    "else:\n",
    "    print(\"\u274c Please upload gas_data.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load data from database\n",
    "conn = sqlite3.connect('gas_data.db')\n",
    "df = pd.read_sql(\"\"\"\n",
    "    SELECT timestamp, current_gas as gas, base_fee, priority_fee, \n",
    "           block_number, gas_used, gas_limit, utilization\n",
    "    FROM gas_prices ORDER BY timestamp ASC\n",
    "\"\"\", conn)\n",
    "conn.close()\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df = df.set_index('timestamp').sort_index()\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "\n",
    "# === IMPROVED: Resample to 30-second intervals (was 1-min, losing too much data) ===\n",
    "print(\"\\nResampling to 30-second intervals (preserves more data)...\")\n",
    "df = df.resample('30s').mean().dropna(subset=['gas'])\n",
    "print(f\"After resample: {len(df):,} records\")\n",
    "\n",
    "# Find segments (gap > 30 min = new segment)\n",
    "df['time_diff'] = df.index.to_series().diff()\n",
    "df['segment'] = (df['time_diff'] > pd.Timedelta(minutes=30)).cumsum()\n",
    "\n",
    "segment_sizes = df.groupby('segment').size()\n",
    "print(f\"\\nSegments found: {len(segment_sizes)}\")\n",
    "print(f\"Segment sizes: {segment_sizes.sort_values(ascending=False).head(10).tolist()}\")\n",
    "\n",
    "# === IMPROVED: Lower threshold from 120 to 30 minutes (keeps more segments) ===\n",
    "MIN_SEGMENT_SIZE = 60  # 30 minutes at 30-sec intervals = 60 records\n",
    "good_segments = segment_sizes[segment_sizes >= MIN_SEGMENT_SIZE].index.tolist()\n",
    "df = df[df['segment'].isin(good_segments)]\n",
    "print(f\"\\nKeeping {len(good_segments)} segments with >= 30 minutes of data\")\n",
    "print(f\"Total usable records: {len(df):,}\")\n",
    "\n",
    "# === DATA SUFFICIENCY CHECK ===\n",
    "MIN_REQUIRED_SAMPLES = 10000\n",
    "if len(df) < MIN_REQUIRED_SAMPLES:\n",
    "    print(f\"\\n\u26a0\ufe0f  WARNING: Only {len(df):,} samples. Recommend at least {MIN_REQUIRED_SAMPLES:,}\")\n",
    "    print(\"   Models may underperform. Consider collecting more data.\")\n",
    "else:\n",
    "    print(f\"\\n\u2713 Data sufficiency check passed: {len(df):,} samples\")\n",
    "\n",
    "RECORDS_PER_HOUR = 120  # 30-sec intervals = 120 records per hour"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Fetch ETH Price Data - IMPROVED with Binance (1-minute data)\nimport requests\n\nprint(\"=\"*60)\nprint(\"FETCHING EXTERNAL DATA\")\nprint(\"=\"*60)\n\ndef fetch_eth_price_binance(start_date, end_date):\n    \"\"\"Fetch ETH price from Binance API (1-minute candles, much better than CoinGecko hourly)\"\"\"\n    try:\n        start_ts = int(start_date.timestamp() * 1000)\n        end_ts = int(end_date.timestamp() * 1000)\n        \n        all_prices = []\n        current_ts = start_ts\n        \n        print(f\"Fetching ETH prices from Binance (1-min candles)...\")\n        \n        while current_ts < end_ts:\n            url = \"https://api.binance.com/api/v3/klines\"\n            params = {\n                'symbol': 'ETHUSDT',\n                'interval': '1m',\n                'startTime': current_ts,\n                'endTime': min(current_ts + 1000 * 60 * 1000, end_ts),  # Max 1000 candles\n                'limit': 1000\n            }\n            \n            response = requests.get(url, params=params, timeout=30)\n            \n            if response.status_code == 200:\n                data = response.json()\n                if not data:\n                    break\n                    \n                for candle in data:\n                    all_prices.append({\n                        'timestamp': pd.to_datetime(candle[0], unit='ms'),\n                        'eth_price': float(candle[4]),  # Close price\n                        'eth_volume': float(candle[5]),  # Volume\n                        'eth_high': float(candle[2]),\n                        'eth_low': float(candle[3])\n                    })\n                \n                current_ts = data[-1][0] + 60000  # Next minute\n                \n                if len(all_prices) % 5000 == 0:\n                    print(f\"  Fetched {len(all_prices):,} candles...\")\n            else:\n                print(f\"  Binance API error: {response.status_code}\")\n                break\n        \n        if all_prices:\n            eth_df = pd.DataFrame(all_prices)\n            eth_df = eth_df.set_index('timestamp')\n            print(f\"  Total: {len(eth_df):,} 1-minute ETH candles\")\n            return eth_df\n        return None\n        \n    except Exception as e:\n        print(f\"  Failed to fetch from Binance: {e}\")\n        return None\n\ndef fetch_eth_price_coingecko(start_date, end_date):\n    \"\"\"Fallback: CoinGecko API (hourly data)\"\"\"\n    try:\n        start_ts = int(start_date.timestamp())\n        end_ts = int(end_date.timestamp())\n        \n        url = \"https://api.coingecko.com/api/v3/coins/ethereum/market_chart/range\"\n        params = {'vs_currency': 'usd', 'from': start_ts, 'to': end_ts}\n        \n        print(f\"Fallback: Fetching from CoinGecko (hourly)...\")\n        response = requests.get(url, params=params, timeout=30)\n        \n        if response.status_code == 200:\n            data = response.json()\n            prices = data.get('prices', [])\n            \n            eth_df = pd.DataFrame(prices, columns=['timestamp', 'eth_price'])\n            eth_df['timestamp'] = pd.to_datetime(eth_df['timestamp'], unit='ms')\n            eth_df = eth_df.set_index('timestamp')\n            eth_df['eth_volume'] = np.nan\n            eth_df['eth_high'] = eth_df['eth_price']\n            eth_df['eth_low'] = eth_df['eth_price']\n            \n            print(f\"  Fetched {len(eth_df)} hourly ETH prices\")\n            return eth_df\n        return None\n    except Exception as e:\n        print(f\"  CoinGecko failed: {e}\")\n        return None\n\n# Try Binance first, fallback to CoinGecko\neth_data = fetch_eth_price_binance(df.index.min(), df.index.max())\nif eth_data is None or len(eth_data) < 100:\n    eth_data = fetch_eth_price_coingecko(df.index.min(), df.index.max())\n\nhas_eth_data = False\nif eth_data is not None and len(eth_data) > 0:\n    # Resample to 30-second intervals\n    eth_data = eth_data.resample('30s').ffill()\n    \n    # Merge with gas data\n    df = df.join(eth_data, how='left')\n    df['eth_price'] = df['eth_price'].ffill().bfill()\n    \n    # Fill other ETH columns\n    for col in ['eth_volume', 'eth_high', 'eth_low']:\n        if col in df.columns:\n            df[col] = df[col].ffill().bfill()\n    \n    eth_coverage = df['eth_price'].notna().mean()\n    print(f\"  ETH price coverage: {eth_coverage:.1%}\")\n    \n    if eth_coverage > 0.5:\n        has_eth_data = True\n        print(\"  \u2713 ETH price data integrated (1-min resolution)\")\nelse:\n    print(\"  \u26a0\ufe0f No ETH price data available\")\n    df['eth_price'] = np.nan\n    df['eth_volume'] = np.nan\n    df['eth_high'] = np.nan\n    df['eth_low'] = np.nan\n\nHAS_ETH_PRICE = has_eth_data"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - SIMPLIFIED v3 + SPIKE-ADJUSTED TARGETS\n",
    "# Focus: 15-20 high-value features to prevent overfitting\n",
    "# NEW: Option for log-transformed or winsorized targets\n",
    "\n",
    "print(\"Engineering SIMPLIFIED feature set (15-20 features)...\")\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "TARGET_TRANSFORM = \"log\"  # Options: \"none\", \"log\", \"winsorize\"\n",
    "WINSORIZE_PERCENTILE = 0.95  # For winsorize: cap at this percentile\n",
    "\n",
    "def engineer_features_for_segment(seg_df, has_eth=False, horizon='all'):\n",
    "    \"\"\"Engineer focused feature set - quality over quantity\"\"\"\n",
    "    df = seg_df.copy()\n",
    "    rph = 120  # records per hour (30-sec intervals)\n",
    "    \n",
    "    # === TIME FEATURES (3 features) ===\n",
    "    df['hour'] = df.index.hour\n",
    "    hour_of_day = df.index.hour + df.index.minute / 60\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * hour_of_day / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * hour_of_day / 24)\n",
    "    \n",
    "    # === ETH FEATURES (2 features) ===\n",
    "    if has_eth and 'eth_price' in df.columns and df['eth_price'].notna().any():\n",
    "        df['eth_log'] = np.log1p(df['eth_price'])\n",
    "        eth_mean = df['eth_price'].rolling(4*rph, min_periods=rph).mean()\n",
    "        eth_std = df['eth_price'].rolling(4*rph, min_periods=rph).std()\n",
    "        df['eth_zscore_4h'] = np.where(eth_std > 0.01, (df['eth_price'] - eth_mean) / eth_std, 0)\n",
    "        df['gas_eth_corr_1h'] = df['gas'].rolling(rph, min_periods=rph//2).corr(df['eth_price']).fillna(0)\n",
    "    \n",
    "    # === NETWORK UTILIZATION (2 features) ===\n",
    "    if 'utilization' in df.columns:\n",
    "        df['util_mean_1h'] = df['utilization'].rolling(rph, min_periods=rph//2).mean()\n",
    "        df['util_mean_2h'] = df['utilization'].rolling(2*rph, min_periods=rph).mean()\n",
    "    \n",
    "    # === GAS LAG FEATURES (7 features) - includes very short-term for 1h model ===\n",
    "    df['gas_lag_1min'] = df['gas'].shift(2)   # Very short-term for 1h model\n",
    "    df['gas_lag_2min'] = df['gas'].shift(4)   # Very short-term for 1h model\n",
    "    df['gas_lag_5min'] = df['gas'].shift(10)\n",
    "    df['gas_lag_15min'] = df['gas'].shift(30)\n",
    "    df['gas_lag_30min'] = df['gas'].shift(60)\n",
    "    df['gas_lag_1h'] = df['gas'].shift(rph)\n",
    "    df['gas_lag_4h'] = df['gas'].shift(4*rph)\n",
    "\n",
    "    # === NIGHT FEATURE (for learning night patterns) ===\n",
    "    df['is_night'] = ((df.index.hour >= 0) & (df.index.hour < 6)).astype(int)\n",
    "    \n",
    "    # === ROLLING STATS (6 features) ===\n",
    "    df['gas_mean_1h'] = df['gas'].rolling(rph, min_periods=rph//2).mean()\n",
    "    df['gas_std_1h'] = df['gas'].rolling(rph, min_periods=rph//2).std()\n",
    "    df['gas_cv_1h'] = np.where(df['gas_mean_1h'] > 0.01, \n",
    "                                df['gas_std_1h'] / df['gas_mean_1h'], 0)\n",
    "    df['gas_mean_2h'] = df['gas'].rolling(2*rph, min_periods=rph).mean()\n",
    "    df['gas_mean_4h'] = df['gas'].rolling(4*rph, min_periods=rph).mean()\n",
    "    \n",
    "    # === MOMENTUM (3 features) ===\n",
    "    df['momentum_1h'] = df['gas'] - df['gas'].shift(rph)\n",
    "    shift_2h = df['gas'].shift(2*rph)\n",
    "    df['momentum_pct_2h'] = np.where(shift_2h > 0.01, (df['gas'] - shift_2h) / shift_2h, 0)\n",
    "    df['trend_1h_4h'] = np.where(df['gas_mean_4h'] > 0.01, df['gas_mean_1h'] / df['gas_mean_4h'], 1.0)\n",
    "    \n",
    "    # === EXTENDED MOMENTUM (7 features) ===\n",
    "    df['momentum_4h'] = df['gas'] - df['gas'].shift(4*rph)\n",
    "    df['momentum_24h'] = df['gas'] - df['gas'].shift(24*rph)\n",
    "    df['momentum_accel_1h'] = df['momentum_1h'] - df['momentum_1h'].shift(rph)\n",
    "    df['momentum_accel_4h'] = df['momentum_4h'] - df['momentum_4h'].shift(4*rph)\n",
    "    \n",
    "    # === ENHANCED VOLATILITY REGIME (5 features) ===\n",
    "    rolling_std = df['gas'].rolling(rph, min_periods=rph//2).std()\n",
    "    rolling_std_4h = df['gas'].rolling(4*rph, min_periods=rph).std()\n",
    "    rolling_std_24h = df['gas'].rolling(24*rph, min_periods=rph).std()\n",
    "    df['vol_regime_stable'] = (rolling_std < rolling_std.quantile(0.33)).astype(int)\n",
    "    df['vol_regime_increasing'] = (rolling_std > rolling_std.shift(rph)).astype(int)\n",
    "    df['vol_ratio_1h_4h'] = np.where(rolling_std_4h > 0.001, rolling_std / rolling_std_4h, 1.0)\n",
    "    df['is_high_volatility'] = (df['gas_cv_1h'] > df['gas_cv_1h'].quantile(0.75)).astype(int)\n",
    "    df['vol_regime_label'] = pd.cut(rolling_std_24h, bins=3, labels=[0, 1, 2], duplicates='drop')\n",
    "    \n",
    "    # === Z-SCORE AND REGIME (3 features) ===\n",
    "    df['gas_zscore_1h'] = np.where(df['gas_std_1h'] > 0.001, \n",
    "        (df['gas'] - df['gas_mean_1h']) / df['gas_std_1h'], 0)\n",
    "    df['is_spike'] = (df['gas'] > df['gas_mean_1h'] + 2 * df['gas_std_1h']).astype(int)\n",
    "    df['is_high_gas'] = (df['gas'] > df['gas'].rolling(4*rph, min_periods=rph).quantile(0.9)).astype(int)\n",
    "    \n",
    "    \n",
    "    # === TIME INTERACTIONS (6 features) ===\n",
    "    df['hour_x_momentum'] = df['hour'] * df['momentum_1h']\n",
    "    df['hour_x_volatility'] = df['hour'] * df['gas_std_1h']\n",
    "    df['is_night_x_gas_level'] = df['is_night'] * df['gas']\n",
    "    df['is_night_x_volatility'] = df['is_night'] * df['gas_cv_1h']\n",
    "    df['weekend_hour_interact'] = (df.index.dayofweek >= 5).astype(int) * df['hour'] if hasattr(df.index, 'dayofweek') else 0\n",
    "    df['peak_hour_volatility'] = ((df['hour'] >= 8) & (df['hour'] <= 20)).astype(int) * df['gas_std_1h']\n",
    "    \n",
    "    # === MEAN REVERSION (5 features) ===\n",
    "    df['dist_from_mean_1h'] = df['gas'] - df['gas_mean_1h']\n",
    "    df['dist_from_mean_4h'] = df['gas'] - df['gas_mean_4h']\n",
    "    df['dist_pct_1h'] = np.where(df['gas_mean_1h'] > 0.01, df['dist_from_mean_1h'] / df['gas_mean_1h'], 0)\n",
    "    df['reversion_pressure'] = df['gas_zscore_1h'] * df['gas_cv_1h']\n",
    "    df['mean_reversion_signal'] = np.where(\n",
    "        np.abs(df['gas_zscore_1h']) > 1.5,\n",
    "        -np.sign(df['gas_zscore_1h']) * df['momentum_1h'],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "\n",
    "    # External data hooks\n",
    "    if hasattr(df.index, 'dayofweek'):\n",
    "        df['day_of_week'] = df.index.dayofweek\n",
    "        df['is_weekend'] = (df.index.dayofweek >= 5).astype(int)\n",
    "    if 'blob_gas_price' in df.columns:\n",
    "        df['blob_gas_log'] = np.log1p(df['blob_gas_price'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Process each segment\n",
    "print(\"\\nProcessing segments...\")\n",
    "segments = df['segment'].unique()\n",
    "processed_segments = []\n",
    "\n",
    "for seg_id in segments:\n",
    "    seg_df = df[df['segment'] == seg_id].copy()\n",
    "    processed = engineer_features_for_segment(seg_df, has_eth=has_eth_data, horizon='all')\n",
    "    processed_segments.append(processed)\n",
    "\n",
    "df_features = pd.concat(processed_segments, axis=0)\n",
    "print(f\"After feature engineering: {len(df_features):,} records\")\n",
    "\n",
    "# Create targets\n",
    "print(\"\\nCreating prediction targets...\")\n",
    "\n",
    "def create_targets_for_segment(seg_df, transform=\"none\", winsorize_pct=0.95):\n",
    "    \"\"\"Create target variables with optional transformation\"\"\"\n",
    "    df = seg_df.copy()\n",
    "    rph = 120\n",
    "    \n",
    "    # Raw future prices\n",
    "    raw_1h = df['gas'].shift(-rph)\n",
    "    raw_4h = df['gas'].shift(-4*rph)\n",
    "    raw_24h = df['gas'].shift(-24*rph)\n",
    "    \n",
    "    # Apply transformation\n",
    "    if transform == \"log\":\n",
    "        # Log transform - better for multiplicative changes\n",
    "        df['target_1h'] = np.log1p(raw_1h)\n",
    "        df['target_4h'] = np.log1p(raw_4h)\n",
    "        df['target_24h'] = np.log1p(raw_24h)\n",
    "        # Also store raw for evaluation\n",
    "        df['target_1h_raw'] = raw_1h\n",
    "        df['target_4h_raw'] = raw_4h\n",
    "        df['target_24h_raw'] = raw_24h\n",
    "    elif transform == \"winsorize\":\n",
    "        # Winsorize - cap extreme values\n",
    "        cap_1h = raw_1h.quantile(winsorize_pct)\n",
    "        cap_4h = raw_4h.quantile(winsorize_pct)\n",
    "        cap_24h = raw_24h.quantile(winsorize_pct) if raw_24h.notna().sum() > 100 else cap_4h\n",
    "        df['target_1h'] = raw_1h.clip(upper=cap_1h)\n",
    "        df['target_4h'] = raw_4h.clip(upper=cap_4h)\n",
    "        df['target_24h'] = raw_24h.clip(upper=cap_24h)\n",
    "        df['target_1h_raw'] = raw_1h\n",
    "        df['target_4h_raw'] = raw_4h\n",
    "        df['target_24h_raw'] = raw_24h\n",
    "        print(f\"  Winsorized caps: 1h={cap_1h:.2f}, 4h={cap_4h:.2f}\")\n",
    "    else:\n",
    "        # No transform\n",
    "        df['target_1h'] = raw_1h\n",
    "        df['target_4h'] = raw_4h\n",
    "        df['target_24h'] = raw_24h\n",
    "    \n",
    "    # Direction classification (always on raw)\n",
    "    threshold = 0.02\n",
    "    for horizon in ['1h', '4h']:\n",
    "        raw_target = raw_1h if horizon == '1h' else raw_4h\n",
    "        pct_change = np.where(df['gas'] > 0.001, \n",
    "            (raw_target - df['gas']) / df['gas'], 0)\n",
    "        df[f'direction_class_{horizon}'] = pd.cut(\n",
    "            pct_change,\n",
    "            bins=[-float('inf'), -threshold, threshold, float('inf')],\n",
    "            labels=['down', 'stable', 'up']\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(f\"Target transform: {TARGET_TRANSFORM}\")\n",
    "processed_with_targets = []\n",
    "for seg_id in df_features['segment'].unique():\n",
    "    seg_df = df_features[df_features['segment'] == seg_id].copy()\n",
    "    processed = create_targets_for_segment(seg_df, transform=TARGET_TRANSFORM, winsorize_pct=WINSORIZE_PERCENTILE)\n",
    "    processed_with_targets.append(processed)\n",
    "\n",
    "df_features = pd.concat(processed_with_targets, axis=0)\n",
    "\n",
    "# Store transform info for later use\n",
    "TARGET_TRANSFORM_USED = TARGET_TRANSFORM\n",
    "\n",
    "# === CLEAN INF/NAN VALUES ===\n",
    "print(\"\\nCleaning inf/nan values...\")\n",
    "numeric_cols = df_features.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df_features[col] = df_features[col].replace([np.inf, -np.inf], np.nan)\n",
    "    if df_features[col].notna().sum() > 0:\n",
    "        q_low = df_features[col].quantile(0.001)\n",
    "        q_high = df_features[col].quantile(0.999)\n",
    "        df_features[col] = df_features[col].clip(q_low, q_high)\n",
    "\n",
    "df_features = df_features.ffill().bfill()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if df_features[col].isna().any():\n",
    "        median_val = df_features[col].median()\n",
    "        if pd.isna(median_val):\n",
    "            median_val = 0\n",
    "        df_features[col] = df_features[col].fillna(median_val)\n",
    "\n",
    "inf_count = np.isinf(df_features.select_dtypes(include=[np.number])).sum().sum()\n",
    "nan_count = df_features.select_dtypes(include=[np.number]).isna().sum().sum()\n",
    "print(f\"  After cleaning: {inf_count} inf, {nan_count} nan values\")\n",
    "\n",
    "# === DEFINE FOCUSED FEATURE SET ===\n",
    "CORE_FEATURES = [\n",
    "    'hour', 'hour_sin', 'hour_cos',\n",
    "    'eth_log', 'eth_zscore_4h', 'gas_eth_corr_1h',\n",
    "    'util_mean_1h', 'util_mean_2h',\n",
    "    'gas_lag_1min', 'gas_lag_2min', 'gas_lag_5min', 'gas_lag_15min', 'gas_lag_30min', 'gas_lag_1h', 'gas_lag_4h',\n",
    "    'is_night',\n",
    "    'gas_mean_1h', 'gas_std_1h', 'gas_cv_1h', 'gas_mean_2h', 'gas_mean_4h',\n",
    "    'momentum_1h', 'momentum_4h', 'momentum_24h', 'momentum_pct_2h', 'momentum_accel_1h', 'momentum_accel_4h', 'trend_1h_4h',\n",
    "    'vol_regime_stable', 'vol_regime_increasing', 'vol_ratio_1h_4h', 'is_high_volatility', 'vol_regime_label',\n",
    "    'hour_x_momentum', 'hour_x_volatility', 'is_night_x_gas_level', 'is_night_x_volatility', 'weekend_hour_interact', 'peak_hour_volatility',\n",
    "    'dist_from_mean_1h', 'dist_from_mean_4h', 'dist_pct_1h', 'reversion_pressure', 'mean_reversion_signal',\n",
    "    'gas_zscore_1h', 'is_spike', 'is_high_gas'\n",
    "]\n",
    "\n",
    "available_features = [f for f in CORE_FEATURES if f in df_features.columns]\n",
    "features_1h = available_features\n",
    "features_4h = available_features  \n",
    "features_24h = available_features\n",
    "\n",
    "print(f\"\\n\u2713 Focused feature set: {len(available_features)} features\")\n",
    "print(f\"  Features: {', '.join(available_features)}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data - with AUTO-ADAPT to distribution shift\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy import stats\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "USE_ROLLING_WINDOW = False  # Set True to use only recent data (AUTO-ENABLED if shift detected)\n",
    "ROLLING_WINDOW_DAYS = 7     # Days of data to use if rolling window enabled\n",
    "HOLDOUT_HOURS = 48          # Hours to reserve for holdout\n",
    "AUTO_ADAPT_ON_SHIFT = True  # Automatically adapt when distribution shift detected\n",
    "\n",
    "# === VOLATILITY ADAPTATION SETTINGS ===\n",
    "# Shorter windows for more aggressive adaptation during volatile periods\n",
    "SEVERE_SHIFT_WINDOW_DAYS = 1.5    # Very aggressive - 36 hours\n",
    "HIGH_SHIFT_WINDOW_DAYS = 2        # Aggressive - 2 days  \n",
    "MODERATE_SHIFT_WINDOW_DAYS = 3    # Moderate - 3 days\n",
    "MILD_SHIFT_WINDOW_DAYS = 5        # Mild - 5 days\n",
    "\n",
    "# Only keep numeric columns\n",
    "numeric_features_1h = df_features[features_1h].select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_features_4h = df_features[features_4h].select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_features_24h = df_features[features_24h].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: 1h={len(numeric_features_1h)}, 4h={len(numeric_features_4h)}, 24h={len(numeric_features_24h)}\")\n",
    "\n",
    "# Drop rows only where TARGET columns are NaN\n",
    "target_cols = ['target_1h', 'target_4h']\n",
    "df_clean = df_features.dropna(subset=target_cols)\n",
    "print(f\"Clean samples (with valid 1h/4h targets): {len(df_clean):,}\")\n",
    "\n",
    "valid_24h = df_features['target_24h'].notna().sum()\n",
    "print(f\"Samples with valid 24h target: {valid_24h:,}\")\n",
    "\n",
    "# === OUT-OF-TIME HOLDOUT (do this FIRST to detect shift) ===\n",
    "rph = 120  # records per hour\n",
    "holdout_size = HOLDOUT_HOURS * rph\n",
    "\n",
    "if len(df_clean) > holdout_size + 5000:\n",
    "    df_train_val_initial = df_clean.iloc[:-holdout_size]\n",
    "    df_holdout = df_clean.iloc[-holdout_size:]\n",
    "    print(f\"\\n\u2713 Out-of-time holdout: {len(df_holdout):,} samples (last {HOLDOUT_HOURS}h)\")\n",
    "    HAS_HOLDOUT = True\n",
    "else:\n",
    "    df_train_val_initial = df_clean\n",
    "    df_holdout = None\n",
    "    print(f\"\\n\u26a0\ufe0f Not enough data for holdout, using all for training\")\n",
    "    HAS_HOLDOUT = False\n",
    "\n",
    "# === DISTRIBUTION SHIFT DETECTION ===\n",
    "def detect_distribution_shift(train_data, holdout_data, name=\"\"):\n",
    "    \"\"\"Detect distribution shift between train and holdout\"\"\"\n",
    "    results = {'name': name, 'warnings': [], 'passed': True, 'shift_magnitude': 0}\n",
    "    \n",
    "    train_mean, train_std = train_data.mean(), train_data.std()\n",
    "    holdout_mean = holdout_data.mean()\n",
    "    mean_shift = abs(holdout_mean - train_mean) / (train_std + 1e-8)\n",
    "    results['mean_shift_std'] = mean_shift\n",
    "    \n",
    "    if mean_shift > 1.0:\n",
    "        results['warnings'].append(f\"Large mean shift: {mean_shift:.2f} std devs\")\n",
    "        results['passed'] = False\n",
    "        results['shift_magnitude'] = max(results['shift_magnitude'], mean_shift)\n",
    "    elif mean_shift > 0.5:\n",
    "        results['warnings'].append(f\"Moderate mean shift: {mean_shift:.2f} std devs\")\n",
    "    \n",
    "    var_ratio = holdout_data.var() / (train_data.var() + 1e-8)\n",
    "    results['var_ratio'] = var_ratio\n",
    "    \n",
    "    if var_ratio > 4 or var_ratio < 0.25:\n",
    "        results['warnings'].append(f\"Large variance change: {var_ratio:.2f}x\")\n",
    "        results['passed'] = False\n",
    "        results['shift_magnitude'] = max(results['shift_magnitude'], abs(np.log(var_ratio)))\n",
    "    \n",
    "    ks_stat, ks_pval = stats.ks_2samp(train_data.values[:5000], holdout_data.values[:5000])\n",
    "    results['ks_statistic'] = ks_stat\n",
    "    results['ks_pvalue'] = ks_pval\n",
    "    \n",
    "    if ks_pval < 0.001 and ks_stat > 0.3:\n",
    "        results['warnings'].append(f\"KS test: distributions differ significantly\")\n",
    "        results['passed'] = False\n",
    "        results['shift_magnitude'] = max(results['shift_magnitude'], ks_stat * 3)\n",
    "    \n",
    "    train_spikes = (train_data > train_data.quantile(0.95)).mean()\n",
    "    holdout_spikes = (holdout_data > train_data.quantile(0.95)).mean()\n",
    "    spike_ratio = holdout_spikes / (train_spikes + 1e-8)\n",
    "    results['spike_ratio'] = spike_ratio\n",
    "    \n",
    "    if spike_ratio > 3:\n",
    "        results['warnings'].append(f\"Spike frequency {spike_ratio:.1f}x higher in holdout\")\n",
    "        results['passed'] = False\n",
    "        results['shift_magnitude'] = max(results['shift_magnitude'], spike_ratio / 2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "DISTRIBUTION_SHIFT_DETECTED = False\n",
    "SHIFT_MAGNITUDE = 0\n",
    "\n",
    "if HAS_HOLDOUT:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DISTRIBUTION SHIFT DETECTION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for horizon in ['1h', '4h']:\n",
    "        target_col = f'target_{horizon}'\n",
    "        train_targets = df_train_val_initial[target_col].dropna()\n",
    "        holdout_targets = df_holdout[target_col].dropna()\n",
    "        \n",
    "        shift_result = detect_distribution_shift(train_targets, holdout_targets, f\"{horizon} target\")\n",
    "        \n",
    "        status = \"\u2713 OK\" if shift_result['passed'] else \"\u26a0\ufe0f SHIFT DETECTED\"\n",
    "        print(f\"\\n{horizon}: {status}\")\n",
    "        print(f\"  Train:   mean={train_targets.mean():.4f}, std={train_targets.std():.4f}\")\n",
    "        print(f\"  Holdout: mean={holdout_targets.mean():.4f}, std={holdout_targets.std():.4f}\")\n",
    "        print(f\"  Mean shift: {shift_result['mean_shift_std']:.2f} std, Var ratio: {shift_result['var_ratio']:.2f}x\")\n",
    "        \n",
    "        if shift_result['warnings']:\n",
    "            for w in shift_result['warnings']:\n",
    "                print(f\"  \u26a0\ufe0f {w}\")\n",
    "            DISTRIBUTION_SHIFT_DETECTED = True\n",
    "            SHIFT_MAGNITUDE = max(SHIFT_MAGNITUDE, shift_result['shift_magnitude'])\n",
    "\n",
    "# === AUTO-ADAPT TO DISTRIBUTION SHIFT (SHORTER WINDOWS) ===\n",
    "if DISTRIBUTION_SHIFT_DETECTED and AUTO_ADAPT_ON_SHIFT:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"AUTO-ADAPTING TO DISTRIBUTION SHIFT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # More aggressive adaptive window based on shift magnitude\n",
    "    if SHIFT_MAGNITUDE > 4:\n",
    "        adaptive_days = SEVERE_SHIFT_WINDOW_DAYS  # 1.5 days (very aggressive)\n",
    "        volatility_level = \"SEVERE\"\n",
    "    elif SHIFT_MAGNITUDE > 2:\n",
    "        adaptive_days = HIGH_SHIFT_WINDOW_DAYS    # 2 days (aggressive)\n",
    "        volatility_level = \"HIGH\"\n",
    "    elif SHIFT_MAGNITUDE > 1:\n",
    "        adaptive_days = MODERATE_SHIFT_WINDOW_DAYS  # 3 days\n",
    "        volatility_level = \"MODERATE\"\n",
    "    else:\n",
    "        adaptive_days = MILD_SHIFT_WINDOW_DAYS    # 5 days\n",
    "        volatility_level = \"MILD\"\n",
    "    \n",
    "    window_samples = int(adaptive_days * 24 * rph)\n",
    "    \n",
    "    if len(df_train_val_initial) > window_samples:\n",
    "        df_train_val = df_train_val_initial.iloc[-window_samples:]\n",
    "        print(f\"\u2713 Auto-enabled rolling window: {adaptive_days} days ({len(df_train_val):,} samples)\")\n",
    "        print(f\"  Volatility: {volatility_level} (shift magnitude: {SHIFT_MAGNITUDE:.2f})\")\n",
    "        print(f\"  Using most recent {adaptive_days} days for training\")\n",
    "        USE_ROLLING_WINDOW = True\n",
    "        ROLLING_WINDOW_DAYS = adaptive_days\n",
    "    else:\n",
    "        df_train_val = df_train_val_initial\n",
    "        print(f\"\u26a0\ufe0f Not enough data for adaptive window, using all training data\")\n",
    "elif USE_ROLLING_WINDOW:\n",
    "    # Manual rolling window\n",
    "    window_samples = int(ROLLING_WINDOW_DAYS * 24 * rph)\n",
    "    if len(df_train_val_initial) > window_samples:\n",
    "        df_train_val = df_train_val_initial.iloc[-window_samples:]\n",
    "        print(f\"\\n\u2713 Rolling window: Using last {ROLLING_WINDOW_DAYS} days ({len(df_train_val):,} samples)\")\n",
    "    else:\n",
    "        df_train_val = df_train_val_initial\n",
    "else:\n",
    "    df_train_val = df_train_val_initial\n",
    "\n",
    "print(f\"\\nFinal training set: {len(df_train_val):,} samples\")\n",
    "\n",
    "# Final safety check\n",
    "for col in df_train_val.select_dtypes(include=[np.float64, np.float32, float]).columns:\n",
    "    df_train_val[col] = df_train_val[col].replace([np.inf, -np.inf], np.nan)\n",
    "    if df_train_val[col].isna().any():\n",
    "        df_train_val[col] = df_train_val[col].fillna(df_train_val[col].median())\n",
    "\n",
    "float_cols = df_train_val.select_dtypes(include=[np.float64, np.float32, float]).columns\n",
    "has_inf = any(np.isinf(df_train_val[col]).any() for col in float_cols)\n",
    "has_nan = any(np.isnan(df_train_val[col]).any() for col in float_cols)\n",
    "assert not has_inf, \"Data still contains inf!\"\n",
    "assert not has_nan, \"Data still contains nan!\"\n",
    "print(\"\u2713 Data validated: no inf/nan values\")\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_1h = df_train_val[numeric_features_1h]\n",
    "X_4h = df_train_val[numeric_features_4h]\n",
    "X_24h = df_train_val[numeric_features_24h]\n",
    "\n",
    "y_1h = df_train_val['target_1h']\n",
    "y_4h = df_train_val['target_4h']\n",
    "y_24h = df_train_val['target_24h']\n",
    "\n",
    "y_dir_1h = df_train_val['direction_class_1h']\n",
    "y_dir_4h = df_train_val['direction_class_4h']\n",
    "\n",
    "current_gas = df_train_val['gas']\n",
    "\n",
    "# === BASELINE MODELS (on both train AND holdout) ===\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BASELINE COMPARISONS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "naive_mae_1h = np.mean(np.abs(y_1h.values - current_gas.values))\n",
    "naive_mae_4h = np.mean(np.abs(y_4h.values - current_gas.values))\n",
    "\n",
    "mean_pred = np.full_like(y_1h.values, y_1h.mean())\n",
    "mean_mae_1h = np.mean(np.abs(y_1h.values - mean_pred))\n",
    "mean_mae_4h = np.mean(np.abs(y_4h.values - mean_pred))\n",
    "\n",
    "print(f\"\\nTRAINING SET Baseline MAEs:\")\n",
    "print(f\"  Naive (current price):     MAE_1h={naive_mae_1h:.6f}, MAE_4h={naive_mae_4h:.6f}\")\n",
    "print(f\"  Mean (historical average): MAE_1h={mean_mae_1h:.6f}, MAE_4h={mean_mae_4h:.6f}\")\n",
    "\n",
    "best_baseline_1h = min(naive_mae_1h, mean_mae_1h)\n",
    "best_baseline_4h = min(naive_mae_4h, mean_mae_4h)\n",
    "\n",
    "BASELINES = {\n",
    "    '1h': {'naive_mae': naive_mae_1h, 'mean_mae': mean_mae_1h, 'best': best_baseline_1h},\n",
    "    '4h': {'naive_mae': naive_mae_4h, 'mean_mae': mean_mae_4h, 'best': best_baseline_4h},\n",
    "}\n",
    "\n",
    "# Holdout baselines (CRITICAL for proper model selection)\n",
    "if HAS_HOLDOUT:\n",
    "    holdout_gas = df_holdout['gas']\n",
    "    holdout_y_1h = df_holdout['target_1h']\n",
    "    holdout_y_4h = df_holdout['target_4h']\n",
    "    \n",
    "    holdout_naive_1h = np.mean(np.abs(holdout_y_1h.values - holdout_gas.values))\n",
    "    holdout_naive_4h = np.mean(np.abs(holdout_y_4h.values - holdout_gas.values))\n",
    "    \n",
    "    holdout_mean_pred = np.full_like(holdout_y_1h.values, y_1h.mean())  # Use training mean\n",
    "    holdout_mean_1h = np.mean(np.abs(holdout_y_1h.values - holdout_mean_pred))\n",
    "    holdout_mean_4h = np.mean(np.abs(holdout_y_4h.values - holdout_mean_pred))\n",
    "    \n",
    "    print(f\"\\nHOLDOUT SET Baseline MAEs:\")\n",
    "    print(f\"  Naive (current price):     MAE_1h={holdout_naive_1h:.6f}, MAE_4h={holdout_naive_4h:.6f}\")\n",
    "    print(f\"  Mean (historical average): MAE_1h={holdout_mean_1h:.6f}, MAE_4h={holdout_mean_4h:.6f}\")\n",
    "    \n",
    "    BASELINES['1h']['holdout_naive_mae'] = holdout_naive_1h\n",
    "    BASELINES['1h']['holdout_mean_mae'] = holdout_mean_1h\n",
    "    BASELINES['1h']['holdout_best'] = min(holdout_naive_1h, holdout_mean_1h)\n",
    "    \n",
    "    BASELINES['4h']['holdout_naive_mae'] = holdout_naive_4h\n",
    "    BASELINES['4h']['holdout_mean_mae'] = holdout_mean_4h\n",
    "    BASELINES['4h']['holdout_best'] = min(holdout_naive_4h, holdout_mean_4h)\n",
    "\n",
    "# === ADAPTIVE CONFIGURATION BASED ON VOLATILITY ===\n",
    "# These will be used in Cell 7 for model training\n",
    "ADAPTIVE_CONFIG = {\n",
    "    'shift_detected': DISTRIBUTION_SHIFT_DETECTED,\n",
    "    'shift_magnitude': SHIFT_MAGNITUDE,\n",
    "    'volatility_level': volatility_level if DISTRIBUTION_SHIFT_DETECTED else 'NORMAL'\n",
    "}\n",
    "\n",
    "if DISTRIBUTION_SHIFT_DETECTED:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ADAPTIVE CONFIGURATION FOR VOLATILE PERIOD\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Volatility level: {ADAPTIVE_CONFIG['volatility_level']}\")\n",
    "    print(f\"  Shift magnitude: {SHIFT_MAGNITUDE:.2f}\")\n",
    "    print(f\"  Rolling window: {ROLLING_WINDOW_DAYS} days\")\n",
    "    print(\"  \u2192 Model training will use relaxed requirements and increased regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training - WITH ADAPTIVE REGULARIZATION & RELAXED BASELINES FOR VOLATILE PERIODS\n",
    "import inspect\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, HuberRegressor, ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === BASE CONFIGURATION ===\n",
    "TRAIN_REGIME_MODELS = True\n",
    "COMPUTE_PERMUTATION_IMPORTANCE = True\n",
    "ENABLE_FEATURE_PRUNING = True\n",
    "MAX_FEATURES_TO_USE = 15  # Limit features to prevent overfitting (was no limit)\n",
    "FEATURE_PRUNING_THRESHOLD = 0.01  # Remove features with < 1% importance - AGGRESSIVE\n",
    "\n",
    "# Ensemble settings\n",
    "USE_ENSEMBLE_BLENDING = True\n",
    "ENSEMBLE_ASYM_WEIGHT = 0.6\n",
    "ASYMMETRIC_ALPHA = 0.6\n",
    "\n",
    "# === ADAPTIVE CONFIGURATION BASED ON VOLATILITY ===\n",
    "# Automatically adjust based on distribution shift detected in Cell 6\n",
    "if 'ADAPTIVE_CONFIG' in dir() and ADAPTIVE_CONFIG.get('shift_detected', False):\n",
    "    volatility = ADAPTIVE_CONFIG['volatility_level']\n",
    "    shift_mag = ADAPTIVE_CONFIG['shift_magnitude']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ADAPTIVE MODE: {volatility} VOLATILITY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if volatility == 'SEVERE':\n",
    "        # SEVERE: Very relaxed requirements, maximum regularization\n",
    "        MINIMUM_IMPROVEMENT = -0.10        # Accept 10% WORSE than baseline\n",
    "        HOLDOUT_DEGRADATION_LIMIT = 0.80   # Allow 80% degradation\n",
    "        REGULARIZATION_STRENGTH = 0.5      # Strong regularization\n",
    "        MIN_SAMPLES_LEAF_MULT = 3.0        # Much larger leaves\n",
    "        print(\"  \u2192 Accepting models up to 10% worse than baseline\")\n",
    "        print(\"  \u2192 Strong regularization (0.5), large leaf sizes (3x)\")\n",
    "        \n",
    "    elif volatility == 'HIGH':\n",
    "        # HIGH: Relaxed requirements, high regularization\n",
    "        MINIMUM_IMPROVEMENT = -0.05        # Accept 5% WORSE than baseline\n",
    "        HOLDOUT_DEGRADATION_LIMIT = 0.60   # Allow 60% degradation\n",
    "        REGULARIZATION_STRENGTH = 0.3      # Higher regularization\n",
    "        MIN_SAMPLES_LEAF_MULT = 2.5        # Larger leaves\n",
    "        print(\"  \u2192 Accepting models up to 5% worse than baseline\")\n",
    "        print(\"  \u2192 High regularization (0.3), larger leaf sizes (2.5x)\")\n",
    "        \n",
    "    elif volatility == 'MODERATE':\n",
    "        # MODERATE: Slightly relaxed\n",
    "        MINIMUM_IMPROVEMENT = 0.0          # Just match baseline\n",
    "        HOLDOUT_DEGRADATION_LIMIT = 0.50   # Allow 50% degradation\n",
    "        REGULARIZATION_STRENGTH = 0.2      # Moderate regularization\n",
    "        MIN_SAMPLES_LEAF_MULT = 2.0        # Larger leaves\n",
    "        print(\"  \u2192 Accepting models that match baseline (0% improvement)\")\n",
    "        print(\"  \u2192 Moderate regularization (0.2), leaf sizes (2x)\")\n",
    "        \n",
    "    else:  # MILD\n",
    "        MINIMUM_IMPROVEMENT = 0.02         # 2% improvement required\n",
    "        HOLDOUT_DEGRADATION_LIMIT = 0.40   # Allow 40% degradation\n",
    "        REGULARIZATION_STRENGTH = 0.15     # Light regularization\n",
    "        MIN_SAMPLES_LEAF_MULT = 1.75       # Slightly larger leaves\n",
    "        print(\"  \u2192 Requiring 2% improvement over baseline\")\n",
    "        print(\"  \u2192 Light regularization (0.15), leaf sizes (1.75x)\")\n",
    "else:\n",
    "    # NORMAL: Standard requirements\n",
    "    MINIMUM_IMPROVEMENT = 0.05\n",
    "    HOLDOUT_DEGRADATION_LIMIT = 0.30\n",
    "    REGULARIZATION_STRENGTH = 0.1\n",
    "    MIN_SAMPLES_LEAF_MULT = 1.5\n",
    "    print(\"\\n[Standard mode - normal volatility requirements]\")\n",
    "\n",
    "# === MODULE-LEVEL CLASS FOR PICKLING ===\n",
    "class EnsembleModel:\n",
    "    \"\"\"Blended ensemble of symmetric and asymmetric models - defined at module level for pickling\"\"\"\n",
    "    def __init__(self, sym_model, asym_model, sym_scaler, asym_scaler, sym_weight, asym_weight):\n",
    "        self.sym_model = sym_model\n",
    "        self.asym_model = asym_model\n",
    "        self.sym_scaler = sym_scaler\n",
    "        self.asym_scaler = asym_scaler\n",
    "        self.sym_weight = sym_weight\n",
    "        self.asym_weight = asym_weight\n",
    "    \n",
    "    def predict(self, X):\n",
    "        sym_pred = self.sym_model.predict(X)\n",
    "        asym_pred = self.asym_model.predict(X)\n",
    "        return self.sym_weight * sym_pred + self.asym_weight * asym_pred\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'sym_weight': self.sym_weight, 'asym_weight': self.asym_weight}\n",
    "\n",
    "def check_baseline_gate(model_mae, baseline_mae, model_name):\n",
    "    \"\"\"Check if model beats baseline by minimum threshold (ADAPTIVE)\"\"\"\n",
    "    improvement = (baseline_mae - model_mae) / baseline_mae\n",
    "    passed = improvement >= MINIMUM_IMPROVEMENT\n",
    "    \n",
    "    if passed:\n",
    "        if MINIMUM_IMPROVEMENT < 0:\n",
    "            print(f\"  \u2713 PASSED (relaxed): {improvement*100:.1f}% vs baseline (threshold: {MINIMUM_IMPROVEMENT*100:.0f}%)\")\n",
    "        else:\n",
    "            print(f\"  \u2713 PASSED baseline gate: {improvement*100:.1f}% improvement\")\n",
    "    else:\n",
    "        print(f\"  \u2717 FAILED baseline gate: {improvement*100:.1f}% (need {MINIMUM_IMPROVEMENT*100:.0f}%+)\")\n",
    "    return passed, improvement\n",
    "\n",
    "def check_holdout_gate(cv_mae, holdout_mae, model_name, holdout_baseline=None):\n",
    "    \"\"\"Check if holdout performance is acceptable (ADAPTIVE)\"\"\"\n",
    "    if cv_mae <= 0:\n",
    "        return False, 0\n",
    "    degradation = (holdout_mae - cv_mae) / cv_mae\n",
    "    \n",
    "    if holdout_baseline is not None:\n",
    "        holdout_improvement = (holdout_baseline - holdout_mae) / holdout_baseline\n",
    "        # During volatile periods, accept worse performance\n",
    "        threshold = MINIMUM_IMPROVEMENT\n",
    "        if holdout_improvement >= threshold:\n",
    "            print(f\"  \u2713 Beats holdout baseline by {holdout_improvement*100:.1f}% (threshold: {threshold*100:.0f}%)\")\n",
    "            return True, degradation\n",
    "    \n",
    "    passed = degradation < HOLDOUT_DEGRADATION_LIMIT\n",
    "    if passed:\n",
    "        print(f\"  \u2713 PASSED holdout gate: {degradation*100:+.1f}% degradation (limit: {HOLDOUT_DEGRADATION_LIMIT*100:.0f}%)\")\n",
    "    else:\n",
    "        print(f\"  \u2717 FAILED holdout gate: {degradation*100:+.1f}% degradation (limit: {HOLDOUT_DEGRADATION_LIMIT*100:.0f}%)\")\n",
    "    return passed, degradation\n",
    "\n",
    "def asymmetric_loss(y_true, y_pred, alpha=0.6):\n",
    "    \"\"\"Pinball loss - alpha > 0.5 penalizes under-prediction more\"\"\"\n",
    "    errors = y_true - y_pred\n",
    "    loss = np.where(errors >= 0, alpha * np.abs(errors), (1 - alpha) * np.abs(errors))\n",
    "    return np.mean(loss)\n",
    "\n",
    "def walk_forward_validate(model_class, model_params, X, y, baseline_mae, n_splits=5, purge_gap=120):\n",
    "    \"\"\"Walk-forward validation with purge gap\"\"\"\n",
    "    n = len(X)\n",
    "    fold_size = n // (n_splits + 1)\n",
    "    fold_results = []\n",
    "    \n",
    "    for fold in range(n_splits):\n",
    "        train_end = fold_size * (fold + 1)\n",
    "        test_start = train_end + purge_gap\n",
    "        test_end = test_start + fold_size\n",
    "        \n",
    "        if test_end > n:\n",
    "            break\n",
    "            \n",
    "        X_train = X.iloc[:train_end]\n",
    "        X_test = X.iloc[test_start:test_end]\n",
    "        y_train = y.iloc[:train_end]\n",
    "        y_test = y.iloc[test_start:test_end]\n",
    "        \n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        model = model_class(**model_params)\n",
    "        # Create sample weights: night samples get 2x weight, recent data gets higher weight\n",
    "    is_night_train = df_train_val['is_night'].iloc[:len(y_train)].values if 'is_night' in df_train_val.columns else np.zeros(len(y_train))\n",
    "    night_weights = np.where(is_night_train == 1, 1.3, 1.0)\n",
    "    \n",
    "    # Recency weighting (more recent = higher weight)\n",
    "    recency_weights = np.linspace(0.9, 1.1, len(y_train))\n",
    "    \n",
    "    # Combined weights\n",
    "    sample_weights = night_weights * recency_weights\n",
    "    \n",
    "    # Check if model supports sample_weight\n",
    "    if 'sample_weight' in inspect.signature(model.fit).parameters:\n",
    "        model.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
    "    else:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        fold_results.append(mae)\n",
    "    \n",
    "    if not fold_results:\n",
    "        return None\n",
    "        \n",
    "    return {\n",
    "        'avg_mae': np.mean(fold_results),\n",
    "        'std_mae': np.std(fold_results),\n",
    "        'improvement': (baseline_mae - np.mean(fold_results)) / baseline_mae,\n",
    "        'weighted_mae': np.average(fold_results, weights=np.arange(1, len(fold_results)+1)) if fold_results else None\n",
    "    }\n",
    "\n",
    "\n",
    "# Use GPU acceleration if available\n",
    "if GPU_AVAILABLE and 'gpu_device_id' in GPU_PARAMS:\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # LightGBM GPU support\n",
    "    lgb_params = {\n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'verbose': -1\n",
    "    }\n",
    "else:\n",
    "    lgb_params = {}\n",
    "\n",
    "\n",
    "def get_models_to_try():\n",
    "    \"\"\"Get list of models with ADAPTIVE REGULARIZATION\"\"\"\n",
    "    # Scale regularization based on volatility\n",
    "    base_min_samples = int(30 * MIN_SAMPLES_LEAF_MULT)\n",
    "    ridge_alpha = 10.0 * (1 + REGULARIZATION_STRENGTH * 5)\n",
    "    huber_alpha = 0.5 * (1 + REGULARIZATION_STRENGTH * 3)\n",
    "    elastic_alpha = 0.5 * (1 + REGULARIZATION_STRENGTH * 3)\n",
    "    gbm_lr = max(0.03, 0.1 - REGULARIZATION_STRENGTH * 0.15)  # Slower learning with more regularization\n",
    "    \n",
    "    models = [\n",
    "        ('Ridge', Ridge, {'alpha': ridge_alpha, 'random_state': 42}),\n",
    "        ('Huber', HuberRegressor, {'epsilon': 1.35, 'alpha': huber_alpha, 'max_iter': 1000}),\n",
    "        ('ElasticNet', ElasticNet, {'alpha': elastic_alpha, 'l1_ratio': 0.8, 'random_state': 42, 'max_iter': 2000}),\n",
    "        ('RF', RandomForestRegressor, {\n",
    "            'n_estimators': 100,  # More trees for stability\n",
    "            'max_depth': 4,       # Shallower trees\n",
    "            'min_samples_leaf': base_min_samples,\n",
    "            'max_features': 0.5,  # Use fewer features per tree for more robustness\n",
    "            'random_state': 42, \n",
    "            'n_jobs': -1\n",
    "        }),\n",
    "        ('GBM', GradientBoostingRegressor, {\n",
    "            'n_estimators': 80, \n",
    "            'max_depth': 3,      # Shallower\n",
    "            'learning_rate': gbm_lr,\n",
    "            'min_samples_leaf': base_min_samples,\n",
    "            'subsample': 0.7,    # More subsampling for regularization\n",
    "            'random_state': 42\n",
    "        }),\n",
    "        ('GBM-Asym', GradientBoostingRegressor, {\n",
    "            'loss': 'quantile', \n",
    "            'alpha': ASYMMETRIC_ALPHA,\n",
    "            'n_estimators': 80, \n",
    "            'max_depth': 3,\n",
    "            'learning_rate': gbm_lr,\n",
    "            'min_samples_leaf': base_min_samples,\n",
    "            'subsample': 0.7,\n",
    "            'random_state': 42\n",
    "        }),\n",
    "    ]\n",
    "    \n",
    "    return models\n",
    "\n",
    "def compute_permutation_importance(model, X, y, scaler, feature_names, n_repeats=5):\n",
    "    \"\"\"Compute permutation importance for any model type\"\"\"\n",
    "    X_scaled = scaler.transform(X)\n",
    "    \n",
    "    result = permutation_importance(\n",
    "        model, X_scaled, y,\n",
    "        n_repeats=n_repeats,\n",
    "        random_state=42,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    importance_dict = {}\n",
    "    for i, feat in enumerate(feature_names):\n",
    "        importance_dict[feat] = -result.importances_mean[i]\n",
    "    \n",
    "    total = sum(importance_dict.values())\n",
    "    if total > 0:\n",
    "        importance_dict = {k: v/total for k, v in importance_dict.items()}\n",
    "    \n",
    "    return importance_dict\n",
    "\n",
    "def prune_features_by_importance(X_train, X_holdout, feature_names, importance_dict, threshold=0.0):\n",
    "    \"\"\"Remove features with importance below threshold\"\"\"\n",
    "    features_to_keep = [f for f in feature_names if importance_dict.get(f, 0) >= threshold]\n",
    "    features_to_remove = [f for f in feature_names if f not in features_to_keep]\n",
    "    \n",
    "    if not features_to_remove:\n",
    "        return X_train, X_holdout, feature_names, []\n",
    "    \n",
    "    print(f\"  Feature pruning: removing {len(features_to_remove)} features with importance < {threshold}\")\n",
    "    for f in features_to_remove[:5]:\n",
    "        print(f\"    - {f}: {importance_dict.get(f, 0):.4f}\")\n",
    "    if len(features_to_remove) > 5:\n",
    "        print(f\"    ... and {len(features_to_remove) - 5} more\")\n",
    "    \n",
    "    X_train_pruned = X_train[features_to_keep]\n",
    "    X_holdout_pruned = X_holdout[features_to_keep]\n",
    "    \n",
    "    return X_train_pruned, X_holdout_pruned, features_to_keep, features_to_remove\n",
    "\n",
    "def create_ensemble_model(sym_model, asym_model, sym_scaler, asym_scaler, sym_weight=0.4, asym_weight=0.6):\n",
    "    \"\"\"Create blended ensemble of symmetric and asymmetric models\"\"\"\n",
    "    return EnsembleModel(sym_model, asym_model, sym_scaler, asym_scaler, sym_weight, asym_weight)\n",
    "\n",
    "def train_model_with_holdout(X_train, y_train, X_holdout, y_holdout, baseline_mae, \n",
    "                             horizon_name, feature_names, holdout_baseline=None):\n",
    "    \"\"\"Train model with ADAPTIVE requirements\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {horizon_name} model\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Train: {len(X_train):,}, Holdout: {len(X_holdout):,}, Features: {X_train.shape[1]}\")\n",
    "    print(f\"Train baseline: {baseline_mae:.6f}\", end=\"\")\n",
    "    if holdout_baseline:\n",
    "        print(f\", Holdout baseline: {holdout_baseline:.6f}\")\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "    # Show adaptive settings\n",
    "    print(f\"Adaptive settings: min_improvement={MINIMUM_IMPROVEMENT*100:.0f}%, \"\n",
    "          f\"reg_strength={REGULARIZATION_STRENGTH:.2f}, leaf_mult={MIN_SAMPLES_LEAF_MULT:.1f}x\")\n",
    "    \n",
    "    models_to_try = get_models_to_try()\n",
    "    results = []\n",
    "    sym_results = []\n",
    "    asym_results = []\n",
    "    \n",
    "    for name, model_class, params in models_to_try:\n",
    "        print(f\"\\n[{name}]\")\n",
    "        try:\n",
    "            wf_result = walk_forward_validate(model_class, params, X_train, y_train, baseline_mae, n_splits=4, purge_gap=120)\n",
    "            if not wf_result:\n",
    "                continue\n",
    "                \n",
    "            cv_mae = wf_result['avg_mae']\n",
    "            print(f\"  CV MAE: {cv_mae:.6f} \u00b1 {wf_result['std_mae']:.6f}\")\n",
    "            \n",
    "            scaler = RobustScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_holdout_scaled = scaler.transform(X_holdout)\n",
    "            \n",
    "            model = model_class(**params)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            y_holdout_pred = model.predict(X_holdout_scaled)\n",
    "            holdout_mae = mean_absolute_error(y_holdout, y_holdout_pred)\n",
    "            holdout_improvement = (baseline_mae - holdout_mae) / baseline_mae\n",
    "            \n",
    "            asym_loss = asymmetric_loss(y_holdout.values, y_holdout_pred, ASYMMETRIC_ALPHA)\n",
    "            \n",
    "            if holdout_baseline:\n",
    "                vs_holdout = (holdout_baseline - holdout_mae) / holdout_baseline\n",
    "                print(f\"  HOLDOUT MAE: {holdout_mae:.6f} ({vs_holdout*100:+.1f}% vs holdout baseline)\")\n",
    "            else:\n",
    "                print(f\"  HOLDOUT MAE: {holdout_mae:.6f} ({holdout_improvement*100:+.1f}% vs train baseline)\")\n",
    "            \n",
    "            use_baseline = holdout_baseline if holdout_baseline else baseline_mae\n",
    "            passed_baseline, _ = check_baseline_gate(holdout_mae, use_baseline, name)\n",
    "            passed_holdout, degradation = check_holdout_gate(cv_mae, holdout_mae, name, holdout_baseline)\n",
    "            \n",
    "            result_entry = {\n",
    "                'name': name, 'model_class': model_class, 'params': params,\n",
    "                'cv_mae': cv_mae, 'holdout_mae': holdout_mae,\n",
    "                'asymmetric_loss': asym_loss,\n",
    "                'holdout_improvement': holdout_improvement,\n",
    "                'vs_holdout_baseline': (holdout_baseline - holdout_mae) / holdout_baseline if holdout_baseline else None,\n",
    "                'model': model, 'scaler': scaler\n",
    "            }\n",
    "            \n",
    "            # Relaxed acceptance during volatile periods\n",
    "            if passed_baseline or passed_holdout:\n",
    "                results.append(result_entry)\n",
    "                if 'Asym' in name:\n",
    "                    asym_results.append(result_entry)\n",
    "                else:\n",
    "                    sym_results.append(result_entry)\n",
    "                print(f\"  \u2192 Accepted\")\n",
    "            else:\n",
    "                print(f\"  \u2192 Rejected\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Failed: {e}\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"\\n\u26a0\ufe0f All models failed! Using Huber fallback...\")\n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        model = HuberRegressor(epsilon=1.35, alpha=0.1 * (1 + REGULARIZATION_STRENGTH * 3), max_iter=1000)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_holdout_pred = model.predict(scaler.transform(X_holdout))\n",
    "        holdout_mae = mean_absolute_error(y_holdout, y_holdout_pred)\n",
    "        \n",
    "        importance = {}\n",
    "        if COMPUTE_PERMUTATION_IMPORTANCE:\n",
    "            importance = compute_permutation_importance(model, X_holdout, y_holdout, scaler, feature_names)\n",
    "        \n",
    "        return model, scaler, {\n",
    "            'name': 'Huber (fallback)',\n",
    "            'mae': holdout_mae,\n",
    "            'improvement': (baseline_mae - holdout_mae) / baseline_mae,\n",
    "            'vs_holdout_baseline': (holdout_baseline - holdout_mae) / holdout_baseline if holdout_baseline else None,\n",
    "            'passed_baseline': False,\n",
    "            'is_fallback': True,\n",
    "            'is_ensemble': False\n",
    "        }, importance, filtered_features)\n",
    "    \n",
    "    # === FILTER FEATURES BY IMPORTANCE ===\n",
    "    if ENABLE_FEATURE_PRUNING and importance:\n",
    "        # Sort by importance and keep top features\n",
    "        sorted_features = sorted(importance.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "        max_features = min(len(sorted_features), MAX_FEATURES_TO_USE if 'MAX_FEATURES_TO_USE' in dir() else 15)\n",
    "        \n",
    "        # Keep features above threshold AND in top N\n",
    "        filtered_features = [\n",
    "            feat for feat, imp in sorted_features[:max_features]\n",
    "            if abs(imp) >= FEATURE_PRUNING_THRESHOLD\n",
    "        ]\n",
    "        \n",
    "        if len(filtered_features) == 0:\n",
    "            # Fallback: keep top 5 even if below threshold\n",
    "            filtered_features = [feat for feat, _ in sorted_features[:5]]\n",
    "        \n",
    "        print(f\"  Feature filtering: {len(feature_names)} \u2192 {len(filtered_features)} (importance >= {FEATURE_PRUNING_THRESHOLD})\")\n",
    "        print(f\"  Kept features: {filtered_features}\")\n",
    "    else:\n",
    "        filtered_features = list(feature_names)\n",
    "    \n",
    "    return model, scaler, {\n",
    "    \n",
    "    # === ENSEMBLE BLENDING ===\n",
    "    best_single = min(results, key=lambda x: x['holdout_mae'])\n",
    "    best_model = best_single['model']\n",
    "    best_scaler = best_single['scaler']\n",
    "    best_metrics = {\n",
    "        'name': best_single['name'],\n",
    "        'mae': best_single['holdout_mae'],\n",
    "        'cv_mae': best_single['cv_mae'],\n",
    "        'asymmetric_loss': best_single.get('asymmetric_loss'),\n",
    "        'improvement': best_single['holdout_improvement'],\n",
    "        'vs_holdout_baseline': best_single['vs_holdout_baseline'],\n",
    "        'passed_baseline': True,\n",
    "        'is_fallback': False,\n",
    "        'is_ensemble': False\n",
    "    }\n",
    "    \n",
    "    # Try ensemble if we have both symmetric and asymmetric models\n",
    "    if USE_ENSEMBLE_BLENDING and sym_results and asym_results:\n",
    "        print(f\"\\n>>> Trying ensemble blend...\")\n",
    "        best_sym = min(sym_results, key=lambda x: x['holdout_mae'])\n",
    "        best_asym = min(asym_results, key=lambda x: x['holdout_mae'])\n",
    "        \n",
    "        X_holdout_scaled = best_sym['scaler'].transform(X_holdout)\n",
    "        sym_pred = best_sym['model'].predict(X_holdout_scaled)\n",
    "        asym_pred = best_asym['model'].predict(X_holdout_scaled)\n",
    "        \n",
    "        best_blend_mae = float('inf')\n",
    "        best_blend_weights = (0.4, 0.6)\n",
    "        \n",
    "        for asym_w in [0.5, 0.55, 0.6, 0.65, 0.7]:\n",
    "            sym_w = 1 - asym_w\n",
    "            blend_pred = sym_w * sym_pred + asym_w * asym_pred\n",
    "            blend_mae = mean_absolute_error(y_holdout, blend_pred)\n",
    "            if blend_mae < best_blend_mae:\n",
    "                best_blend_mae = blend_mae\n",
    "                best_blend_weights = (sym_w, asym_w)\n",
    "        \n",
    "        print(f\"  Best single ({best_single['name']}): MAE={best_single['holdout_mae']:.6f}\")\n",
    "        print(f\"  Best ensemble ({best_sym['name']}+{best_asym['name']}): MAE={best_blend_mae:.6f}\")\n",
    "        print(f\"  Blend weights: {best_blend_weights[0]:.0%} sym + {best_blend_weights[1]:.0%} asym\")\n",
    "        \n",
    "        if best_blend_mae < best_single['holdout_mae']:\n",
    "            print(f\"  \u2713 Using ensemble (improves by {(best_single['holdout_mae'] - best_blend_mae) / best_single['holdout_mae'] * 100:.1f}%)\")\n",
    "            \n",
    "            ensemble = create_ensemble_model(\n",
    "                best_sym['model'], best_asym['model'],\n",
    "                best_sym['scaler'], best_asym['scaler'],\n",
    "                best_blend_weights[0], best_blend_weights[1]\n",
    "            )\n",
    "            \n",
    "            best_model = ensemble\n",
    "            best_scaler = best_sym['scaler']\n",
    "            best_metrics = {\n",
    "                'name': f\"Ensemble({best_sym['name']}+{best_asym['name']})\",\n",
    "                'mae': best_blend_mae,\n",
    "                'cv_mae': (best_sym['cv_mae'] + best_asym['cv_mae']) / 2,\n",
    "                'improvement': (baseline_mae - best_blend_mae) / baseline_mae,\n",
    "                'vs_holdout_baseline': (holdout_baseline - best_blend_mae) / holdout_baseline if holdout_baseline else None,\n",
    "                'passed_baseline': True,\n",
    "                'is_fallback': False,\n",
    "                'is_ensemble': True,\n",
    "                'ensemble_weights': best_blend_weights,\n",
    "                'sym_model': best_sym['name'],\n",
    "                'asym_model': best_asym['name']\n",
    "            }\n",
    "        else:\n",
    "            print(f\"  \u2717 Single model is better, not using ensemble\")\n",
    "    \n",
    "    print(f\"\\n>>> Best: {best_metrics['name']} (Holdout MAE: {best_metrics['mae']:.6f})\")\n",
    "    \n",
    "    # Compute permutation importance\n",
    "    importance = {}\n",
    "    if COMPUTE_PERMUTATION_IMPORTANCE:\n",
    "        print(\"  Computing permutation importance...\")\n",
    "        if best_metrics.get('is_ensemble'):\n",
    "            importance = compute_permutation_importance(\n",
    "                best_single['model'], X_holdout, y_holdout, best_scaler, list(feature_names)\n",
    "            )\n",
    "        else:\n",
    "            importance = compute_permutation_importance(\n",
    "                best_model, X_holdout, y_holdout, best_scaler, list(feature_names)\n",
    "            )\n",
    "        top_3 = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        print(f\"  Top features: {', '.join([f'{f[0]}({f[1]:.2f})' for f in top_3])}\")\n",
    "        \n",
    "        # Feature pruning (only if not ensemble)\n",
    "        if ENABLE_FEATURE_PRUNING and not best_metrics.get('is_ensemble'):\n",
    "            negative_features = [f for f, v in importance.items() if v < FEATURE_PRUNING_THRESHOLD]\n",
    "            if negative_features and len(feature_names) - len(negative_features) >= 5:\n",
    "                X_train_pruned, X_holdout_pruned, pruned_features, removed = prune_features_by_importance(\n",
    "                    X_train, X_holdout, list(feature_names), importance, FEATURE_PRUNING_THRESHOLD\n",
    "                )\n",
    "                \n",
    "                if len(pruned_features) >= 5:\n",
    "                    print(f\"  Re-training with {len(pruned_features)} features...\")\n",
    "                    \n",
    "                    scaler_pruned = RobustScaler()\n",
    "                    X_train_pruned_scaled = scaler_pruned.fit_transform(X_train_pruned)\n",
    "                    X_holdout_pruned_scaled = scaler_pruned.transform(X_holdout_pruned)\n",
    "                    \n",
    "                    model_pruned = best_single['model_class'](**best_single['params'])\n",
    "                    model_pruned.fit(X_train_pruned_scaled, y_train)\n",
    "                    \n",
    "                    y_holdout_pred_pruned = model_pruned.predict(X_holdout_pruned_scaled)\n",
    "                    holdout_mae_pruned = mean_absolute_error(y_holdout, y_holdout_pred_pruned)\n",
    "                    \n",
    "                    print(f\"  Pruned model MAE: {holdout_mae_pruned:.6f} (original: {best_metrics['mae']:.6f})\")\n",
    "                    \n",
    "                    if holdout_mae_pruned <= best_metrics['mae'] * 1.02:\n",
    "                        print(f\"  \u2713 Using pruned model ({len(pruned_features)} features)\")\n",
    "                        best_model = model_pruned\n",
    "                        best_scaler = scaler_pruned\n",
    "                        best_metrics['mae'] = holdout_mae_pruned\n",
    "                        best_metrics['n_features_pruned'] = len(removed)\n",
    "                        feature_names = pruned_features\n",
    "                        importance = {f: importance[f] for f in pruned_features}\n",
    "    \n",
    "    return best_model, best_scaler, best_metrics, importance, list(feature_names)\n",
    "\n",
    "def train_regime_models(X_train, y_train, X_holdout, y_holdout, regime_train, regime_holdout,\n",
    "                        baseline_mae, horizon_name, feature_names, holdout_baseline=None):\n",
    "    \"\"\"Train separate models for each regime\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training REGIME-SPECIFIC {horizon_name} models\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    regime_models = {}\n",
    "    \n",
    "    for regime_val, regime_name in [(0, 'normal'), (1, 'elevated'), (2, 'spike')]:\n",
    "        train_mask = regime_train == regime_val\n",
    "        holdout_mask = regime_holdout == regime_val\n",
    "        \n",
    "        n_train = train_mask.sum()\n",
    "        n_holdout = holdout_mask.sum()\n",
    "        \n",
    "        print(f\"\\n[{regime_name.upper()}] Train: {n_train}, Holdout: {n_holdout}\")\n",
    "        \n",
    "        if n_train < 500 or n_holdout < 100:\n",
    "            print(f\"  Insufficient data, skipping\")\n",
    "            continue\n",
    "        \n",
    "        X_r_train = X_train[train_mask]\n",
    "        y_r_train = y_train[train_mask]\n",
    "        X_r_holdout = X_holdout[holdout_mask]\n",
    "        y_r_holdout = y_holdout[holdout_mask]\n",
    "        \n",
    "        model, scaler, metrics, importance, final_features = train_model_with_holdout(\n",
    "            X_r_train, y_r_train, X_r_holdout, y_r_holdout,\n",
    "            baseline_mae, f\"{horizon_name}_{regime_name}\", feature_names, holdout_baseline\n",
    "        )\n",
    "        \n",
    "        if model:\n",
    "            regime_models[regime_val] = {\n",
    "                'model': model, 'scaler': scaler, 'metrics': metrics,\n",
    "                'regime_name': regime_name, 'n_samples': n_train,\n",
    "                'features': final_features\n",
    "            }\n",
    "    \n",
    "    return regime_models\n",
    "\n",
    "def print_distribution_diagnostics(y_train, y_holdout, name=\"\"):\n",
    "    \"\"\"Print diagnostics\"\"\"\n",
    "    print(f\"\\n[Distribution - {name}]\")\n",
    "    print(f\"  Train:   mean={y_train.mean():.4f}, std={y_train.std():.4f}\")\n",
    "    print(f\"  Holdout: mean={y_holdout.mean():.4f}, std={y_holdout.std():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models with ENSEMBLE REGIME SWITCHING\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING ALL PREDICTION MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trained_models = {}\n",
    "regime_specific_models = {}\n",
    "all_feature_importance = {}\n",
    "pruned_features_log = {}  # NEW: Track which features were pruned\n",
    "\n",
    "if not HAS_HOLDOUT or df_holdout is None or len(df_holdout) < 1000:\n",
    "    print(\"\\n\u26a0\ufe0f WARNING: Limited holdout data\")\n",
    "\n",
    "print(f\"\\nTraining set: {len(df_train_val):,} samples\")\n",
    "print(f\"Holdout set:  {len(df_holdout) if df_holdout is not None else 0:,} samples\")\n",
    "if DISTRIBUTION_SHIFT_DETECTED:\n",
    "    print(f\"\u26a0\ufe0f Distribution shift detected (magnitude: {SHIFT_MAGNITUDE:.2f})\")\n",
    "    if USE_ROLLING_WINDOW:\n",
    "        print(f\"   Auto-adapted to {ROLLING_WINDOW_DAYS}-day rolling window\")\n",
    "\n",
    "# === CREATE REGIME LABELS ===\n",
    "print(\"\\nCreating regime labels...\")\n",
    "regime_train = pd.Series(0, index=df_train_val.index)\n",
    "if 'gas_zscore_1h' in df_train_val.columns:\n",
    "    regime_train[df_train_val['gas_zscore_1h'] > 1] = 1\n",
    "if 'is_spike' in df_train_val.columns:\n",
    "    regime_train[df_train_val['is_spike'] == 1] = 2\n",
    "\n",
    "regime_holdout = None\n",
    "if HAS_HOLDOUT:\n",
    "    regime_holdout = pd.Series(0, index=df_holdout.index)\n",
    "    if 'gas_zscore_1h' in df_holdout.columns:\n",
    "        regime_holdout[df_holdout['gas_zscore_1h'] > 1] = 1\n",
    "    if 'is_spike' in df_holdout.columns:\n",
    "        regime_holdout[df_holdout['is_spike'] == 1] = 2\n",
    "\n",
    "print(f\"Regime distribution (train): {dict(regime_train.value_counts().sort_index())}\")\n",
    "if regime_holdout is not None:\n",
    "    print(f\"Regime distribution (holdout): {dict(regime_holdout.value_counts().sort_index())}\")\n",
    "\n",
    "# === ENSEMBLE PREDICTION FUNCTION ===\n",
    "def create_ensemble_predictor(global_model, global_scaler, regime_models, features):\n",
    "    \"\"\"Create a predictor that uses regime-specific models when available.\n",
    "    Handles feature mismatches when regime models use different (pruned) features.\n",
    "    \"\"\"\n",
    "    def predict(X, current_regime=None):\n",
    "        # Handle both DataFrame and array inputs\n",
    "        if hasattr(X, 'columns'):\n",
    "            # DataFrame input - select features by name\n",
    "            X_global = X[features] if all(f in X.columns for f in features) else X\n",
    "            X_scaled = global_scaler.transform(X_global)\n",
    "        else:\n",
    "            # Array input - assume correct feature order\n",
    "            X_scaled = global_scaler.transform(X)\n",
    "        \n",
    "        global_pred = global_model.predict(X_scaled)\n",
    "        \n",
    "        if current_regime is not None and regime_models and current_regime in regime_models:\n",
    "            regime_data = regime_models[current_regime]\n",
    "            regime_features = regime_data.get('features', features)\n",
    "            \n",
    "            if hasattr(X, 'columns'):\n",
    "                # DataFrame - select regime-specific features\n",
    "                available_features = [f for f in regime_features if f in X.columns]\n",
    "                if len(available_features) == len(regime_features):\n",
    "                    X_regime = X[regime_features]\n",
    "                    X_regime_scaled = regime_data['scaler'].transform(X_regime)\n",
    "                    regime_pred = regime_data['model'].predict(X_regime_scaled)\n",
    "                    # Weighted average: 70% regime, 30% global\n",
    "                    return 0.7 * regime_pred + 0.3 * global_pred\n",
    "            else:\n",
    "                # Array input - only use if feature counts match\n",
    "                if X.shape[1] == len(regime_features):\n",
    "                    X_regime_scaled = regime_data['scaler'].transform(X)\n",
    "                    regime_pred = regime_data['model'].predict(X_regime_scaled)\n",
    "                    return 0.7 * regime_pred + 0.3 * global_pred\n",
    "        \n",
    "        return global_pred\n",
    "    \n",
    "    return predict\n",
    "\n",
    "# === 1H MODEL ===\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"1-HOUR MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_holdout_1h = df_holdout[numeric_features_1h] if HAS_HOLDOUT else X_1h.iloc[-1000:]\n",
    "y_holdout_1h = df_holdout['target_1h'] if HAS_HOLDOUT else y_1h.iloc[-1000:]\n",
    "mask_1h = y_holdout_1h.notna()\n",
    "X_holdout_1h = X_holdout_1h[mask_1h]\n",
    "y_holdout_1h = y_holdout_1h[mask_1h]\n",
    "\n",
    "print_distribution_diagnostics(y_1h, y_holdout_1h, \"1h targets\")\n",
    "\n",
    "holdout_baseline_1h = BASELINES['1h'].get('holdout_best', None)\n",
    "\n",
    "# Updated: now returns 5 values (model, scaler, metrics, importance, final_features)\n",
    "model_1h, scaler_1h, metrics_1h, importance_1h, features_1h = train_model_with_holdout(\n",
    "    X_1h, y_1h, X_holdout_1h, y_holdout_1h,\n",
    "    BASELINES['1h']['best'], '1h', list(numeric_features_1h), holdout_baseline_1h\n",
    ")\n",
    "if model_1h:\n",
    "    trained_models['1h'] = {\n",
    "        'model': model_1h, 'scaler': scaler_1h, \n",
    "        'metrics': metrics_1h, 'features': features_1h  # Use possibly-pruned features\n",
    "    }\n",
    "    if importance_1h:\n",
    "        all_feature_importance['1h'] = importance_1h\n",
    "    \n",
    "    # Log if features were pruned\n",
    "    if len(features_1h) < len(numeric_features_1h):\n",
    "        pruned_features_log['1h'] = {\n",
    "            'original_count': len(numeric_features_1h),\n",
    "            'pruned_count': len(features_1h),\n",
    "            'removed': list(set(numeric_features_1h) - set(features_1h))\n",
    "        }\n",
    "        print(f\"  \u2713 Features pruned: {len(numeric_features_1h)} \u2192 {len(features_1h)}\")\n",
    "\n",
    "# Train regime-specific models\n",
    "if TRAIN_REGIME_MODELS and regime_holdout is not None:\n",
    "    regime_holdout_1h = regime_holdout[mask_1h]\n",
    "    regime_models_1h = train_regime_models(\n",
    "        X_1h, y_1h, X_holdout_1h, y_holdout_1h,\n",
    "        regime_train, regime_holdout_1h,\n",
    "        BASELINES['1h']['best'], '1h', list(numeric_features_1h), holdout_baseline_1h\n",
    "    )\n",
    "    if regime_models_1h:\n",
    "        regime_specific_models['1h'] = regime_models_1h\n",
    "        # Create ensemble predictor with the actual features used\n",
    "        trained_models['1h']['ensemble_predict'] = create_ensemble_predictor(\n",
    "            model_1h, scaler_1h, regime_models_1h, features_1h\n",
    "        )\n",
    "\n",
    "# === 4H MODEL ===\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"4-HOUR MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_holdout_4h = df_holdout[numeric_features_4h] if HAS_HOLDOUT else X_4h.iloc[-1000:]\n",
    "y_holdout_4h = df_holdout['target_4h'] if HAS_HOLDOUT else y_4h.iloc[-1000:]\n",
    "mask_4h = y_holdout_4h.notna()\n",
    "X_holdout_4h = X_holdout_4h[mask_4h]\n",
    "y_holdout_4h = y_holdout_4h[mask_4h]\n",
    "\n",
    "print_distribution_diagnostics(y_4h, y_holdout_4h, \"4h targets\")\n",
    "\n",
    "holdout_baseline_4h = BASELINES['4h'].get('holdout_best', None)\n",
    "\n",
    "model_4h, scaler_4h, metrics_4h, importance_4h, features_4h = train_model_with_holdout(\n",
    "    X_4h, y_4h, X_holdout_4h, y_holdout_4h,\n",
    "    BASELINES['4h']['best'], '4h', list(numeric_features_4h), holdout_baseline_4h\n",
    ")\n",
    "if model_4h:\n",
    "    trained_models['4h'] = {\n",
    "        'model': model_4h, 'scaler': scaler_4h,\n",
    "        'metrics': metrics_4h, 'features': features_4h\n",
    "    }\n",
    "    if importance_4h:\n",
    "        all_feature_importance['4h'] = importance_4h\n",
    "    \n",
    "    if len(features_4h) < len(numeric_features_4h):\n",
    "        pruned_features_log['4h'] = {\n",
    "            'original_count': len(numeric_features_4h),\n",
    "            'pruned_count': len(features_4h),\n",
    "            'removed': list(set(numeric_features_4h) - set(features_4h))\n",
    "        }\n",
    "        print(f\"  \u2713 Features pruned: {len(numeric_features_4h)} \u2192 {len(features_4h)}\")\n",
    "\n",
    "if TRAIN_REGIME_MODELS and regime_holdout is not None:\n",
    "    regime_holdout_4h = regime_holdout[mask_4h]\n",
    "    regime_models_4h = train_regime_models(\n",
    "        X_4h, y_4h, X_holdout_4h, y_holdout_4h,\n",
    "        regime_train, regime_holdout_4h,\n",
    "        BASELINES['4h']['best'], '4h', list(numeric_features_4h), holdout_baseline_4h\n",
    "    )\n",
    "    if regime_models_4h:\n",
    "        regime_specific_models['4h'] = regime_models_4h\n",
    "        trained_models['4h']['ensemble_predict'] = create_ensemble_predictor(\n",
    "            model_4h, scaler_4h, regime_models_4h, features_4h\n",
    "        )\n",
    "\n",
    "# === 24H MODEL ===\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"24-HOUR MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rph = 120\n",
    "total_hours = len(df_clean) / rph\n",
    "total_days = total_hours / 24\n",
    "print(f\"Total data: {total_days:.1f} days\")\n",
    "\n",
    "if total_days >= 30:\n",
    "    mask_24h_train = y_24h.notna()\n",
    "    X_24h_valid = X_24h[mask_24h_train]\n",
    "    y_24h_valid = y_24h[mask_24h_train]\n",
    "    \n",
    "    if HAS_HOLDOUT:\n",
    "        y_holdout_24h = df_holdout['target_24h']\n",
    "        mask_24h_holdout = y_holdout_24h.notna()\n",
    "        X_holdout_24h = df_holdout[numeric_features_24h][mask_24h_holdout]\n",
    "        y_holdout_24h = y_holdout_24h[mask_24h_holdout]\n",
    "    else:\n",
    "        X_holdout_24h = X_24h_valid.iloc[-500:]\n",
    "        y_holdout_24h = y_24h_valid.iloc[-500:]\n",
    "    \n",
    "    if len(y_holdout_24h) > 100:\n",
    "        model_24h, scaler_24h, metrics_24h, _, features_24h = train_model_with_holdout(\n",
    "            X_24h_valid, y_24h_valid, X_holdout_24h, y_holdout_24h,\n",
    "            BASELINES['4h']['best'], '24h', list(numeric_features_24h)\n",
    "        )\n",
    "        if model_24h:\n",
    "            trained_models['24h'] = {\n",
    "                'model': model_24h, 'scaler': scaler_24h,\n",
    "                'metrics': metrics_24h, 'features': features_24h,\n",
    "                'is_fallback': False\n",
    "            }\n",
    "    else:\n",
    "        print(f\"\u26a0\ufe0f Using 4h model as 24h fallback\")\n",
    "        if model_4h:\n",
    "            trained_models['24h'] = {\n",
    "                'model': model_4h, 'scaler': scaler_4h,\n",
    "                'metrics': {'name': metrics_4h['name'] + ' (4h fallback)', 'mae': metrics_4h['mae'],\n",
    "                           'improvement': metrics_4h['improvement'], \n",
    "                           'vs_holdout_baseline': metrics_4h.get('vs_holdout_baseline'),\n",
    "                           'passed_baseline': metrics_4h.get('passed_baseline', False)},\n",
    "                'features': features_4h,\n",
    "                'is_fallback': True\n",
    "            }\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f Using 4h model as 24h fallback ({total_days:.1f} days < 30)\")\n",
    "    if model_4h:\n",
    "        trained_models['24h'] = {\n",
    "            'model': model_4h, 'scaler': scaler_4h,\n",
    "            'metrics': {'name': metrics_4h['name'] + ' (4h fallback)', 'mae': metrics_4h['mae'],\n",
    "                       'improvement': metrics_4h['improvement'],\n",
    "                       'vs_holdout_baseline': metrics_4h.get('vs_holdout_baseline'),\n",
    "                       'passed_baseline': metrics_4h.get('passed_baseline', False)},\n",
    "            'features': features_4h,\n",
    "            'is_fallback': True\n",
    "        }\n",
    "\n",
    "# === SUMMARY ===\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for horizon, data in trained_models.items():\n",
    "    m = data['metrics']\n",
    "    status = \"\u2713\" if m.get('passed_baseline', False) else \"\u26a0\"\n",
    "    fallback = \" (fallback)\" if data.get('is_fallback') else \"\"\n",
    "    \n",
    "    # Show vs holdout baseline if available\n",
    "    if m.get('vs_holdout_baseline') is not None:\n",
    "        vs_baseline = f\"{m['vs_holdout_baseline']*100:+.1f}% vs holdout baseline\"\n",
    "    else:\n",
    "        vs_baseline = f\"{m['improvement']*100:+.1f}% vs train baseline\"\n",
    "    \n",
    "    has_ensemble = \" [+ensemble]\" if 'ensemble_predict' in data else \"\"\n",
    "    n_features = len(data.get('features', []))\n",
    "    print(f\"{status} {horizon}: {m['name']}{fallback} | MAE: {m['mae']:.4f} | {vs_baseline} | {n_features} features{has_ensemble}\")\n",
    "\n",
    "if regime_specific_models:\n",
    "    print(f\"\\nRegime-specific models:\")\n",
    "    for horizon, regime_dict in regime_specific_models.items():\n",
    "        for regime_val, regime_data in regime_dict.items():\n",
    "            print(f\"  {horizon}_{regime_data['regime_name']}: MAE={regime_data['metrics']['mae']:.4f}\")\n",
    "\n",
    "if pruned_features_log:\n",
    "    print(f\"\\nFeature pruning summary:\")\n",
    "    for horizon, log in pruned_features_log.items():\n",
    "        print(f\"  {horizon}: {log['original_count']} \u2192 {log['pruned_count']} features\")\n",
    "\n",
    "FEATURE_IMPORTANCE = all_feature_importance.get('4h', all_feature_importance.get('1h', {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME-OF-DAY SPECIFIC MODELS\n",
    "# Train lightweight models for each time period (afternoon has 2x higher errors)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TIME-OF-DAY SPECIFIC MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "TRAIN_TIME_SPECIFIC_MODELS = True\n",
    "MIN_SAMPLES_PER_PERIOD = 500\n",
    "\n",
    "time_specific_models = {}\n",
    "\n",
    "if TRAIN_TIME_SPECIFIC_MODELS:\n",
    "    time_periods = {\n",
    "        'night': (0, 6),\n",
    "        'morning': (6, 12),\n",
    "        'afternoon': (12, 18),  # Highest errors\n",
    "        'evening': (18, 24)\n",
    "    }\n",
    "    \n",
    "    for horizon in ['1h', '4h']:\n",
    "        if horizon not in trained_models:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{horizon} Time-Specific Models:\")\n",
    "        \n",
    "        features = trained_models[horizon]['features']\n",
    "        global_model = trained_models[horizon]['model']\n",
    "        global_scaler = trained_models[horizon]['scaler']\n",
    "        global_mae = trained_models[horizon]['metrics']['mae']\n",
    "        \n",
    "        time_specific_models[horizon] = {}\n",
    "        \n",
    "        for period_name, (start_hour, end_hour) in time_periods.items():\n",
    "            # Filter data for this time period\n",
    "            train_hours = df_train_val.index.hour\n",
    "            train_mask = (train_hours >= start_hour) & (train_hours < end_hour)\n",
    "            \n",
    "            X_train_period = df_train_val.loc[train_mask, features]\n",
    "            y_train_period = df_train_val.loc[train_mask, f'target_{horizon}']\n",
    "            \n",
    "            # Remove NaN\n",
    "            valid_mask = y_train_period.notna()\n",
    "            X_train_period = X_train_period[valid_mask]\n",
    "            y_train_period = y_train_period[valid_mask]\n",
    "            \n",
    "            if len(X_train_period) < MIN_SAMPLES_PER_PERIOD:\n",
    "                print(f\"  {period_name}: Insufficient data ({len(X_train_period)} samples)\")\n",
    "                continue\n",
    "            \n",
    "            # Holdout data for this period\n",
    "            if HAS_HOLDOUT:\n",
    "                holdout_hours = df_holdout.index.hour\n",
    "                holdout_mask = (holdout_hours >= start_hour) & (holdout_hours < end_hour)\n",
    "                \n",
    "                X_holdout_period = df_holdout.loc[holdout_mask, features]\n",
    "                y_holdout_period = df_holdout.loc[holdout_mask, f'target_{horizon}']\n",
    "                \n",
    "                valid_mask_h = y_holdout_period.notna()\n",
    "                X_holdout_period = X_holdout_period[valid_mask_h]\n",
    "                y_holdout_period = y_holdout_period[valid_mask_h]\n",
    "            else:\n",
    "                # Split training data\n",
    "                split_idx = int(len(X_train_period) * 0.8)\n",
    "                X_holdout_period = X_train_period.iloc[split_idx:]\n",
    "                y_holdout_period = y_train_period.iloc[split_idx:]\n",
    "                X_train_period = X_train_period.iloc[:split_idx]\n",
    "                y_train_period = y_train_period.iloc[:split_idx]\n",
    "            \n",
    "            if len(X_holdout_period) < 50:\n",
    "                print(f\"  {period_name}: Insufficient holdout data\")\n",
    "                continue\n",
    "            \n",
    "            # Train a simple model for this period\n",
    "            scaler = RobustScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train_period)\n",
    "            X_holdout_scaled = scaler.transform(X_holdout_period)\n",
    "            \n",
    "            # Use GBM with moderate complexity\n",
    "            model = GradientBoostingRegressor(\n",
    "                n_estimators=30, max_depth=4, learning_rate=0.1,\n",
    "                min_samples_leaf=30, subsample=0.8, random_state=42\n",
    "            )\n",
    "            model.fit(X_train_scaled, y_train_period)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_holdout_scaled)\n",
    "            period_mae = mean_absolute_error(y_holdout_period, y_pred)\n",
    "            \n",
    "            # Compare to global model\n",
    "            global_pred = global_model.predict(global_scaler.transform(X_holdout_period))\n",
    "            global_period_mae = mean_absolute_error(y_holdout_period, global_pred)\n",
    "            \n",
    "            improvement = (global_period_mae - period_mae) / global_period_mae * 100\n",
    "            \n",
    "            print(f\"  {period_name}: Global MAE={global_period_mae:.4f}, Period MAE={period_mae:.4f} ({improvement:+.1f}%)\")\n",
    "            \n",
    "            # Only use period-specific model if it's better\n",
    "            if period_mae < global_period_mae:\n",
    "                time_specific_models[horizon][period_name] = {\n",
    "                    'model': model,\n",
    "                    'scaler': scaler,\n",
    "                    'mae': float(period_mae),\n",
    "                    'global_mae': float(global_period_mae),\n",
    "                    'improvement': float(improvement),\n",
    "                    'n_samples': len(X_train_period),\n",
    "                    'features': features\n",
    "                }\n",
    "                print(f\"    \u2713 Using period-specific model\")\n",
    "            else:\n",
    "                print(f\"    \u2717 Global model is better, skipping\")\n",
    "        \n",
    "        # Create time-adaptive predictor\n",
    "        if time_specific_models.get(horizon):\n",
    "            def create_time_adaptive_predictor(global_model, global_scaler, time_models, features):\n",
    "                def predict(X, hour=None):\n",
    "                    if hour is None:\n",
    "                        # Use global\n",
    "                        X_scaled = global_scaler.transform(X[features] if hasattr(X, 'columns') else X)\n",
    "                        return global_model.predict(X_scaled)\n",
    "                    \n",
    "                    # Determine period\n",
    "                    if 0 <= hour < 6:\n",
    "                        period = 'night'\n",
    "                    elif 6 <= hour < 12:\n",
    "                        period = 'morning'\n",
    "                    elif 12 <= hour < 18:\n",
    "                        period = 'afternoon'\n",
    "                    else:\n",
    "                        period = 'evening'\n",
    "                    \n",
    "                    # Use period-specific model if available\n",
    "                    if period in time_models:\n",
    "                        tm = time_models[period]\n",
    "                        X_scaled = tm['scaler'].transform(X[features] if hasattr(X, 'columns') else X)\n",
    "                        return tm['model'].predict(X_scaled)\n",
    "                    else:\n",
    "                        # Fall back to global\n",
    "                        X_scaled = global_scaler.transform(X[features] if hasattr(X, 'columns') else X)\n",
    "                        return global_model.predict(X_scaled)\n",
    "                \n",
    "                return predict\n",
    "            \n",
    "            trained_models[horizon]['time_adaptive_predict'] = create_time_adaptive_predictor(\n",
    "                global_model, global_scaler, time_specific_models[horizon], features\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n  \u2713 Time-adaptive predictor created with {len(time_specific_models[horizon])} period models\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TIME-SPECIFIC MODELS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for horizon, periods in time_specific_models.items():\n",
    "    if periods:\n",
    "        print(f\"\\n{horizon}:\")\n",
    "        for period, data in periods.items():\n",
    "            print(f\"  {period}: MAE={data['mae']:.4f} ({data['improvement']:+.1f}% vs global)\")\n",
    "    else:\n",
    "        print(f\"\\n{horizon}: No period-specific models (global is best)\")\n",
    "\n",
    "print(f\"\\n\u2713 Time-of-day model training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# PREDICTION INTERVALS - FIXED CALIBRATION on HOLDOUT DATA\n",
    "from sklearn.ensemble import GradientBoostingRegressor, IsolationForest\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING PREDICTION INTERVALS (HOLDOUT-CALIBRATED)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Tail risk caps - compute \u00b13\u03c3 bounds from rolling statistics\n",
    "tail_risk_caps = {}\n",
    "\n",
    "# Compute tail risk caps from training data for each horizon\n",
    "print(\"Computing tail risk caps (\u00b13\u03c3 bounds) for inference...\")\n",
    "for horizon in ['1h', '4h']:\n",
    "    target_col = f'target_{horizon}'\n",
    "    if target_col in df_features.columns:\n",
    "        # Use rolling statistics from training period\n",
    "        rolling_mean = df_features[target_col].rolling(24*12, min_periods=100).mean()  # 24h rolling\n",
    "        rolling_std = df_features[target_col].rolling(24*12, min_periods=100).std()\n",
    "\n",
    "        # Get latest (most recent) values for baseline\n",
    "        latest_mean = rolling_mean.dropna().iloc[-1] if rolling_mean.dropna().any() else df_features[target_col].mean()\n",
    "        latest_std = rolling_std.dropna().iloc[-1] if rolling_std.dropna().any() else df_features[target_col].std()\n",
    "\n",
    "        # Also compute global stats as fallback\n",
    "        global_mean = df_features[target_col].mean()\n",
    "        global_std = df_features[target_col].std()\n",
    "\n",
    "        # Cap bounds: use 3\u03c3 (99.7% coverage)\n",
    "        tail_risk_caps[horizon] = {\n",
    "            'rolling_mean': float(latest_mean),\n",
    "            'rolling_std': float(latest_std),\n",
    "            'lower_cap': float(max(0, latest_mean - 3 * latest_std)),\n",
    "            'upper_cap': float(latest_mean + 3 * latest_std),\n",
    "            'global_mean': float(global_mean),\n",
    "            'global_std': float(global_std),\n",
    "            'global_lower': float(max(0, global_mean - 3 * global_std)),\n",
    "            'global_upper': float(global_mean + 3 * global_std)\n",
    "        }\n",
    "        print(f\"  {horizon}: caps at [{tail_risk_caps[horizon]['lower_cap']:.4f}, {tail_risk_caps[horizon]['upper_cap']:.4f}]\")\n",
    "\n",
    "# 24h uses 4h caps\n",
    "tail_risk_caps['24h'] = tail_risk_caps.get('4h', {})\n",
    "print(\"\")\n",
    "\n",
    "quantile_models = {}\n",
    "conformal_residuals = {}\n",
    "uncertainty_scalers = {}\n",
    "time_period_calibration = {}\n",
    "\n",
    "def compute_holdout_conformal_interval(X_holdout, y_holdout, model, scaler, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Compute conformal interval on HOLDOUT data for proper calibration.\n",
    "    alpha=0.2 means 80% interval.\n",
    "    \"\"\"\n",
    "    X_scaled = scaler.transform(X_holdout)\n",
    "    y_pred = model.predict(X_scaled)\n",
    "    \n",
    "    residuals = np.abs(y_holdout.values - y_pred)\n",
    "    \n",
    "    # Use holdout residuals directly for calibration\n",
    "    q = np.quantile(residuals, 1 - alpha)\n",
    "    \n",
    "    # Verify coverage on holdout\n",
    "    coverage = np.mean(residuals <= q)\n",
    "    \n",
    "    return {\n",
    "        'quantile': q,\n",
    "        'residuals': residuals,\n",
    "        'coverage_target': 1 - alpha,\n",
    "        'actual_coverage': coverage,\n",
    "        'calibration_source': 'holdout'  # Flag that this is holdout-calibrated\n",
    "    }\n",
    "\n",
    "def compute_adaptive_conformal_intervals(X_holdout, y_holdout, model, scaler, hours, regimes=None):\n",
    "    \"\"\"\n",
    "    Compute separate conformal intervals for each time period and regime.\n",
    "    This ensures 80% coverage across all conditions.\n",
    "    \"\"\"\n",
    "    X_scaled = scaler.transform(X_holdout)\n",
    "    y_pred = model.predict(X_scaled)\n",
    "    residuals = np.abs(y_holdout.values - y_pred)\n",
    "    \n",
    "    # Time-period specific intervals\n",
    "    time_intervals = {}\n",
    "    time_periods = {\n",
    "        'night': (0, 6),\n",
    "        'morning': (6, 12),\n",
    "        'afternoon': (12, 18),\n",
    "        'evening': (18, 24)\n",
    "    }\n",
    "    \n",
    "    for period_name, (start, end) in time_periods.items():\n",
    "        mask = (hours >= start) & (hours < end)\n",
    "        if mask.sum() >= 50:\n",
    "            period_residuals = residuals[mask]\n",
    "            q80 = np.quantile(period_residuals, 0.8)\n",
    "            q90 = np.quantile(period_residuals, 0.9)\n",
    "            \n",
    "            # Calculate multiplier relative to overall\n",
    "            overall_q80 = np.quantile(residuals, 0.8)\n",
    "            multiplier = q80 / overall_q80 if overall_q80 > 0 else 1.0\n",
    "            \n",
    "            # Check if night needs wider intervals - FORCE minimum 1.5x multiplier\n",
    "            if period_name == 'night':\n",
    "                actual_coverage = np.mean(period_residuals <= q80)\n",
    "                # Always apply minimum 1.5x multiplier at night regardless of coverage\n",
    "                NIGHT_MIN_MULTIPLIER = 1.5\n",
    "                if multiplier < NIGHT_MIN_MULTIPLIER:\n",
    "                    old_mult = multiplier\n",
    "                    multiplier = NIGHT_MIN_MULTIPLIER\n",
    "                    q80 = q80 * (NIGHT_MIN_MULTIPLIER / old_mult) if old_mult > 0 else q80 * NIGHT_MIN_MULTIPLIER\n",
    "                    q90 = q90 * (NIGHT_MIN_MULTIPLIER / old_mult) if old_mult > 0 else q90 * NIGHT_MIN_MULTIPLIER\n",
    "                    print(f\"    Night: forced minimum {NIGHT_MIN_MULTIPLIER}x multiplier (was {old_mult:.2f}x)\")\n",
    "                # Additional coverage-based adjustment if still under target\n",
    "                if actual_coverage < 0.78:\n",
    "                    coverage_gap = 0.80 - actual_coverage\n",
    "                    extra_factor = 1 + coverage_gap * 1.5  # More aggressive scaling\n",
    "                    q80 = q80 * extra_factor\n",
    "                    q90 = q90 * extra_factor\n",
    "                    multiplier = multiplier * extra_factor\n",
    "                    print(f\"    Night coverage fix: additional {(extra_factor-1)*100:.0f}% widening\")\n",
    "\n",
    "            time_intervals[period_name] = {\n",
    "                'interval_80': float(q80),\n",
    "                'interval_90': float(q90),\n",
    "                'multiplier': float(max(multiplier, 1.0)),  # Never shrink\n",
    "                'mae': float(np.mean(period_residuals)),\n",
    "                'n_samples': int(mask.sum()),\n",
    "                'actual_coverage_80': float(np.mean(period_residuals <= q80))\n",
    "            }\n",
    "    \n",
    "    # Regime-specific intervals (if provided)\n",
    "    regime_intervals = {}\n",
    "    if regimes is not None:\n",
    "        for regime_val, regime_name in [(0, 'normal'), (1, 'elevated'), (2, 'spike')]:\n",
    "            mask = regimes == regime_val\n",
    "            if mask.sum() >= 50:\n",
    "                regime_residuals = residuals[mask]\n",
    "                q80 = np.quantile(regime_residuals, 0.8)\n",
    "                q90 = np.quantile(regime_residuals, 0.9)\n",
    "                \n",
    "                overall_q80 = np.quantile(residuals, 0.8)\n",
    "                multiplier = q80 / overall_q80 if overall_q80 > 0 else 1.0\n",
    "                \n",
    "                regime_intervals[regime_name] = {\n",
    "                    'interval_80': float(q80),\n",
    "                    'interval_90': float(q90),\n",
    "                    'multiplier': float(max(multiplier, 1.0)),\n",
    "                    'mae': float(np.mean(regime_residuals)),\n",
    "                    'n_samples': int(mask.sum()),\n",
    "                    'actual_coverage_80': float(np.mean(regime_residuals <= q80))\n",
    "                }\n",
    "    \n",
    "    return time_intervals, regime_intervals\n",
    "\n",
    "def train_uncertainty_scaler(X_train, scaler, residuals, features):\n",
    "    \"\"\"Train OOD detector for uncertainty scaling\"\"\"\n",
    "    X_scaled = scaler.transform(X_train)\n",
    "    \n",
    "    iso_forest = IsolationForest(\n",
    "        n_estimators=50, contamination=0.1,\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    iso_forest.fit(X_scaled)\n",
    "    \n",
    "    feature_means = X_scaled.mean(axis=0)\n",
    "    feature_stds = X_scaled.std(axis=0) + 1e-8\n",
    "    base_interval = np.quantile(residuals, 0.8)\n",
    "    \n",
    "    return {\n",
    "        'iso_forest': iso_forest,\n",
    "        'feature_means': feature_means,\n",
    "        'feature_stds': feature_stds,\n",
    "        'base_interval': base_interval,\n",
    "        'features': features\n",
    "    }\n",
    "\n",
    "def get_adaptive_interval(base_interval, hour=None, regime=None, time_intervals=None, regime_intervals=None):\n",
    "    \"\"\"Get the appropriate interval width for given conditions\"\"\"\n",
    "    multiplier = 1.0\n",
    "    \n",
    "    # Time-based adjustment\n",
    "    if hour is not None and time_intervals:\n",
    "        period = None\n",
    "        if 0 <= hour < 6:\n",
    "            period = 'night'\n",
    "        elif 6 <= hour < 12:\n",
    "            period = 'morning'\n",
    "        elif 12 <= hour < 18:\n",
    "            period = 'afternoon'\n",
    "        else:\n",
    "            period = 'evening'\n",
    "        \n",
    "        if period in time_intervals:\n",
    "            multiplier = max(multiplier, time_intervals[period]['multiplier'])\n",
    "    \n",
    "    # Regime-based adjustment\n",
    "    if regime is not None and regime_intervals:\n",
    "        regime_name = {0: 'normal', 1: 'elevated', 2: 'spike'}.get(regime)\n",
    "        if regime_name in regime_intervals:\n",
    "            multiplier = max(multiplier, regime_intervals[regime_name]['multiplier'])\n",
    "    \n",
    "    return base_interval * multiplier\n",
    "\n",
    "for horizon in ['1h', '4h']:\n",
    "    if horizon not in trained_models:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n{horizon} prediction intervals...\")\n",
    "    \n",
    "    data = trained_models[horizon]\n",
    "    model = data['model']\n",
    "    scaler = data['scaler']\n",
    "    features = data['features']\n",
    "    \n",
    "    # === Use HOLDOUT data for calibration ===\n",
    "    if not HAS_HOLDOUT:\n",
    "        print(f\"  \u26a0\ufe0f No holdout data, using training data (may be miscalibrated)\")\n",
    "        X_cal = df_train_val[features]\n",
    "        y_cal = df_train_val[f'target_{horizon}']\n",
    "    else:\n",
    "        X_cal = df_holdout[features]\n",
    "        y_cal = df_holdout[f'target_{horizon}']\n",
    "    \n",
    "    mask = y_cal.notna()\n",
    "    X_cal = X_cal[mask]\n",
    "    y_cal = y_cal[mask]\n",
    "    \n",
    "    if len(X_cal) < 500:\n",
    "        print(f\"  \u26a0\ufe0f Insufficient data for {horizon} intervals, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Get hours and regimes for adaptive calibration\n",
    "    hours = X_cal.index.hour if hasattr(X_cal.index, 'hour') else pd.Series(12, index=X_cal.index)\n",
    "    \n",
    "    regimes = pd.Series(0, index=X_cal.index)\n",
    "    if 'gas_zscore_1h' in df_holdout.columns if HAS_HOLDOUT else df_train_val.columns:\n",
    "        source_df = df_holdout if HAS_HOLDOUT else df_train_val\n",
    "        regimes[source_df.loc[X_cal.index, 'gas_zscore_1h'] > 1] = 1\n",
    "    if 'is_spike' in df_holdout.columns if HAS_HOLDOUT else df_train_val.columns:\n",
    "        source_df = df_holdout if HAS_HOLDOUT else df_train_val\n",
    "        regimes[source_df.loc[X_cal.index, 'is_spike'] == 1] = 2\n",
    "    \n",
    "    # === Compute HOLDOUT-CALIBRATED conformal intervals ===\n",
    "    conformal = compute_holdout_conformal_interval(X_cal, y_cal, model, scaler, alpha=0.2)\n",
    "    conformal_residuals[horizon] = conformal\n",
    "    print(f\"  \u2713 Conformal interval (holdout-calibrated): \u00b1{conformal['quantile']:.4f}\")\n",
    "    print(f\"    Actual coverage on holdout: {conformal['actual_coverage']:.1%}\")\n",
    "    \n",
    "    # === Compute ADAPTIVE intervals for time/regime ===\n",
    "    time_intervals, regime_intervals = compute_adaptive_conformal_intervals(\n",
    "        X_cal, y_cal, model, scaler, hours.values, regimes.values\n",
    "    )\n",
    "    time_period_calibration[horizon] = {\n",
    "        'time': time_intervals,\n",
    "        'regime': regime_intervals\n",
    "    }\n",
    "    \n",
    "    print(f\"  \u2713 Time-adaptive intervals computed:\")\n",
    "    for period, info in time_intervals.items():\n",
    "        print(f\"    {period}: \u00b1{info['interval_80']:.4f} (coverage: {info['actual_coverage_80']:.1%})\")\n",
    "    \n",
    "    # === Train quantile models on training data ===\n",
    "    X_train_q = df_train_val[features]\n",
    "    y_train_q = df_train_val[f'target_{horizon}']\n",
    "    mask_q = y_train_q.notna()\n",
    "    X_train_q = X_train_q[mask_q]\n",
    "    y_train_q = y_train_q[mask_q]\n",
    "    \n",
    "    split_idx = int(len(X_train_q) * 0.8)\n",
    "    X_train, X_test = X_train_q.iloc[:split_idx], X_train_q.iloc[split_idx:]\n",
    "    y_train, y_test = y_train_q.iloc[:split_idx], y_train_q.iloc[split_idx:]\n",
    "    \n",
    "    q_scaler = RobustScaler()\n",
    "    X_train_scaled = q_scaler.fit_transform(X_train)\n",
    "    \n",
    "    q_models = {}\n",
    "    for q in [0.1, 0.5, 0.9]:\n",
    "        qmodel = GradientBoostingRegressor(\n",
    "            loss='quantile', alpha=q,\n",
    "            n_estimators=50, max_depth=4,\n",
    "            learning_rate=0.1, random_state=42\n",
    "        )\n",
    "        qmodel.fit(X_train_scaled, y_train)\n",
    "        q_models[q] = qmodel\n",
    "    \n",
    "    quantile_models[horizon] = (q_models, q_scaler)\n",
    "    print(f\"  \u2713 Quantile models trained\")\n",
    "    \n",
    "    # === Train uncertainty scaler ===\n",
    "    unc_scaler = train_uncertainty_scaler(X_train, q_scaler, conformal['residuals'], features)\n",
    "    uncertainty_scalers[horizon] = unc_scaler\n",
    "    print(f\"  \u2713 Uncertainty scaler trained\")\n",
    "    \n",
    "    # === Verify calibration on holdout ===\n",
    "    X_test_scaled = scaler.transform(X_cal)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Standard conformal coverage\n",
    "    abs_errors = np.abs(y_cal.values - y_pred)\n",
    "    conf_coverage = np.mean(abs_errors <= conformal['quantile'])\n",
    "    \n",
    "    # Adaptive coverage (using time/regime specific intervals)\n",
    "    adaptive_coverages = []\n",
    "    for i, (idx, row) in enumerate(X_cal.iterrows()):\n",
    "        h = hours.iloc[i] if hasattr(hours, 'iloc') else hours[i]\n",
    "        r = regimes.iloc[i] if hasattr(regimes, 'iloc') else regimes[i]\n",
    "        \n",
    "        adaptive_interval = get_adaptive_interval(\n",
    "            conformal['quantile'], hour=h, regime=r,\n",
    "            time_intervals=time_intervals, regime_intervals=regime_intervals\n",
    "        )\n",
    "        adaptive_coverages.append(abs_errors[i] <= adaptive_interval)\n",
    "    \n",
    "    adaptive_coverage = np.mean(adaptive_coverages)\n",
    "    \n",
    "    print(f\"\\n  Calibration Verification (on holdout):\")\n",
    "    print(f\"    Conformal 80% interval: actual = {conf_coverage:.1%}\")\n",
    "    print(f\"    Adaptive 80% interval: actual = {adaptive_coverage:.1%}\")\n",
    "    \n",
    "    # Store calibration in model data\n",
    "    trained_models[horizon]['calibration'] = {\n",
    "        'conformal_coverage': float(conf_coverage),\n",
    "        'adaptive_coverage': float(adaptive_coverage),\n",
    "        'conformal_width': float(conformal['quantile']),\n",
    "        'time_intervals': time_intervals,\n",
    "        'regime_intervals': regime_intervals,\n",
    "        'calibration_source': 'holdout'\n",
    "    }\n",
    "    \n",
    "    if abs(conf_coverage - 0.8) > 0.1:\n",
    "        print(f\"    \u26a0\ufe0f Conformal interval may need adjustment\")\n",
    "\n",
    "# Copy 4h to 24h\n",
    "if '4h' in quantile_models:\n",
    "    quantile_models['24h'] = quantile_models['4h']\n",
    "    print(\"\\n24h: Using 4h quantile models\")\n",
    "\n",
    "if '4h' in conformal_residuals:\n",
    "    conformal_residuals['24h'] = conformal_residuals['4h']\n",
    "\n",
    "if '4h' in uncertainty_scalers:\n",
    "    uncertainty_scalers['24h'] = uncertainty_scalers['4h']\n",
    "\n",
    "if '4h' in time_period_calibration:\n",
    "    time_period_calibration['24h'] = time_period_calibration['4h']\n",
    "\n",
    "print(f\"\\n\u2713 Prediction intervals (holdout-calibrated) ready for: {list(quantile_models.keys())}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI-REGIME CALIBRATION VERIFICATION\n",
    "# Verify that prediction intervals are well-calibrated across different conditions\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-REGIME CALIBRATION VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "regime_calibration = {}\n",
    "\n",
    "def verify_calibration_by_regime(X, y, model, scaler, conformal_quantile, \n",
    "                                  regime_labels, time_calibration=None, \n",
    "                                  uncertainty_scaler=None):\n",
    "    \"\"\"\n",
    "    Verify prediction interval coverage for each regime and time period.\n",
    "    Returns detailed calibration metrics.\n",
    "    \"\"\"\n",
    "    X_scaled = scaler.transform(X)\n",
    "    y_pred = model.predict(X_scaled)\n",
    "    errors = np.abs(y.values - y_pred)\n",
    "    \n",
    "    results = {\n",
    "        'overall': {},\n",
    "        'by_regime': {},\n",
    "        'by_time': {},\n",
    "        'warnings': []\n",
    "    }\n",
    "    \n",
    "    # Overall coverage at different interval widths\n",
    "    for coverage_target in [0.8, 0.9, 0.95]:\n",
    "        interval_width = np.quantile(errors, coverage_target)\n",
    "        actual_coverage = np.mean(errors <= interval_width)\n",
    "        results['overall'][f'coverage_{int(coverage_target*100)}'] = {\n",
    "            'target': coverage_target,\n",
    "            'actual': float(actual_coverage),\n",
    "            'interval_width': float(interval_width),\n",
    "            'calibrated': abs(actual_coverage - coverage_target) < 0.05\n",
    "        }\n",
    "    \n",
    "    # Coverage by regime\n",
    "    for regime_val, regime_name in [(0, 'normal'), (1, 'elevated'), (2, 'spike')]:\n",
    "        mask = regime_labels == regime_val\n",
    "        if mask.sum() < 50:\n",
    "            continue\n",
    "        \n",
    "        regime_errors = errors[mask]\n",
    "        regime_y = y.values[mask]\n",
    "        regime_pred = y_pred[mask]\n",
    "        \n",
    "        # Standard conformal coverage\n",
    "        in_interval = regime_errors <= conformal_quantile\n",
    "        regime_coverage = np.mean(in_interval)\n",
    "        \n",
    "        # Adaptive coverage (if time calibration available)\n",
    "        if time_calibration and uncertainty_scaler:\n",
    "            hours = X.index.hour[mask] if hasattr(X.index, 'hour') else np.full(mask.sum(), 12)\n",
    "            adaptive_coverages = []\n",
    "            for i, (h, err) in enumerate(zip(hours, regime_errors)):\n",
    "                period = 'afternoon' if 12 <= h < 18 else ('morning' if 6 <= h < 12 else ('evening' if 18 <= h < 24 else 'night'))\n",
    "                mult = time_calibration.get(period, {}).get('multiplier', 1.0)\n",
    "                adapted_interval = conformal_quantile * mult\n",
    "                adaptive_coverages.append(err <= adapted_interval)\n",
    "            adaptive_coverage = np.mean(adaptive_coverages)\n",
    "        else:\n",
    "            adaptive_coverage = regime_coverage\n",
    "        \n",
    "        results['by_regime'][regime_name] = {\n",
    "            'n_samples': int(mask.sum()),\n",
    "            'mae': float(np.mean(regime_errors)),\n",
    "            'std': float(np.std(regime_errors)),\n",
    "            'conformal_coverage': float(regime_coverage),\n",
    "            'adaptive_coverage': float(adaptive_coverage),\n",
    "            'target_coverage': 0.8,\n",
    "            'calibrated': abs(regime_coverage - 0.8) < 0.1\n",
    "        }\n",
    "        \n",
    "        if regime_coverage < 0.7:\n",
    "            results['warnings'].append(f\"{regime_name} regime under-covered: {regime_coverage:.1%} (target 80%)\")\n",
    "    \n",
    "    # Coverage by time period\n",
    "    hours = X.index.hour if hasattr(X.index, 'hour') else pd.Series(12, index=X.index)\n",
    "    time_periods = {\n",
    "        'night': (0, 6),\n",
    "        'morning': (6, 12),\n",
    "        'afternoon': (12, 18),\n",
    "        'evening': (18, 24)\n",
    "    }\n",
    "    \n",
    "    for period_name, (start, end) in time_periods.items():\n",
    "        mask = (hours >= start) & (hours < end)\n",
    "        if mask.sum() < 50:\n",
    "            continue\n",
    "        \n",
    "        period_errors = errors[mask]\n",
    "        \n",
    "        # Standard conformal coverage\n",
    "        in_interval = period_errors <= conformal_quantile\n",
    "        period_coverage = np.mean(in_interval)\n",
    "        \n",
    "        # Adaptive coverage\n",
    "        if time_calibration and period_name in time_calibration:\n",
    "            mult = time_calibration[period_name].get('multiplier', 1.0)\n",
    "            adapted_interval = conformal_quantile * mult\n",
    "            adaptive_coverage = np.mean(period_errors <= adapted_interval)\n",
    "        else:\n",
    "            adaptive_coverage = period_coverage\n",
    "        \n",
    "        results['by_time'][period_name] = {\n",
    "            'n_samples': int(mask.sum()),\n",
    "            'mae': float(np.mean(period_errors)),\n",
    "            'conformal_coverage': float(period_coverage),\n",
    "            'adaptive_coverage': float(adaptive_coverage),\n",
    "            'calibrated': abs(adaptive_coverage - 0.8) < 0.1\n",
    "        }\n",
    "        \n",
    "        if period_coverage < 0.7:\n",
    "            results['warnings'].append(f\"{period_name} period under-covered: {period_coverage:.1%} (target 80%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "for horizon in ['1h', '4h']:\n",
    "    if horizon not in trained_models:\n",
    "        continue\n",
    "    if horizon not in conformal_residuals:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{horizon} Calibration Verification\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    data = trained_models[horizon]\n",
    "    model = data['model']\n",
    "    scaler = data['scaler']\n",
    "    features = data['features']\n",
    "    conformal = conformal_residuals[horizon]\n",
    "    time_cal = time_period_calibration.get(horizon, {})\n",
    "    unc_scaler = uncertainty_scalers.get(horizon, None)\n",
    "    \n",
    "    if not HAS_HOLDOUT:\n",
    "        print(\"  \u26a0\ufe0f No holdout data for calibration verification\")\n",
    "        continue\n",
    "    \n",
    "    # Get holdout data\n",
    "    X_test = df_holdout[features]\n",
    "    y_test = df_holdout[f'target_{horizon}']\n",
    "    mask = y_test.notna()\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    \n",
    "    # Create regime labels\n",
    "    regime_test = pd.Series(0, index=X_test.index)\n",
    "    if 'gas_zscore_1h' in df_holdout.columns:\n",
    "        regime_test[df_holdout.loc[X_test.index, 'gas_zscore_1h'] > 1] = 1\n",
    "    if 'is_spike' in df_holdout.columns:\n",
    "        regime_test[df_holdout.loc[X_test.index, 'is_spike'] == 1] = 2\n",
    "    \n",
    "    # Verify calibration\n",
    "    cal_results = verify_calibration_by_regime(\n",
    "        X_test, y_test, model, scaler,\n",
    "        conformal['quantile'], regime_test.values,\n",
    "        time_cal, unc_scaler\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n  Overall Calibration:\")\n",
    "    for level, metrics in cal_results['overall'].items():\n",
    "        status = \"\u2713\" if metrics['calibrated'] else \"\u26a0\"\n",
    "        print(f\"    {status} {int(metrics['target']*100)}% target: actual={metrics['actual']:.1%}, width={metrics['interval_width']:.4f}\")\n",
    "    \n",
    "    print(\"\\n  Calibration by Regime:\")\n",
    "    for regime_name, metrics in cal_results['by_regime'].items():\n",
    "        status = \"\u2713\" if metrics['calibrated'] else \"\u26a0\"\n",
    "        print(f\"    {status} {regime_name}: conformal={metrics['conformal_coverage']:.1%}, adaptive={metrics['adaptive_coverage']:.1%} ({metrics['n_samples']} samples)\")\n",
    "    \n",
    "    print(\"\\n  Calibration by Time Period:\")\n",
    "    for period_name, metrics in cal_results['by_time'].items():\n",
    "        status = \"\u2713\" if metrics['calibrated'] else \"\u26a0\"\n",
    "        print(f\"    {status} {period_name}: conformal={metrics['conformal_coverage']:.1%}, adaptive={metrics['adaptive_coverage']:.1%}\")\n",
    "    \n",
    "    if cal_results['warnings']:\n",
    "        print(\"\\n  \u26a0\ufe0f Calibration Warnings:\")\n",
    "        for warning in cal_results['warnings']:\n",
    "            print(f\"    - {warning}\")\n",
    "    \n",
    "    # Store results\n",
    "    regime_calibration[horizon] = cal_results\n",
    "    \n",
    "    # Update trained_models with detailed calibration\n",
    "    if 'calibration' not in trained_models[horizon]:\n",
    "        trained_models[horizon]['calibration'] = {}\n",
    "    trained_models[horizon]['calibration']['regime_breakdown'] = cal_results['by_regime']\n",
    "    trained_models[horizon]['calibration']['time_breakdown'] = cal_results['by_time']\n",
    "    trained_models[horizon]['calibration']['warnings'] = cal_results['warnings']\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CALIBRATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_calibrated = True\n",
    "for horizon, cal in regime_calibration.items():\n",
    "    n_warnings = len(cal['warnings'])\n",
    "    if n_warnings == 0:\n",
    "        print(f\"  \u2713 {horizon}: All regimes and time periods well-calibrated\")\n",
    "    else:\n",
    "        all_calibrated = False\n",
    "        print(f\"  \u26a0 {horizon}: {n_warnings} calibration warning(s)\")\n",
    "\n",
    "if all_calibrated:\n",
    "    print(\"\\n\u2713 Prediction intervals are well-calibrated across all conditions\")\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f Some conditions show poor calibration - consider regime-specific intervals\")\n",
    "\n",
    "print(f\"\\n\u2713 Multi-regime calibration verification complete\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Direction Prediction - IMPROVED\n",
    "# Changes: Binary up/down, class weights, holdout evaluation, adaptive threshold\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING DIRECTION MODELS (IMPROVED)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "direction_models = {}\n",
    "\n",
    "# Configuration\n",
    "USE_BINARY = True  # Binary (up/down) vs 3-class (down/stable/up)\n",
    "DIRECTION_THRESHOLD = 0.01  # 1% threshold for direction change\n",
    "\n",
    "def create_binary_direction(target, current, threshold=0.01):\n",
    "    \"\"\"Create binary direction labels: 1=up, 0=down/stable\"\"\"\n",
    "    pct_change = (target - current) / (current + 1e-8)\n",
    "    return (pct_change > threshold).astype(int)\n",
    "\n",
    "def create_ternary_direction(target, current, threshold=0.02):\n",
    "    \"\"\"Create 3-class direction labels\"\"\"\n",
    "    pct_change = (target - current) / (current + 1e-8)\n",
    "    direction = pd.Series('stable', index=target.index)\n",
    "    direction[pct_change > threshold] = 'up'\n",
    "    direction[pct_change < -threshold] = 'down'\n",
    "    return direction\n",
    "\n",
    "for horizon in ['1h', '4h']:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{horizon.upper()} DIRECTION MODEL\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Get features and targets\n",
    "    X_h = X_1h if horizon == '1h' else X_4h\n",
    "    features = numeric_features_1h if horizon == '1h' else numeric_features_4h\n",
    "    \n",
    "    # Get raw target for direction calculation\n",
    "    if 'target_1h_raw' in df_train_val.columns:\n",
    "        target_raw = df_train_val[f'target_{horizon}_raw']\n",
    "    else:\n",
    "        target_raw = df_train_val[f'target_{horizon}']\n",
    "    \n",
    "    current = df_train_val['gas']\n",
    "    \n",
    "    # Create direction labels\n",
    "    if USE_BINARY:\n",
    "        y_dir = create_binary_direction(target_raw, current, DIRECTION_THRESHOLD)\n",
    "        print(f\"Binary classification (threshold: {DIRECTION_THRESHOLD*100}%)\")\n",
    "    else:\n",
    "        y_dir = create_ternary_direction(target_raw, current)\n",
    "        print(f\"3-class classification (threshold: {DIRECTION_THRESHOLD*100}%)\")\n",
    "    \n",
    "    mask = y_dir.notna() & target_raw.notna()\n",
    "    X_d = X_h[mask]\n",
    "    y_d = y_dir[mask]\n",
    "    \n",
    "    if len(X_d) < 1000:\n",
    "        print(f\"  \u26a0\ufe0f Insufficient data, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Class distribution\n",
    "    class_counts = y_d.value_counts()\n",
    "    print(f\"Class distribution: {dict(class_counts)}\")\n",
    "    \n",
    "    # Compute class weights\n",
    "    classes = np.unique(y_d)\n",
    "    weights = compute_class_weight('balanced', classes=classes, y=y_d)\n",
    "    class_weight_dict = dict(zip(classes, weights))\n",
    "    print(f\"Class weights: {class_weight_dict}\")\n",
    "    \n",
    "    # Split - use holdout if available\n",
    "    if HAS_HOLDOUT:\n",
    "        X_train, X_test = X_d, df_holdout[features]\n",
    "        y_train = y_d\n",
    "        \n",
    "        # Create holdout labels\n",
    "        if 'target_1h_raw' in df_holdout.columns:\n",
    "            holdout_target = df_holdout[f'target_{horizon}_raw']\n",
    "        else:\n",
    "            holdout_target = df_holdout[f'target_{horizon}']\n",
    "        holdout_current = df_holdout['gas']\n",
    "        \n",
    "        if USE_BINARY:\n",
    "            y_test = create_binary_direction(holdout_target, holdout_current, DIRECTION_THRESHOLD)\n",
    "        else:\n",
    "            y_test = create_ternary_direction(holdout_target, holdout_current)\n",
    "        \n",
    "        test_mask = y_test.notna() & holdout_target.notna()\n",
    "        X_test = X_test[test_mask]\n",
    "        y_test = y_test[test_mask]\n",
    "        print(f\"Using holdout for evaluation ({len(y_test)} samples)\")\n",
    "    else:\n",
    "        split_idx = int(len(X_d) * 0.8)\n",
    "        X_train, X_test = X_d.iloc[:split_idx], X_d.iloc[split_idx:]\n",
    "        y_train, y_test = y_d.iloc[:split_idx], y_d.iloc[split_idx:]\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Try multiple classifiers\n",
    "    classifiers = [\n",
    "        ('LogReg', LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)),\n",
    "        ('RF', RandomForestClassifier(n_estimators=30, max_depth=4, class_weight='balanced', random_state=42, n_jobs=-1)),\n",
    "        ('GBM', GradientBoostingClassifier(n_estimators=30, max_depth=3, learning_rate=0.1, random_state=42)),\n",
    "    ]\n",
    "    \n",
    "    best_clf = None\n",
    "    best_acc = 0\n",
    "    best_name = None\n",
    "    \n",
    "    for name, clf in classifiers:\n",
    "        try:\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            y_pred = clf.predict(X_test_scaled)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "            print(f\"  {name}: Acc={acc:.1%}, F1={f1:.3f}\")\n",
    "            \n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_clf = clf\n",
    "                best_name = name\n",
    "                best_f1 = f1\n",
    "        except Exception as e:\n",
    "            print(f\"  {name}: Failed - {e}\")\n",
    "    \n",
    "    if best_clf is None:\n",
    "        print(f\"  \u26a0\ufe0f All classifiers failed\")\n",
    "        continue\n",
    "    \n",
    "    # Baseline: always predict majority class\n",
    "    majority_class = y_train.mode()[0]\n",
    "    baseline_acc = (y_test == majority_class).mean()\n",
    "    improvement = (best_acc - baseline_acc) / baseline_acc * 100\n",
    "    \n",
    "    print(f\"\\n  >>> Best: {best_name} (Acc: {best_acc:.1%}, vs baseline {baseline_acc:.1%}: {improvement:+.1f}%)\")\n",
    "    \n",
    "    direction_models[horizon] = {\n",
    "        'model': best_clf,\n",
    "        'scaler': scaler,\n",
    "        'accuracy': float(best_acc),\n",
    "        'f1_score': float(best_f1),\n",
    "        'baseline_accuracy': float(baseline_acc),\n",
    "        'improvement_vs_baseline': float(improvement),\n",
    "        'model_name': best_name,\n",
    "        'is_binary': USE_BINARY,\n",
    "        'threshold': DIRECTION_THRESHOLD\n",
    "    }\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DIRECTION MODEL SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "for horizon, data in direction_models.items():\n",
    "    imp = data['improvement_vs_baseline']\n",
    "    status = \"\u2713\" if imp > 5 else \"\u26a0\" if imp > 0 else \"\u2717\"\n",
    "    print(f\"{status} {horizon}: {data['model_name']} | Acc: {data['accuracy']:.1%} | vs baseline: {imp:+.1f}%\")\n",
    ""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# REGIME DETECTION\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING REGIME DETECTION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create regime labels from gas statistics (instead of volatility_regime)\n",
    "# 0 = Normal, 1 = Elevated, 2 = Spike\n",
    "if 'gas_zscore_1h' in df_train_val.columns and 'is_spike' in df_train_val.columns:\n",
    "    # Create regime from z-score: low (<-0.5), normal (-0.5 to 1), elevated (1 to 2), spike (>2)\n",
    "    zscore = df_train_val['gas_zscore_1h']\n",
    "    is_spike = df_train_val['is_spike']\n",
    "    \n",
    "    regime_labels = pd.Series(0, index=df_train_val.index)  # Default: Normal\n",
    "    regime_labels[zscore > 1] = 1  # Elevated\n",
    "    regime_labels[is_spike == 1] = 2  # Spike\n",
    "    \n",
    "    X_r = X_4h.copy()\n",
    "    y_r = regime_labels\n",
    "    \n",
    "    if len(X_r) < 500:\n",
    "        print(\"\u26a0\ufe0f Insufficient data for regime detection\")\n",
    "        regime_clf = None\n",
    "        regime_scaler = None\n",
    "        regime_accuracy = 0\n",
    "    else:\n",
    "        # Train/test split\n",
    "        split_idx = int(len(X_r) * 0.8)\n",
    "        X_train, X_test = X_r.iloc[:split_idx], X_r.iloc[split_idx:]\n",
    "        y_train, y_test = y_r.iloc[:split_idx], y_r.iloc[split_idx:]\n",
    "        \n",
    "        regime_scaler = RobustScaler()\n",
    "        X_train_scaled = regime_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = regime_scaler.transform(X_test)\n",
    "        \n",
    "        # Train classifier (simple, reduced complexity)\n",
    "        regime_clf = RandomForestClassifier(\n",
    "            n_estimators=30, max_depth=4,\n",
    "            min_samples_leaf=20,\n",
    "            random_state=42, n_jobs=-1\n",
    "        )\n",
    "        regime_clf.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = regime_clf.predict(X_test_scaled)\n",
    "        regime_accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Regime classes: Normal (0), Elevated (1), Spike (2)\")\n",
    "        print(f\"Class distribution: {dict(y_r.value_counts().sort_index())}\")\n",
    "        print(f\"Accuracy: {regime_accuracy:.1%}\")\n",
    "        \n",
    "        if regime_accuracy > 0.95:\n",
    "            print(\"\u26a0\ufe0f Warning: Very high accuracy may indicate class imbalance or overfitting\")\n",
    "else:\n",
    "    regime_clf = None\n",
    "    regime_scaler = None\n",
    "    regime_accuracy = 0\n",
    "    print(\"\u26a0\ufe0f Missing gas_zscore_1h or is_spike, skipping regime detection\")\n",
    ""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Spike Detectors\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nprint(\"\\n",
    "\" + \"=\"*60)\nprint(\"TRAINING SPIKE DETECTORS\")\nprint(\"=\"*60)\n\nspike_models = {}\n\nfor horizon, X_h, y_target in [('1h', X_1h, y_1h), ('4h', X_4h, y_4h)]:\n    print(f\"\\n",
    "{horizon} spike detector...\")\n    \n    # Create spike labels (>2 std from mean is a spike)\n    mask = y_target.notna()\n    X_s = X_h[mask]\n    y_s = y_target[mask]\n    current = current_gas[mask]\n    \n    # Define spike threshold\n    price_change = y_s - current\n    threshold = price_change.std() * 2\n    spike_labels = (price_change > threshold).astype(int)\n    \n    spike_rate = spike_labels.mean()\n    print(f\"  Spike rate: {spike_rate:.1%}\")\n    \n    if spike_rate < 0.01 or spike_rate > 0.5:\n        print(f\"  \u26a0\ufe0f Unusual spike rate, skipping\")\n        continue\n    \n    if len(X_s) < 1000:\n        print(f\"  \u26a0\ufe0f Insufficient data, skipping\")\n        continue\n    \n    # Train/test split\n    split_idx = int(len(X_s) * 0.8)\n    X_train, X_test = X_s.iloc[:split_idx], X_s.iloc[split_idx:]\n    y_train, y_test = spike_labels.iloc[:split_idx], spike_labels.iloc[split_idx:]\n    \n    scaler = RobustScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Train with class weights\n    clf = GradientBoostingClassifier(\n        n_estimators=50, max_depth=4,\n        learning_rate=0.1, random_state=42\n    )\n    clf.fit(X_train_scaled, y_train)\n    \n    # Evaluate\n    y_pred = clf.predict(X_test_scaled)\n    acc = accuracy_score(y_test, y_pred)\n    \n    spike_models[horizon] = (clf, scaler)\n    print(f\"  Accuracy: {acc:.1%}\")\n\n# Copy 4h to 24h if available\nif '4h' in spike_models:\n    spike_models['24h'] = spike_models['4h']\n    print(\"\\n",
    "24h: Using 4h spike detector (fallback)\")\n\nprint(f\"\\n",
    "\u2713 Spike detectors trained for: {list(spike_models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# SPIKE FORECASTING - IMPROVED with Multiple Definitions & Volatility Features\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SPIKE FORECASTING (IMPROVED)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for SMOTE\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    HAS_SMOTE = True\n",
    "except ImportError:\n",
    "    HAS_SMOTE = False\n",
    "\n",
    "spike_forecast_models = {}\n",
    "\n",
    "def create_volatility_features(df):\n",
    "    \"\"\"\n",
    "    Create comprehensive volatility and regime-transition features.\n",
    "    Focus on patterns that precede spikes, not just rate-of-change.\n",
    "    \"\"\"\n",
    "    rph = 120  # rows per hour\n",
    "    features = {}\n",
    "    \n",
    "    # === VOLATILITY CLUSTERING (GARCH-like) ===\n",
    "    # Squared returns as volatility proxy\n",
    "    returns = df['gas'].pct_change()\n",
    "    features['volatility_5min'] = returns.rolling(10).std()\n",
    "    features['volatility_15min'] = returns.rolling(30).std()\n",
    "    features['volatility_1h'] = returns.rolling(rph).std()\n",
    "    \n",
    "    # Volatility of volatility (regime change indicator)\n",
    "    features['vol_of_vol'] = features['volatility_15min'].rolling(rph).std()\n",
    "    \n",
    "    # Volatility ratio (short/long) - high ratio = volatility increasing\n",
    "    features['vol_ratio'] = features['volatility_5min'] / (features['volatility_1h'] + 1e-8)\n",
    "    \n",
    "    # === REGIME TRANSITION FEATURES ===\n",
    "    # Moving average crossovers\n",
    "    ma_5min = df['gas'].rolling(10).mean()\n",
    "    ma_30min = df['gas'].rolling(60).mean()\n",
    "    ma_1h = df['gas'].rolling(rph).mean()\n",
    "    features['ma_cross_5_30'] = (ma_5min - ma_30min) / (ma_30min + 1e-8)\n",
    "    features['ma_cross_30_60'] = (ma_30min - ma_1h) / (ma_1h + 1e-8)\n",
    "    \n",
    "    # Distance from recent low (potential for mean reversion spike)\n",
    "    rolling_low_1h = df['gas'].rolling(rph).min()\n",
    "    features['dist_from_low'] = (df['gas'] - rolling_low_1h) / (rolling_low_1h + 1e-8)\n",
    "    \n",
    "    # Distance from recent high\n",
    "    rolling_high_1h = df['gas'].rolling(rph).max()\n",
    "    features['dist_from_high'] = (rolling_high_1h - df['gas']) / (df['gas'] + 1e-8)\n",
    "    \n",
    "    # Range as % of price (consolidation vs expansion)\n",
    "    features['range_pct'] = (rolling_high_1h - rolling_low_1h) / (df['gas'] + 1e-8)\n",
    "    \n",
    "    # === ABSOLUTE LEVEL FEATURES ===\n",
    "    # Current gas relative to historical percentiles\n",
    "    rolling_median_4h = df['gas'].rolling(4 * rph).median()\n",
    "    features['above_median_4h'] = (df['gas'] > rolling_median_4h).astype(float)\n",
    "    features['pct_above_median'] = (df['gas'] - rolling_median_4h) / (rolling_median_4h + 1e-8)\n",
    "    \n",
    "    # Absolute gas level buckets (spikes often start from low base)\n",
    "    features['gas_level'] = df['gas']\n",
    "    features['is_low_gas'] = (df['gas'] < df['gas'].rolling(4 * rph).quantile(0.25)).astype(float)\n",
    "    features['is_high_gas'] = (df['gas'] > df['gas'].rolling(4 * rph).quantile(0.75)).astype(float)\n",
    "    \n",
    "    # === MOMENTUM FEATURES ===\n",
    "    features['momentum_15min'] = df['gas'].diff(30)\n",
    "    features['momentum_1h'] = df['gas'].diff(rph)\n",
    "    features['momentum_accel'] = features['momentum_15min'].diff(30)\n",
    "    \n",
    "    # Rate of change\n",
    "    features['roc_5min'] = df['gas'].pct_change(10)\n",
    "    features['roc_15min'] = df['gas'].pct_change(30)\n",
    "    features['roc_1h'] = df['gas'].pct_change(rph)\n",
    "    \n",
    "    # === TIME FEATURES ===\n",
    "    if hasattr(df.index, 'hour'):\n",
    "        features['hour'] = df.index.hour\n",
    "        features['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 24)\n",
    "        features['hour_cos'] = np.cos(2 * np.pi * df.index.hour / 24)\n",
    "        # High activity hours (when spikes are more likely)\n",
    "        features['is_active_hours'] = ((df.index.hour >= 13) & (df.index.hour <= 21)).astype(float)\n",
    "    \n",
    "    return pd.DataFrame(features, index=df.index)\n",
    "\n",
    "def create_spike_target_multi(df, horizon_hours, definitions):\n",
    "    \"\"\"\n",
    "    Create spike target using multiple definitions.\n",
    "    Returns dict of targets for each definition.\n",
    "    \"\"\"\n",
    "    rph = 120\n",
    "    horizon_periods = horizon_hours * rph\n",
    "    targets = {}\n",
    "    \n",
    "    # Calculate forward max\n",
    "    future_max = df['gas'].shift(-1).rolling(horizon_periods, min_periods=1).max()\n",
    "    future_max = future_max.shift(-horizon_periods + 1)\n",
    "    current_gas = df['gas']\n",
    "    pct_change = (future_max - current_gas) / (current_gas + 1e-8)\n",
    "    abs_change = future_max - current_gas\n",
    "    \n",
    "    for defn in definitions:\n",
    "        name = defn['name']\n",
    "        if defn['type'] == 'pct':\n",
    "            targets[name] = (pct_change > defn['threshold']).astype(int)\n",
    "        elif defn['type'] == 'abs':\n",
    "            targets[name] = (abs_change > defn['threshold']).astype(int)\n",
    "        elif defn['type'] == 'combined':\n",
    "            targets[name] = ((pct_change > defn['pct_threshold']) | \n",
    "                            (abs_change > defn['abs_threshold'])).astype(int)\n",
    "        elif defn['type'] == 'percentile':\n",
    "            # Spike = future max is in top X percentile\n",
    "            threshold = df['gas'].rolling(4 * rph, min_periods=rph).quantile(defn['percentile'])\n",
    "            targets[name] = (future_max > threshold).astype(int)\n",
    "        \n",
    "        targets[name] = targets[name].fillna(0).astype(int)\n",
    "    \n",
    "    return targets\n",
    "\n",
    "def find_optimal_threshold(y_true, y_prob):\n",
    "    \"\"\"Find threshold that maximizes F1\"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-8)\n",
    "    if len(f1_scores) == 0:\n",
    "        return 0.5, 0.0\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    return thresholds[best_idx], f1_scores[best_idx]\n",
    "\n",
    "# === CREATE FEATURES ===\n",
    "print(\"\\nCreating volatility features...\")\n",
    "vol_features_train = create_volatility_features(df_train_val)\n",
    "if HAS_HOLDOUT:\n",
    "    vol_features_holdout = create_volatility_features(df_holdout)\n",
    "\n",
    "# Add to dataframes\n",
    "for col in vol_features_train.columns:\n",
    "    if col not in df_train_val.columns:\n",
    "        df_train_val[col] = vol_features_train[col]\n",
    "        if HAS_HOLDOUT:\n",
    "            df_holdout[col] = vol_features_holdout[col]\n",
    "\n",
    "# Select features for spike prediction\n",
    "spike_features = [\n",
    "    'hour_sin', 'hour_cos', 'is_active_hours',\n",
    "    'volatility_5min', 'volatility_15min', 'volatility_1h',\n",
    "    'vol_of_vol', 'vol_ratio',\n",
    "    'ma_cross_5_30', 'ma_cross_30_60',\n",
    "    'dist_from_low', 'dist_from_high', 'range_pct',\n",
    "    'is_low_gas', 'is_high_gas', 'pct_above_median',\n",
    "    'momentum_15min', 'momentum_1h', 'momentum_accel',\n",
    "    'roc_5min', 'roc_15min', 'roc_1h'\n",
    "]\n",
    "\n",
    "# Horizon-specific features\n",
    "def get_spike_features(horizon_hours):\n",
    "    if horizon_hours <= 2:\n",
    "        short_features = ['hour_sin', 'hour_cos', 'volatility_5min', 'volatility_15min',\n",
    "                         'vol_ratio', 'roc_5min', 'roc_15min', 'momentum_15min', \n",
    "                         'is_low_gas', 'is_high_gas', 'is_active_hours']\n",
    "        return [f for f in short_features if f in spike_features]\n",
    "    return spike_features\n",
    "\n",
    "available_features = [f for f in spike_features if f in df_train_val.columns]\n",
    "print(f\"Using {len(available_features)} features for spike forecasting\")\n",
    "\n",
    "# === SPIKE DEFINITIONS TO TRY ===\n",
    "SPIKE_DEFINITIONS = [\n",
    "    {'name': 'pct_15', 'type': 'pct', 'threshold': 0.15},\n",
    "    {'name': 'pct_25', 'type': 'pct', 'threshold': 0.25},\n",
    "    {'name': 'pct_50', 'type': 'pct', 'threshold': 0.50},\n",
    "    {'name': 'abs_5gwei', 'type': 'abs', 'threshold': 5.0},\n",
    "    {'name': 'abs_10gwei', 'type': 'abs', 'threshold': 10.0},\n",
    "    {'name': 'combined', 'type': 'combined', 'pct_threshold': 0.20, 'abs_threshold': 5.0},\n",
    "    {'name': 'top10pct', 'type': 'percentile', 'percentile': 0.90},\n",
    "]\n",
    "\n",
    "# === TRAIN SPIKE FORECASTERS ===\n",
    "for horizon_name, horizon_hours in [('1h', 1), ('4h', 4), ('2h', 2)]:  # Added 2h\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{horizon_name} Spike Forecast Model\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    best_model_result = None\n",
    "    best_auc = 0\n",
    "    \n",
    "    # Create all spike targets\n",
    "    spike_targets_train = create_spike_target_multi(df_train_val, horizon_hours, SPIKE_DEFINITIONS)\n",
    "    if HAS_HOLDOUT:\n",
    "        spike_targets_holdout = create_spike_target_multi(df_holdout, horizon_hours, SPIKE_DEFINITIONS)\n",
    "    \n",
    "    for defn in SPIKE_DEFINITIONS:\n",
    "        defn_name = defn['name']\n",
    "        print(f\"\\n  Testing {defn_name}...\")\n",
    "        \n",
    "        y_sf = spike_targets_train[defn_name]\n",
    "        X_sf = df_train_val[available_features].copy()\n",
    "        \n",
    "        # Remove NaN\n",
    "        mask = y_sf.notna() & X_sf.notna().all(axis=1)\n",
    "        X_sf = X_sf[mask]\n",
    "        y_sf = y_sf[mask]\n",
    "        \n",
    "        if len(X_sf) < 1000:\n",
    "            print(f\"    \u26a0\ufe0f Insufficient data ({len(X_sf)} samples)\")\n",
    "            continue\n",
    "        \n",
    "        spike_rate = y_sf.mean()\n",
    "        print(f\"    Spike rate: {spike_rate:.1%} ({y_sf.sum():.0f} spikes)\")\n",
    "        \n",
    "        # Skip extreme imbalance\n",
    "        if spike_rate > 0.7 or spike_rate < 0.01:\n",
    "            print(f\"    \u26a0\ufe0f Class imbalance too extreme\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare test set\n",
    "        if HAS_HOLDOUT:\n",
    "            X_train = X_sf\n",
    "            y_train = y_sf\n",
    "            \n",
    "            y_test = spike_targets_holdout[defn_name]\n",
    "            X_test = df_holdout[available_features].copy()\n",
    "            \n",
    "            mask_test = y_test.notna() & X_test.notna().all(axis=1)\n",
    "            X_test = X_test[mask_test]\n",
    "            y_test = y_test[mask_test]\n",
    "        else:\n",
    "            split_idx = int(len(X_sf) * 0.8)\n",
    "            X_train, X_test = X_sf.iloc[:split_idx], X_sf.iloc[split_idx:]\n",
    "            y_train, y_test = y_sf.iloc[:split_idx], y_sf.iloc[split_idx:]\n",
    "        \n",
    "        if len(X_test) < 100 or y_test.sum() < 5:\n",
    "            print(f\"    \u26a0\ufe0f Insufficient test data\")\n",
    "            continue\n",
    "        \n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Handle class imbalance\n",
    "        use_smote = HAS_SMOTE and y_train.sum() >= 10 and (y_train == 0).sum() >= 10\n",
    "        \n",
    "        if use_smote:\n",
    "            try:\n",
    "                k_neighbors = min(5, int(min(y_train.sum(), (y_train == 0).sum())) - 1)\n",
    "                k_neighbors = max(1, k_neighbors)\n",
    "                smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "                X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "            except:\n",
    "                X_train_resampled, y_train_resampled = X_train_scaled, y_train\n",
    "                use_smote = False\n",
    "        else:\n",
    "            X_train_resampled, y_train_resampled = X_train_scaled, y_train\n",
    "        \n",
    "        # Try classifiers\n",
    "        classifiers = [\n",
    "            ('GBM', GradientBoostingClassifier(n_estimators=100, max_depth=4, learning_rate=0.1, \n",
    "                                               min_samples_leaf=20, subsample=0.8, random_state=42)),\n",
    "            ('RF', RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_leaf=20,\n",
    "                                          class_weight='balanced', random_state=42, n_jobs=-1)),\n",
    "            ('LogReg', LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000, random_state=42))\n",
    "        ]\n",
    "        \n",
    "        for clf_name, clf in classifiers:\n",
    "            try:\n",
    "                if use_smote:\n",
    "                    clf.fit(X_train_resampled, y_train_resampled)\n",
    "                else:\n",
    "                    clf.fit(X_train_scaled, y_train)\n",
    "                \n",
    "                y_prob = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "                \n",
    "                # Calculate AUC\n",
    "                try:\n",
    "                    auc = roc_auc_score(y_test, y_prob)\n",
    "                except:\n",
    "                    auc = 0.5\n",
    "                \n",
    "                # Only consider if AUC > 0.55 (better than random)\n",
    "                if auc > best_auc and auc > 0.55:\n",
    "                    optimal_threshold, optimal_f1 = find_optimal_threshold(y_test, y_prob)\n",
    "                    y_pred = (y_prob >= optimal_threshold).astype(int)\n",
    "                    \n",
    "                    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "                    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "                    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "                    \n",
    "                    # Must have reasonable precision (>50%) to be useful\n",
    "                    if precision >= 0.5:\n",
    "                        best_auc = auc\n",
    "                        best_model_result = {\n",
    "                            'model': clf,\n",
    "                            'scaler': scaler,\n",
    "                            'spike_definition': defn,\n",
    "                            'classifier': clf_name,\n",
    "                            'optimal_threshold': float(optimal_threshold),\n",
    "                            'precision': float(precision),\n",
    "                            'recall': float(recall),\n",
    "                            'f1_score': float(f1),\n",
    "                            'auc': float(auc),\n",
    "                            'spike_rate_train': float(spike_rate),\n",
    "                            'spike_rate_test': float(y_test.mean()),\n",
    "                            'features': available_features,\n",
    "                            'horizon': horizon_name\n",
    "                        }\n",
    "                        print(f\"    {clf_name}: AUC={auc:.3f}, P={precision:.1%}, R={recall:.1%}, F1={f1:.3f} \u2713\")\n",
    "                    else:\n",
    "                        print(f\"    {clf_name}: AUC={auc:.3f}, P={precision:.1%} (too low)\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    # Store results\n",
    "    if best_model_result:\n",
    "        spike_forecast_models[horizon_name] = best_model_result\n",
    "        print(f\"\\n  \u2713 Best: {best_model_result['classifier']} with {best_model_result['spike_definition']['name']}\")\n",
    "        print(f\"    AUC={best_model_result['auc']:.3f}, P={best_model_result['precision']:.1%}, R={best_model_result['recall']:.1%}\")\n",
    "    else:\n",
    "        print(f\"\\n  \u26a0\ufe0f No viable {horizon_name} spike model (need AUC>0.55 and P>50%)\")\n",
    "        spike_forecast_models[horizon_name] = {\n",
    "            'model': None,\n",
    "            'scaler': None,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1_score': 0.0,\n",
    "            'auc': 0.5,\n",
    "            'has_model': False,\n",
    "            'note': 'No model met quality thresholds'\n",
    "        }\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SPIKE FORECAST SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for h, data in spike_forecast_models.items():\n",
    "    if data.get('model') is not None:\n",
    "        print(f\"  {h}: AUC={data['auc']:.3f}, P={data['precision']:.1%}, R={data['recall']:.1%}\")\n",
    "        print(f\"      Definition: {data['spike_definition']['name']}\")\n",
    "    else:\n",
    "        print(f\"  {h}: No viable model\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# DYNAMIC ENSEMBLE WEIGHTING + CONFIDENCE SCORING\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DYNAMIC ENSEMBLE WEIGHTING + CONFIDENCE SCORING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def calculate_dynamic_weights(global_errors, regime_errors, decay=0.9):\n",
    "    \"\"\"Calculate dynamic weights based on exponentially weighted recent errors\"\"\"\n",
    "    if len(global_errors) == 0 or len(regime_errors) == 0:\n",
    "        return 0.5, 0.5\n",
    "    \n",
    "    n = len(global_errors)\n",
    "    exp_weights = np.array([decay ** (n - i - 1) for i in range(n)])\n",
    "    exp_weights = exp_weights / exp_weights.sum()\n",
    "    \n",
    "    global_weighted_error = np.sum(global_errors * exp_weights)\n",
    "    regime_weighted_error = np.sum(regime_errors * exp_weights)\n",
    "    \n",
    "    total_inv_error = 1/(global_weighted_error + 1e-8) + 1/(regime_weighted_error + 1e-8)\n",
    "    global_weight = (1/(global_weighted_error + 1e-8)) / total_inv_error\n",
    "    regime_weight = (1/(regime_weighted_error + 1e-8)) / total_inv_error\n",
    "    \n",
    "    return global_weight, regime_weight\n",
    "\n",
    "def compute_prediction_confidence(predictions, base_interval, volatility_ratio=1.0):\n",
    "    \"\"\"\n",
    "    Compute confidence score based on model agreement and volatility.\n",
    "    Returns confidence (0-1) and suggested interval multiplier.\n",
    "    \"\"\"\n",
    "    if len(predictions) <= 1:\n",
    "        return 1.0, 1.0  # High confidence, no multiplier\n",
    "    \n",
    "    # Model disagreement as spread\n",
    "    pred_std = np.std(predictions)\n",
    "    pred_mean = np.mean(predictions)\n",
    "    pred_cv = pred_std / (np.abs(pred_mean) + 1e-8)  # Coefficient of variation\n",
    "    \n",
    "    # Normalize disagreement (higher = less confident)\n",
    "    # pred_cv > 0.5 means models disagree by more than 50%\n",
    "    disagreement = min(pred_cv / 0.5, 1.0)\n",
    "    \n",
    "    # Confidence = 1 - disagreement, adjusted for volatility\n",
    "    confidence = (1 - disagreement) / (1 + volatility_ratio * 0.5)\n",
    "    confidence = max(0.2, min(1.0, confidence))\n",
    "    \n",
    "    # Interval multiplier: widen when confidence is low\n",
    "    # confidence=1.0 -> multiplier=1.0, confidence=0.5 -> multiplier=1.5\n",
    "    multiplier = 1.0 + (1.0 - confidence)\n",
    "    \n",
    "    return confidence, multiplier\n",
    "\n",
    "class ConfidenceScoringPredictor:\n",
    "    \"\"\"Predictor that outputs confidence scores alongside predictions.\"\"\"\n",
    "    \n",
    "    def __init__(self, main_model, main_scaler, features, \n",
    "                 quantile_models=None, quantile_scaler=None,\n",
    "                 regime_models=None, base_interval=0.3):\n",
    "        self.main_model = main_model\n",
    "        self.main_scaler = main_scaler\n",
    "        self.features = features\n",
    "        self.quantile_models = quantile_models\n",
    "        self.quantile_scaler = quantile_scaler\n",
    "        self.regime_models = regime_models\n",
    "        self.base_interval = base_interval\n",
    "        self.recent_errors = []\n",
    "        self.max_history = 100\n",
    "        self.hit_rate_80 = 0.8\n",
    "        self.rolling_mae = base_interval\n",
    "    \n",
    "    def predict_with_confidence(self, X, hour=None, regime=None, volatility_ratio=1.0):\n",
    "        \"\"\"\n",
    "        Make prediction with confidence score and adaptive interval.\n",
    "        Returns: (prediction, confidence, interval_low, interval_high)\n",
    "        \"\"\"\n",
    "        # Prepare input\n",
    "        if hasattr(X, 'columns'):\n",
    "            X_subset = X[self.features] if all(f in X.columns for f in self.features) else X\n",
    "        else:\n",
    "            X_subset = X\n",
    "        \n",
    "        # Get main prediction\n",
    "        X_scaled = self.main_scaler.transform(X_subset)\n",
    "        main_pred = self.main_model.predict(X_scaled)\n",
    "        \n",
    "        # Collect predictions from different models\n",
    "        all_predictions = [main_pred]\n",
    "        \n",
    "        # Add quantile median if available\n",
    "        if self.quantile_models and self.quantile_scaler:\n",
    "            try:\n",
    "                X_q_scaled = self.quantile_scaler.transform(X_subset)\n",
    "                if 0.5 in self.quantile_models:\n",
    "                    q_pred = self.quantile_models[0.5].predict(X_q_scaled)\n",
    "                    all_predictions.append(q_pred)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Add regime model prediction if available\n",
    "        if regime is not None and self.regime_models and regime in self.regime_models:\n",
    "            try:\n",
    "                regime_data = self.regime_models[regime]\n",
    "                regime_features = regime_data.get('features', self.features)\n",
    "                if hasattr(X, 'columns'):\n",
    "                    available = [f for f in regime_features if f in X.columns]\n",
    "                    if len(available) == len(regime_features):\n",
    "                        X_regime = X[regime_features]\n",
    "                        X_regime_scaled = regime_data['scaler'].transform(X_regime)\n",
    "                        regime_pred = regime_data['model'].predict(X_regime_scaled)\n",
    "                        all_predictions.append(regime_pred)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Compute confidence from model agreement\n",
    "        all_preds_array = np.array([p.flatten()[0] if hasattr(p, 'flatten') else p for p in all_predictions])\n",
    "        confidence, interval_multiplier = compute_prediction_confidence(\n",
    "            all_preds_array, self.base_interval, volatility_ratio\n",
    "        )\n",
    "        \n",
    "        # Get prediction intervals\n",
    "        interval_width = self.base_interval * interval_multiplier\n",
    "        \n",
    "        # Adjust for time period if we have that info\n",
    "        if hour is not None:\n",
    "            if 12 <= hour < 18:  # Afternoon - higher uncertainty\n",
    "                interval_width *= 1.2\n",
    "                confidence *= 0.9\n",
    "            elif 18 <= hour < 24:  # Evening - moderate uncertainty\n",
    "                interval_width *= 1.1\n",
    "        \n",
    "        # Final prediction (use main model)\n",
    "        pred_value = main_pred.flatten()[0] if hasattr(main_pred, 'flatten') else float(main_pred)\n",
    "        \n",
    "        return {\n",
    "            'prediction': pred_value,\n",
    "            'confidence': float(confidence),\n",
    "            'interval_low': pred_value - interval_width,\n",
    "            'interval_high': pred_value + interval_width,\n",
    "            'interval_width': float(interval_width),\n",
    "            'model_disagreement': float(np.std(all_preds_array)) if len(all_preds_array) > 1 else 0.0,\n",
    "            'n_models': len(all_predictions)\n",
    "        }\n",
    "    \n",
    "    def update_with_actual(self, prediction, actual):\n",
    "        \"\"\"Update error history for adaptive confidence\"\"\"\n",
    "        error = abs(actual - prediction)\n",
    "        self.recent_errors.append(error)\n",
    "        if len(self.recent_errors) > self.max_history:\n",
    "            self.recent_errors.pop(0)\n",
    "\n",
    "def create_dynamic_ensemble_predictor(global_model, global_scaler, regime_models, features, decay=0.9):\n",
    "    \"\"\"Create ensemble predictor with dynamic weighting and confidence scoring.\"\"\"\n",
    "    recent_errors = {'global': [], 'regime': {}}\n",
    "    \n",
    "    def predict(X, current_regime=None, actual_value=None):\n",
    "        nonlocal recent_errors\n",
    "        \n",
    "        if hasattr(X, 'columns'):\n",
    "            X_global = X[features] if all(f in X.columns for f in features) else X\n",
    "            X_scaled = global_scaler.transform(X_global)\n",
    "        else:\n",
    "            X_scaled = global_scaler.transform(X)\n",
    "        \n",
    "        global_pred = global_model.predict(X_scaled)\n",
    "        \n",
    "        if current_regime is not None and regime_models and current_regime in regime_models:\n",
    "            regime_data = regime_models[current_regime]\n",
    "            regime_features = regime_data.get('features', features)\n",
    "            \n",
    "            try:\n",
    "                if hasattr(X, 'columns'):\n",
    "                    available_features = [f for f in regime_features if f in X.columns]\n",
    "                    if len(available_features) == len(regime_features):\n",
    "                        X_regime = X[regime_features]\n",
    "                        X_regime_scaled = regime_data['scaler'].transform(X_regime)\n",
    "                        regime_pred = regime_data['model'].predict(X_regime_scaled)\n",
    "                    else:\n",
    "                        return global_pred\n",
    "                else:\n",
    "                    expected_features = regime_data['scaler'].n_features_in_\n",
    "                    if X.shape[1] == expected_features:\n",
    "                        X_regime_scaled = regime_data['scaler'].transform(X)\n",
    "                        regime_pred = regime_data['model'].predict(X_regime_scaled)\n",
    "                    else:\n",
    "                        return global_pred\n",
    "            except Exception:\n",
    "                return global_pred\n",
    "            \n",
    "            if current_regime not in recent_errors['regime']:\n",
    "                recent_errors['regime'][current_regime] = []\n",
    "            \n",
    "            global_errors = np.array(recent_errors['global']) if recent_errors['global'] else np.array([1.0])\n",
    "            regime_errors = np.array(recent_errors['regime'].get(current_regime, [1.0]))\n",
    "            \n",
    "            g_weight, r_weight = calculate_dynamic_weights(global_errors, regime_errors, decay)\n",
    "            final_pred = g_weight * global_pred + r_weight * regime_pred\n",
    "            \n",
    "            if actual_value is not None:\n",
    "                g_error = abs(actual_value - global_pred)\n",
    "                r_error = abs(actual_value - regime_pred)\n",
    "                recent_errors['global'].append(float(g_error))\n",
    "                recent_errors['regime'][current_regime].append(float(r_error))\n",
    "                \n",
    "                if len(recent_errors['global']) > 100:\n",
    "                    recent_errors['global'].pop(0)\n",
    "                if len(recent_errors['regime'][current_regime]) > 100:\n",
    "                    recent_errors['regime'][current_regime].pop(0)\n",
    "            \n",
    "            return final_pred\n",
    "        \n",
    "        return global_pred\n",
    "    \n",
    "    return predict\n",
    "\n",
    "# Create confidence scoring predictors for each horizon\n",
    "confidence_predictors = {}\n",
    "\n",
    "for horizon in ['1h', '4h']:\n",
    "    if horizon not in trained_models:\n",
    "        continue\n",
    "    \n",
    "    data = trained_models[horizon]\n",
    "    \n",
    "    # Get quantile models if available\n",
    "    q_models = None\n",
    "    q_scaler = None\n",
    "    if horizon in quantile_models:\n",
    "        q_models, q_scaler = quantile_models[horizon]\n",
    "    \n",
    "    # Get regime models if available\n",
    "    r_models = None\n",
    "    if 'regime_specific_models' in dir() and regime_specific_models.get(horizon):\n",
    "        r_models = regime_specific_models[horizon]\n",
    "    \n",
    "    # Get base interval from conformal calibration\n",
    "    base_interval = conformal_residuals.get(horizon, {}).get('quantile', 0.3)\n",
    "    \n",
    "    # Create confidence predictor\n",
    "    conf_pred = ConfidenceScoringPredictor(\n",
    "        main_model=data['model'],\n",
    "        main_scaler=data['scaler'],\n",
    "        features=data['features'],\n",
    "        quantile_models=q_models,\n",
    "        quantile_scaler=q_scaler,\n",
    "        regime_models=r_models,\n",
    "        base_interval=base_interval\n",
    "    )\n",
    "    \n",
    "    confidence_predictors[horizon] = conf_pred\n",
    "    \n",
    "    # Store in trained_models for saving\n",
    "    trained_models[horizon]['confidence_predictor'] = conf_pred\n",
    "    trained_models[horizon]['has_confidence_scoring'] = True\n",
    "    \n",
    "    print(f\"\\n{horizon}: Confidence scoring predictor created\")\n",
    "    print(f\"  Base interval: \u00b1{base_interval:.4f}\")\n",
    "    print(f\"  Quantile models: {'Yes' if q_models else 'No'}\")\n",
    "    print(f\"  Regime models: {'Yes' if r_models else 'No'}\")\n",
    "\n",
    "# Also create dynamic ensemble predictors\n",
    "for horizon in ['1h', '4h']:\n",
    "    if horizon not in trained_models:\n",
    "        continue\n",
    "    \n",
    "    data = trained_models[horizon]\n",
    "    r_models = None\n",
    "    if 'regime_specific_models' in dir() and regime_specific_models.get(horizon):\n",
    "        r_models = regime_specific_models[horizon]\n",
    "    \n",
    "    if r_models:\n",
    "        ensemble_pred = create_dynamic_ensemble_predictor(\n",
    "            data['model'], data['scaler'], r_models, data['features']\n",
    "        )\n",
    "        trained_models[horizon]['dynamic_ensemble_predict'] = ensemble_pred\n",
    "        trained_models[horizon]['has_dynamic_ensemble'] = True\n",
    "        print(f\"  {horizon}: Dynamic ensemble predictor created\")\n",
    "\n",
    "print(f\"\\n\u2713 Confidence scoring and dynamic ensemble setup complete\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ERROR ANALYSIS + AUTOMATIC BIAS CORRECTION (IMPROVED)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ERROR ANALYSIS + BIAS CORRECTION\")\n",
    "print(\"=\"*60)\n",
    "def compute_enhanced_metrics(horizon, y_test, y_pred, y_lower, y_upper,\n",
    "                              timestamps, regimes=None, volatility_levels=None):\n",
    "    \"\"\"\n",
    "    Compute comprehensive evaluation metrics including:\n",
    "    - Pinball loss for quantile predictions\n",
    "    - Coverage by regime and time period\n",
    "    - Directional accuracy\n",
    "    - Interval width statistics\n",
    "    - Spike prediction metrics\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    \n",
    "    metrics = {}\n",
    "\n",
    "    # 1. Basic metrics\n",
    "    metrics['mae'] = mean_absolute_error(y_test, y_pred)\n",
    "    metrics['rmse'] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    metrics['mape'] = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100\n",
    "    metrics['r2'] = r2_score(y_test, y_pred)\n",
    "\n",
    "    # 2. Pinball loss for quantile predictions\n",
    "    if y_lower is not None and y_upper is not None:\n",
    "        pinball_lower = np.mean(np.where(y_test >= y_lower,\n",
    "                                         0.1 * (y_test - y_lower),\n",
    "                                         0.9 * (y_lower - y_test)))\n",
    "        pinball_upper = np.mean(np.where(y_test >= y_upper,\n",
    "                                         0.9 * (y_test - y_upper),\n",
    "                                         0.1 * (y_upper - y_test)))\n",
    "        metrics['pinball_loss_lower'] = pinball_lower\n",
    "        metrics['pinball_loss_upper'] = pinball_upper\n",
    "        metrics['pinball_loss_avg'] = (pinball_lower + pinball_upper) / 2\n",
    "\n",
    "    # 3. Interval coverage\n",
    "    if y_lower is not None and y_upper is not None:\n",
    "        coverage = np.mean((y_test >= y_lower) & (y_test <= y_upper))\n",
    "        metrics['interval_coverage'] = coverage\n",
    "        metrics['interval_width_mean'] = np.mean(y_upper - y_lower)\n",
    "        metrics['interval_width_median'] = np.median(y_upper - y_lower)\n",
    "        metrics['interval_width_std'] = np.std(y_upper - y_lower)\n",
    "\n",
    "        # Coverage by time period\n",
    "        hours = pd.to_datetime(timestamps).hour\n",
    "        for period, (start, end) in [('night', (0, 6)), ('morning', (6, 12)),\n",
    "                                       ('afternoon', (12, 18)), ('evening', (18, 24))]:\n",
    "            mask = (hours >= start) & (hours < end)\n",
    "            if mask.sum() > 0:\n",
    "                coverage_period = np.mean((y_test[mask] >= y_lower[mask]) &\n",
    "                                          (y_test[mask] <= y_upper[mask]))\n",
    "                metrics[f'coverage_{period}'] = coverage_period\n",
    "                metrics[f'mae_{period}'] = mean_absolute_error(y_test[mask], y_pred[mask])\n",
    "\n",
    "        # Coverage by regime\n",
    "        if regimes is not None:\n",
    "            for regime_val, regime_name in [(0, 'normal'), (1, 'elevated'), (2, 'spike')]:\n",
    "                regime_mask = regimes == regime_val\n",
    "                if regime_mask.sum() > 0:\n",
    "                    coverage_regime = np.mean((y_test[regime_mask] >= y_lower[regime_mask]) &\n",
    "                                              (y_test[regime_mask] <= y_upper[regime_mask]))\n",
    "                    metrics[f'coverage_{regime_name}'] = coverage_regime\n",
    "                    metrics[f'mae_{regime_name}'] = mean_absolute_error(y_test[regime_mask],\n",
    "                                                                          y_pred[regime_mask])\n",
    "\n",
    "    # 4. Directional accuracy\n",
    "    if len(y_test) > 1:\n",
    "        actual_direction = np.sign(np.diff(y_test))\n",
    "        pred_direction = np.sign(np.diff(y_pred))\n",
    "        metrics['directional_accuracy'] = np.mean(actual_direction == pred_direction)\n",
    "\n",
    "    # 5. Spike prediction metrics\n",
    "    spike_threshold = np.percentile(y_test, 95)\n",
    "    actual_spikes = y_test > spike_threshold\n",
    "    pred_high = y_pred > np.percentile(y_pred, 90)\n",
    "\n",
    "    if actual_spikes.sum() > 0:\n",
    "        metrics['spike_recall'] = np.mean(pred_high[actual_spikes])\n",
    "        metrics['spike_precision'] = np.mean(actual_spikes[pred_high]) if pred_high.sum() > 0 else 0\n",
    "\n",
    "        # F1 score for spikes\n",
    "        if metrics['spike_precision'] + metrics['spike_recall'] > 0:\n",
    "            metrics['spike_f1'] = 2 * (metrics['spike_precision'] * metrics['spike_recall']) / \\\n",
    "                                 (metrics['spike_precision'] + metrics['spike_recall'])\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "error_analysis = {}\n",
    "bias_correction_factors = {}\n",
    "\n",
    "# === MODULE-LEVEL CLASS FOR PICKLING ===\n",
    "class BiasCorrectedModel:\n",
    "    \"\"\"\n",
    "    Model wrapper that automatically applies bias correction during inference.\n",
    "    Defined at module level for pickling compatibility.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, scaler, bias_factors, features):\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "        self.bias_factors = bias_factors\n",
    "        self.features = features\n",
    "    \n",
    "    def predict(self, X, hour=None, regime=None):\n",
    "        \"\"\"Predict with automatic bias correction\"\"\"\n",
    "        if hasattr(X, 'columns'):\n",
    "            X_subset = X[self.features]\n",
    "        else:\n",
    "            X_subset = X\n",
    "        \n",
    "        X_scaled = self.scaler.transform(X_subset)\n",
    "        raw_pred = self.model.predict(X_scaled)\n",
    "        \n",
    "        # Apply bias correction\n",
    "        correction = 0.0\n",
    "        \n",
    "        # Time-based correction (most specific)\n",
    "        if hour is not None and 'by_time' in self.bias_factors:\n",
    "            period = 'afternoon' if 12 <= hour < 18 else (\n",
    "                'morning' if 6 <= hour < 12 else (\n",
    "                'evening' if 18 <= hour < 24 else 'night'))\n",
    "            \n",
    "            if period in self.bias_factors['by_time']:\n",
    "                tb = self.bias_factors['by_time'][period]\n",
    "                if tb.get('should_apply', False):\n",
    "                    correction = tb['correction']\n",
    "        \n",
    "        # Regime-based correction (fallback)\n",
    "        elif regime is not None and 'by_regime' in self.bias_factors:\n",
    "            regime_name = {0: 'normal', 1: 'elevated', 2: 'spike'}.get(regime)\n",
    "            if regime_name in self.bias_factors['by_regime']:\n",
    "                rb = self.bias_factors['by_regime'][regime_name]\n",
    "                if rb.get('should_apply', False):\n",
    "                    correction = rb['correction']\n",
    "        \n",
    "        # Overall correction (last resort)\n",
    "        elif self.bias_factors.get('overall', {}).get('should_apply', False):\n",
    "            correction = self.bias_factors['overall']['correction']\n",
    "        \n",
    "        return raw_pred + correction\n",
    "    \n",
    "    def predict_raw(self, X):\n",
    "        \"\"\"Predict without bias correction\"\"\"\n",
    "        if hasattr(X, 'columns'):\n",
    "            X_subset = X[self.features]\n",
    "        else:\n",
    "            X_subset = X\n",
    "        X_scaled = self.scaler.transform(X_subset)\n",
    "        return self.model.predict(X_scaled)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "def compute_bias_correction(y_true, y_pred, hours, regimes):\n",
    "    \"\"\"\n",
    "    Compute bias correction factors that can be applied during inference.\n",
    "    IMPROVED: Use median for robustness, lower threshold for high-error periods.\n",
    "    \"\"\"\n",
    "    errors = y_true - y_pred  # Positive = under-prediction, Negative = over-prediction\n",
    "    abs_errors = np.abs(errors)\n",
    "    \n",
    "    # Overall bias\n",
    "    overall_bias = np.mean(errors)\n",
    "    overall_median_bias = np.median(errors)\n",
    "    recent_bias = np.mean(errors[-min(240, len(errors)):])  # Last 2 hours\n",
    "    \n",
    "    # Time-period specific bias\n",
    "    time_bias = {}\n",
    "    time_periods = {\n",
    "        'night': (0, 6),\n",
    "        'morning': (6, 12),\n",
    "        'afternoon': (12, 18),\n",
    "        'evening': (18, 24)\n",
    "    }\n",
    "    \n",
    "    overall_mae = np.mean(abs_errors)\n",
    "    \n",
    "    for period_name, (start, end) in time_periods.items():\n",
    "        mask = (hours >= start) & (hours < end)\n",
    "        if mask.sum() >= 50:\n",
    "            period_errors = errors[mask]\n",
    "            period_abs_errors = abs_errors[mask]\n",
    "            period_bias = np.mean(period_errors)\n",
    "            period_median_bias = np.median(period_errors)\n",
    "            period_mae = np.mean(period_abs_errors)\n",
    "            \n",
    "            # Use median bias for correction (more robust)\n",
    "            correction = period_median_bias\n",
    "            \n",
    "            # Apply if bias > 2% (lowered from 3%) OR if this period has much higher MAE\n",
    "            is_high_error_period = period_mae > overall_mae * 1.2\n",
    "            should_apply = abs(period_bias) > 0.02 or (is_high_error_period and abs(period_bias) > 0.01)\n",
    "            \n",
    "            time_bias[period_name] = {\n",
    "                'bias': float(period_bias),\n",
    "                'median_bias': float(period_median_bias),\n",
    "                'correction': float(correction),\n",
    "                'mae': float(period_mae),\n",
    "                'mae_ratio': float(period_mae / overall_mae) if overall_mae > 0 else 1.0,\n",
    "                'n_samples': int(mask.sum()),\n",
    "                'should_apply': should_apply,\n",
    "                'is_high_error': is_high_error_period\n",
    "            }\n",
    "    \n",
    "    # Regime-specific bias\n",
    "    regime_bias = {}\n",
    "    for regime_val, regime_name in [(0, 'normal'), (1, 'elevated'), (2, 'spike')]:\n",
    "        mask = regimes == regime_val\n",
    "        if mask.sum() >= 50:\n",
    "            r_errors = errors[mask]\n",
    "            r_bias = np.mean(r_errors)\n",
    "            r_median_bias = np.median(r_errors)\n",
    "            \n",
    "            regime_bias[regime_name] = {\n",
    "                'bias': float(r_bias),\n",
    "                'median_bias': float(r_median_bias),\n",
    "                'correction': float(r_median_bias),\n",
    "                'n_samples': int(mask.sum()),\n",
    "                'should_apply': abs(r_bias) > 0.02\n",
    "            }\n",
    "    \n",
    "    return {\n",
    "        'overall': {\n",
    "            'bias': float(overall_bias),\n",
    "            'median_bias': float(overall_median_bias),\n",
    "            'recent_bias': float(recent_bias),\n",
    "            'correction': float(overall_median_bias),\n",
    "            'mae': float(overall_mae),\n",
    "            'should_apply': abs(overall_median_bias) > 0.05\n",
    "        },\n",
    "        'by_time': time_bias,\n",
    "        'by_regime': regime_bias\n",
    "    }\n",
    "\n",
    "def create_bias_corrected_predictor(model, scaler, bias_factors, features):\n",
    "    \"\"\"Create a predictor that automatically applies bias correction.\"\"\"\n",
    "    return BiasCorrectedModel(model, scaler, bias_factors, features)\n",
    "\n",
    "for horizon in ['1h', '4h']:\n",
    "    if horizon not in trained_models:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{horizon} Error Analysis...\")\n",
    "    \n",
    "    data = trained_models[horizon]\n",
    "    model = data['model']\n",
    "    scaler = data['scaler']\n",
    "    features = data['features']\n",
    "    \n",
    "    if not HAS_HOLDOUT:\n",
    "        print(\"  \u26a0\ufe0f No holdout data\")\n",
    "        continue\n",
    "    \n",
    "    # Get predictions on holdout\n",
    "    X_test = df_holdout[features]\n",
    "    y_test = df_holdout[f'target_{horizon}']\n",
    "    \n",
    "    mask = y_test.notna()\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # === COMPUTE ENHANCED METRICS ===\n",
    "    try:\n",
    "        # Get prediction intervals if available  \n",
    "        pred_lower = np.zeros_like(y_pred)\n",
    "        pred_upper = np.ones_like(y_pred) * 1.0\n",
    "        \n",
    "        # Try to get actual intervals from conformal prediction\n",
    "        if horizon in conf_lower_by_horizon:\n",
    "            pred_lower = conf_lower_by_horizon[horizon]\n",
    "            pred_upper = conf_upper_by_horizon[horizon]\n",
    "        \n",
    "        enhanced_m = compute_enhanced_metrics(\n",
    "            horizon=horizon,\n",
    "            y_test=y_test.values,\n",
    "            y_pred=y_pred,\n",
    "            y_lower=pred_lower if pred_lower is not None else None,\n",
    "            y_upper=pred_upper if pred_upper is not None else None,\n",
    "            timestamps=X_test.index,\n",
    "            regimes=None,\n",
    "            volatility_levels=None\n",
    "        )\n",
    "        \n",
    "        # Print key metrics\n",
    "        print(f\"  Directional Accuracy: {enhanced_m.get('directional_accuracy', 0):.1%}\")\n",
    "        if 'spike_recall' in enhanced_m:\n",
    "            print(f\"  Spike Recall: {enhanced_m.get('spike_recall', 0):.1%} | F1: {enhanced_m.get('spike_f1', 0):.3f}\")\n",
    "    except Exception as e:\n",
    "        pass  # Skip if error\n",
    "\n",
    "    \n",
    "    errors = y_test.values - y_pred\n",
    "    abs_errors = np.abs(errors)\n",
    "    \n",
    "    # Get hours and regimes\n",
    "    hours = X_test.index.hour if hasattr(X_test.index, 'hour') else np.full(len(X_test), 12)\n",
    "    \n",
    "    regimes = np.zeros(len(X_test))\n",
    "    if 'gas_zscore_1h' in df_holdout.columns:\n",
    "        regimes[df_holdout.loc[X_test.index, 'gas_zscore_1h'] > 1] = 1\n",
    "    if 'is_spike' in df_holdout.columns:\n",
    "        regimes[df_holdout.loc[X_test.index, 'is_spike'] == 1] = 2\n",
    "    \n",
    "    # === COMPUTE BIAS CORRECTION FACTORS ===\n",
    "    bias_factors = compute_bias_correction(y_test.values, y_pred, hours, regimes)\n",
    "    bias_correction_factors[horizon] = bias_factors\n",
    "    \n",
    "    print(f\"\\n  Bias Analysis:\")\n",
    "    print(f\"    Overall: mean={bias_factors['overall']['bias']:.4f}, median={bias_factors['overall']['median_bias']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Time-Period Bias:\")\n",
    "    for period, tb in bias_factors['by_time'].items():\n",
    "        apply_flag = \"\u2713 APPLY\" if tb['should_apply'] else \"  skip\"\n",
    "        high_err = \" [HIGH ERROR]\" if tb.get('is_high_error', False) else \"\"\n",
    "        print(f\"    {period}: bias={tb['bias']:+.4f}, MAE={tb['mae']:.4f} ({tb['mae_ratio']:.1f}x){high_err} {apply_flag}\")\n",
    "    \n",
    "    print(f\"\\n  Regime Bias:\")\n",
    "    for regime_name, rb in bias_factors['by_regime'].items():\n",
    "        apply_flag = \"\u2713 APPLY\" if rb['should_apply'] else \"  skip\"\n",
    "        print(f\"    {regime_name}: {rb['bias']:+.4f} ({rb['n_samples']} samples) {apply_flag}\")\n",
    "    \n",
    "    # === TEST BIAS-CORRECTED PREDICTIONS ===\n",
    "    print(f\"\\n  Bias Correction Test:\")\n",
    "    \n",
    "    # Predictions without correction\n",
    "    raw_mae = np.mean(abs_errors)\n",
    "    \n",
    "    # Predictions with time-based correction only\n",
    "    corrected_preds_time = y_pred.copy()\n",
    "    for i in range(len(corrected_preds_time)):\n",
    "        h = hours[i]\n",
    "        period = 'afternoon' if 12 <= h < 18 else (\n",
    "            'morning' if 6 <= h < 12 else (\n",
    "            'evening' if 18 <= h < 24 else 'night'))\n",
    "        \n",
    "        if period in bias_factors['by_time'] and bias_factors['by_time'][period]['should_apply']:\n",
    "            corrected_preds_time[i] += bias_factors['by_time'][period]['correction']\n",
    "    \n",
    "    corrected_mae_time = mean_absolute_error(y_test, corrected_preds_time)\n",
    "    improvement_time = (raw_mae - corrected_mae_time) / raw_mae * 100\n",
    "    \n",
    "    print(f\"    Raw MAE: {raw_mae:.4f}\")\n",
    "    print(f\"    Time-corrected MAE: {corrected_mae_time:.4f} ({improvement_time:+.1f}%)\")\n",
    "    \n",
    "    # Check per-period improvement\n",
    "    print(f\"\\n  Per-Period Results (with correction):\")\n",
    "    for period, (start, end) in [('night', (0, 6)), ('morning', (6, 12)), ('afternoon', (12, 18)), ('evening', (18, 24))]:\n",
    "        period_mask = (hours >= start) & (hours < end)\n",
    "        if period_mask.sum() > 10:\n",
    "            raw_period_mae = np.mean(abs_errors[period_mask])\n",
    "            corrected_period_mae = np.mean(np.abs(y_test.values[period_mask] - corrected_preds_time[period_mask]))\n",
    "            pct_change = (raw_period_mae - corrected_period_mae) / raw_period_mae * 100\n",
    "            print(f\"    {period}: {raw_period_mae:.4f} \u2192 {corrected_period_mae:.4f} ({pct_change:+.1f}%)\")\n",
    "    \n",
    "    # Decide whether to use bias correction\n",
    "    # IMPROVED: Use bias correction if it helps ANY high-error period, even if overall MAE is worse\n",
    "    high_error_periods = [p for p, tb in bias_factors['by_time'].items() if tb.get('is_high_error', False)]\n",
    "    helps_high_error = False\n",
    "    \n",
    "    for period in high_error_periods:\n",
    "        (start, end) = {'night': (0, 6), 'morning': (6, 12), 'afternoon': (12, 18), 'evening': (18, 24)}[period]\n",
    "        period_mask = (hours >= start) & (hours < end)\n",
    "        if period_mask.sum() > 10:\n",
    "            raw_period_mae = np.mean(abs_errors[period_mask])\n",
    "            corrected_period_mae = np.mean(np.abs(y_test.values[period_mask] - corrected_preds_time[period_mask]))\n",
    "            if corrected_period_mae < raw_period_mae:\n",
    "                helps_high_error = True\n",
    "                break\n",
    "    \n",
    "    use_bias_correction = corrected_mae_time < raw_mae or helps_high_error\n",
    "    \n",
    "    if use_bias_correction:\n",
    "        print(f\"\\n  \u2713 Bias correction ENABLED\")\n",
    "        if helps_high_error:\n",
    "            print(f\"    Reason: Helps high-error periods: {high_error_periods}\")\n",
    "        bias_corrected_model = create_bias_corrected_predictor(model, scaler, bias_factors, features)\n",
    "        trained_models[horizon]['bias_corrected_model'] = bias_corrected_model\n",
    "        trained_models[horizon]['use_bias_correction'] = True\n",
    "    else:\n",
    "        print(f\"\\n  \u2717 Bias correction disabled (no improvement)\")\n",
    "        trained_models[horizon]['use_bias_correction'] = False\n",
    "    \n",
    "    # Standard error analysis\n",
    "    analysis = {\n",
    "        'mean_error': float(np.mean(errors)),\n",
    "        'std_error': float(np.std(errors)),\n",
    "        'mae': float(np.mean(abs_errors)),\n",
    "        'corrected_mae': float(corrected_mae_time),\n",
    "        'median_ae': float(np.median(abs_errors)),\n",
    "        'max_error': float(np.max(abs_errors)),\n",
    "        'p95_error': float(np.percentile(abs_errors, 95)),\n",
    "        'p99_error': float(np.percentile(abs_errors, 99)),\n",
    "        'bias_correction_applied': use_bias_correction\n",
    "    }\n",
    "    \n",
    "    # Error by regime\n",
    "    regime_errors = {}\n",
    "    for regime_val, regime_name in [(0, 'normal'), (1, 'elevated'), (2, 'spike')]:\n",
    "        regime_mask = regimes == regime_val\n",
    "        if regime_mask.sum() > 10:\n",
    "            regime_errors[regime_name] = float(np.mean(abs_errors[regime_mask]))\n",
    "    analysis['regime_errors'] = regime_errors\n",
    "    \n",
    "    # Error by time\n",
    "    time_errors = {}\n",
    "    for period, (start, end) in [('night', (0, 6)), ('morning', (6, 12)), ('afternoon', (12, 18)), ('evening', (18, 24))]:\n",
    "        period_mask = (hours >= start) & (hours < end)\n",
    "        if period_mask.sum() > 10:\n",
    "            time_errors[period] = float(np.mean(abs_errors[period_mask]))\n",
    "    analysis['time_errors'] = time_errors\n",
    "    \n",
    "    error_analysis[horizon] = analysis\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BIAS CORRECTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for horizon, bcf in bias_correction_factors.items():\n",
    "    use_bc = trained_models.get(horizon, {}).get('use_bias_correction', False)\n",
    "    status = \"ENABLED\" if use_bc else \"DISABLED\"\n",
    "    print(f\"\\n{horizon}: {status}\")\n",
    "    \n",
    "    if use_bc:\n",
    "        print(f\"  Time-specific corrections:\")\n",
    "        for period, tb in bcf['by_time'].items():\n",
    "            if tb['should_apply']:\n",
    "                print(f\"    {period}: {tb['correction']:+.4f}\")\n",
    "\n",
    "print(f\"\\n\u2713 Error analysis complete\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# AUTOMATED RETRAINING TRIGGERS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RETRAINING TRIGGER ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "retraining_recommendations = {}\n",
    "\n",
    "# Thresholds for retraining\n",
    "PERFORMANCE_DEGRADATION_THRESHOLD = 0.20  # 20% worse than baseline\n",
    "DISTRIBUTION_SHIFT_THRESHOLD = 1.5  # 1.5 std dev shift\n",
    "ERROR_INCREASE_THRESHOLD = 0.15  # 15% increase in recent errors\n",
    "\n",
    "def check_retraining_needed(horizon, metrics, baselines, error_analysis_data):\n",
    "    \"\"\"Check if model needs retraining based on multiple signals\"\"\"\n",
    "    signals = []\n",
    "    should_retrain = False\n",
    "    \n",
    "    # 1. Check vs holdout baseline\n",
    "    if 'holdout_best' in baselines.get(horizon.replace('24h', '4h'), {}):\n",
    "        holdout_baseline = baselines[horizon.replace('24h', '4h')]['holdout_best']\n",
    "        model_mae = metrics['mae']\n",
    "        \n",
    "        if model_mae > holdout_baseline:\n",
    "            degradation = (model_mae - holdout_baseline) / holdout_baseline\n",
    "            signals.append(f\"Model worse than baseline by {degradation*100:.1f}%\")\n",
    "            if degradation > PERFORMANCE_DEGRADATION_THRESHOLD:\n",
    "                should_retrain = True\n",
    "    \n",
    "    # 2. Check distribution shift\n",
    "    if DISTRIBUTION_SHIFT_DETECTED:\n",
    "        signals.append(f\"Distribution shift detected (magnitude: {SHIFT_MAGNITUDE:.2f})\")\n",
    "        if SHIFT_MAGNITUDE > DISTRIBUTION_SHIFT_THRESHOLD:\n",
    "            should_retrain = True\n",
    "    \n",
    "    # 3. Check error patterns\n",
    "    if horizon in error_analysis:\n",
    "        ea = error_analysis[horizon]\n",
    "        \n",
    "        # Check for systematic bias\n",
    "        if abs(ea['mean_error']) > 0.1:\n",
    "            signals.append(f\"Systematic bias: {ea['mean_error']:.4f}\")\n",
    "            should_retrain = True\n",
    "        \n",
    "        # Check P99 errors (catastrophic failures)\n",
    "        if ea['p99_error'] > 3 * ea['mae']:\n",
    "            signals.append(f\"High P99 error: {ea['p99_error']:.4f} (3x MAE)\")\n",
    "    \n",
    "    # 4. Check regime-specific degradation\n",
    "    if horizon in error_analysis and 'regime_errors' in error_analysis[horizon]:\n",
    "        regime_errors = error_analysis[horizon]['regime_errors']\n",
    "        if 'spike' in regime_errors and 'normal' in regime_errors:\n",
    "            spike_ratio = regime_errors['spike'] / (regime_errors['normal'] + 1e-8)\n",
    "            if spike_ratio > 3:\n",
    "                signals.append(f\"Spike regime error {spike_ratio:.1f}x worse than normal\")\n",
    "    \n",
    "    return should_retrain, signals\n",
    "\n",
    "for horizon in ['1h', '4h', '24h']:\n",
    "    if horizon not in trained_models:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{horizon} Retraining Analysis:\")\n",
    "    \n",
    "    metrics = trained_models[horizon]['metrics']\n",
    "    should_retrain, signals = check_retraining_needed(\n",
    "        horizon, metrics, BASELINES, \n",
    "        error_analysis if 'error_analysis' in dir() else {}\n",
    "    )\n",
    "    \n",
    "    retraining_recommendations[horizon] = {\n",
    "        'should_retrain': should_retrain,\n",
    "        'signals': signals,\n",
    "        'urgency': 'high' if should_retrain and len(signals) > 2 else 'medium' if should_retrain else 'low'\n",
    "    }\n",
    "    \n",
    "    if signals:\n",
    "        for signal in signals:\n",
    "            print(f\"  \u26a0\ufe0f {signal}\")\n",
    "    else:\n",
    "        print(f\"  \u2713 No retraining signals detected\")\n",
    "    \n",
    "    if should_retrain:\n",
    "        print(f\"  >>> RECOMMENDATION: Retrain {horizon} model\")\n",
    "    else:\n",
    "        print(f\"  >>> Model OK, no retraining needed\")\n",
    "\n",
    "# Overall recommendation\n",
    "any_retrain = any(r['should_retrain'] for r in retraining_recommendations.values())\n",
    "high_urgency = any(r['urgency'] == 'high' for r in retraining_recommendations.values())\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"OVERALL RETRAINING RECOMMENDATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if high_urgency:\n",
    "    print(\"\ud83d\udd34 HIGH URGENCY: Models show significant degradation\")\n",
    "    print(\"   Recommendation: Retrain immediately with fresh data\")\n",
    "elif any_retrain:\n",
    "    print(\"\ud83d\udfe1 MEDIUM: Some models could benefit from retraining\")\n",
    "    print(\"   Recommendation: Schedule retraining when convenient\")\n",
    "else:\n",
    "    print(\"\ud83d\udfe2 LOW: Models performing within acceptable parameters\")\n",
    "    print(\"   Recommendation: Continue monitoring, retrain in 1-2 weeks\")\n",
    "\n",
    "\n",
    "# Export retraining trigger\n",
    "try:\n",
    "    import json as json_export\n",
    "    trigger = {'recommendation': 'retrain_now' if high_urgency else ('schedule' if any_retrain else 'ok')}\n",
    "    with open('/content/retraining_needed.json', 'w') as f:\n",
    "        json_export.dump(trigger, f)\n",
    "    print(\"\\n\u2713 Saved /content/retraining_needed.json\")\n",
    "except: pass\n",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# BACKTESTING FRAMEWORK + ONLINE LEARNING HOOKS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BACKTESTING + ONLINE LEARNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def run_backtest(model, scaler, features, df, target_col, conformal_interval=None, \n",
    "                 time_intervals=None, window_days=3):\n",
    "    \"\"\"\n",
    "    Run walk-forward backtest with interval coverage verification.\n",
    "    \"\"\"\n",
    "    rph = 120\n",
    "    window_size = window_days * 24 * rph\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(window_size, len(df) - rph, rph):  # Step by 1 hour\n",
    "        X_current = df[features].iloc[i:i+1]\n",
    "        \n",
    "        if i + rph < len(df):\n",
    "            y_actual = df[target_col].iloc[i + rph] if target_col in df.columns else np.nan\n",
    "        else:\n",
    "            y_actual = np.nan\n",
    "        \n",
    "        if pd.isna(y_actual) or X_current.isna().any().any():\n",
    "            continue\n",
    "        \n",
    "        # Predict\n",
    "        X_scaled = scaler.transform(X_current)\n",
    "        y_pred = model.predict(X_scaled)[0]\n",
    "        \n",
    "        error = abs(y_actual - y_pred)\n",
    "        \n",
    "        # Get hour for time-specific interval\n",
    "        hour = df.index[i].hour if hasattr(df.index[i], 'hour') else 12\n",
    "        period = 'afternoon' if 12 <= hour < 18 else (\n",
    "            'morning' if 6 <= hour < 12 else (\n",
    "            'evening' if 18 <= hour < 24 else 'night'))\n",
    "        \n",
    "        # Compute interval coverage\n",
    "        if conformal_interval:\n",
    "            base_interval = conformal_interval\n",
    "            if time_intervals and period in time_intervals:\n",
    "                interval = time_intervals[period].get('interval_80', base_interval)\n",
    "            else:\n",
    "                interval = base_interval\n",
    "            in_interval = error <= interval\n",
    "        else:\n",
    "            in_interval = None\n",
    "            interval = None\n",
    "        \n",
    "        results.append({\n",
    "            'timestamp': df.index[i],\n",
    "            'hour': hour,\n",
    "            'period': period,\n",
    "            'actual': y_actual,\n",
    "            'predicted': y_pred,\n",
    "            'error': error,\n",
    "            'signed_error': y_actual - y_pred,\n",
    "            'current_gas': df['gas'].iloc[i] if 'gas' in df.columns else np.nan,\n",
    "            'in_interval': in_interval,\n",
    "            'interval_width': interval\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def compute_online_bias_update(recent_errors, decay=0.95, max_history=50):\n",
    "    \"\"\"\n",
    "    Compute exponentially weighted bias update from recent errors.\n",
    "    Can be used to update bias correction factors online.\n",
    "    \"\"\"\n",
    "    if len(recent_errors) == 0:\n",
    "        return {'bias': 0.0, 'confidence': 0.0}\n",
    "    \n",
    "    n = min(len(recent_errors), max_history)\n",
    "    errors = np.array(recent_errors[-n:])\n",
    "    \n",
    "    # Exponential weights\n",
    "    weights = np.array([decay ** (n - i - 1) for i in range(n)])\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    weighted_bias = np.sum(errors * weights)\n",
    "    weighted_abs_error = np.sum(np.abs(errors) * weights)\n",
    "    \n",
    "    # Confidence based on consistency\n",
    "    error_std = np.std(errors)\n",
    "    confidence = 1.0 / (1.0 + error_std)\n",
    "    \n",
    "    return {\n",
    "        'bias': float(weighted_bias),\n",
    "        'mae': float(weighted_abs_error),\n",
    "        'confidence': float(confidence),\n",
    "        'n_samples': n\n",
    "    }\n",
    "\n",
    "class OnlineBiasTracker:\n",
    "    \"\"\"Track and update bias corrections online.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_bias_factors=None, decay=0.95, max_history=100):\n",
    "        self.decay = decay\n",
    "        self.max_history = max_history\n",
    "        self.errors_by_period = {\n",
    "            'night': [], 'morning': [], 'afternoon': [], 'evening': []\n",
    "        }\n",
    "        self.errors_overall = []\n",
    "        self.bias_factors = initial_bias_factors or {}\n",
    "    \n",
    "    def update(self, signed_error, hour):\n",
    "        \"\"\"Update with new prediction error.\"\"\"\n",
    "        period = 'afternoon' if 12 <= hour < 18 else (\n",
    "            'morning' if 6 <= hour < 12 else (\n",
    "            'evening' if 18 <= hour < 24 else 'night'))\n",
    "        \n",
    "        self.errors_overall.append(signed_error)\n",
    "        self.errors_by_period[period].append(signed_error)\n",
    "        \n",
    "        # Trim history\n",
    "        if len(self.errors_overall) > self.max_history:\n",
    "            self.errors_overall.pop(0)\n",
    "        if len(self.errors_by_period[period]) > self.max_history // 4:\n",
    "            self.errors_by_period[period].pop(0)\n",
    "    \n",
    "    def get_current_bias(self, period=None):\n",
    "        \"\"\"Get current estimated bias for a period or overall.\"\"\"\n",
    "        if period and period in self.errors_by_period:\n",
    "            return compute_online_bias_update(self.errors_by_period[period], self.decay)\n",
    "        return compute_online_bias_update(self.errors_overall, self.decay)\n",
    "    \n",
    "    def should_alert(self, threshold=0.1):\n",
    "        \"\"\"Check if any period has significant bias drift.\"\"\"\n",
    "        alerts = []\n",
    "        for period, errors in self.errors_by_period.items():\n",
    "            if len(errors) >= 10:\n",
    "                bias_info = compute_online_bias_update(errors, self.decay)\n",
    "                if abs(bias_info['bias']) > threshold and bias_info['confidence'] > 0.5:\n",
    "                    alerts.append({\n",
    "                        'period': period,\n",
    "                        'bias': bias_info['bias'],\n",
    "                        'confidence': bias_info['confidence']\n",
    "                    })\n",
    "        return alerts\n",
    "\n",
    "# Store online trackers for each model\n",
    "online_trackers = {}\n",
    "\n",
    "backtest_results = {}\n",
    "\n",
    "for horizon in ['1h', '4h']:\n",
    "    if horizon not in trained_models:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{horizon} Backtest + Online Learning Simulation\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    data = trained_models[horizon]\n",
    "    model = data['model']\n",
    "    scaler = data['scaler']\n",
    "    features = data['features']\n",
    "    \n",
    "    target_col = f'target_{horizon}'\n",
    "    \n",
    "    if target_col not in df_clean.columns:\n",
    "        print(f\"  \u26a0\ufe0f Target column not found\")\n",
    "        continue\n",
    "    \n",
    "    # Get calibration data\n",
    "    conformal_interval = conformal_residuals.get(horizon, {}).get('quantile')\n",
    "    time_intervals = time_period_calibration.get(horizon, {}).get('time', {})\n",
    "    \n",
    "    # Run backtest\n",
    "    bt_results = run_backtest(\n",
    "        model, scaler, features, df_clean, target_col, \n",
    "        conformal_interval, time_intervals, window_days=3\n",
    "    )\n",
    "    \n",
    "    if len(bt_results) < 100:\n",
    "        print(f\"  \u26a0\ufe0f Insufficient backtest data ({len(bt_results)} points)\")\n",
    "        continue\n",
    "    \n",
    "    # === BASIC METRICS ===\n",
    "    mae = bt_results['error'].mean()\n",
    "    rmse = np.sqrt((bt_results['error'] ** 2).mean())\n",
    "    naive_errors = abs(bt_results['actual'] - bt_results['current_gas'])\n",
    "    naive_mae = naive_errors.mean()\n",
    "    improvement = (naive_mae - mae) / naive_mae * 100\n",
    "    \n",
    "    print(f\"\\n  Basic Metrics:\")\n",
    "    print(f\"    Backtest samples: {len(bt_results)}\")\n",
    "    print(f\"    Model MAE: {mae:.4f}, Naive MAE: {naive_mae:.4f}\")\n",
    "    print(f\"    Improvement vs naive: {improvement:+.1f}%\")\n",
    "    \n",
    "    # === INTERVAL COVERAGE ===\n",
    "    if 'in_interval' in bt_results.columns and bt_results['in_interval'].notna().any():\n",
    "        overall_coverage = bt_results['in_interval'].mean()\n",
    "        print(f\"\\n  Interval Coverage:\")\n",
    "        print(f\"    Overall: {overall_coverage:.1%} (target: 80%)\")\n",
    "        \n",
    "        for period in ['night', 'morning', 'afternoon', 'evening']:\n",
    "            period_mask = bt_results['period'] == period\n",
    "            if period_mask.sum() >= 20:\n",
    "                period_coverage = bt_results.loc[period_mask, 'in_interval'].mean()\n",
    "                status = \"\u2713\" if period_coverage >= 0.75 else \"\u26a0\ufe0f\"\n",
    "                print(f\"    {period}: {period_coverage:.1%} {status}\")\n",
    "    \n",
    "    # === ROLLING PERFORMANCE ===\n",
    "    bt_results['date'] = bt_results['timestamp'].dt.date\n",
    "    daily_mae = bt_results.groupby('date')['error'].mean()\n",
    "    \n",
    "    print(f\"\\n  Rolling Performance:\")\n",
    "    print(f\"    Daily MAE range: {daily_mae.min():.4f} - {daily_mae.max():.4f}\")\n",
    "    \n",
    "    # Performance trend\n",
    "    if len(daily_mae) >= 3:\n",
    "        first_half = daily_mae.iloc[:len(daily_mae)//2].mean()\n",
    "        second_half = daily_mae.iloc[len(daily_mae)//2:].mean()\n",
    "        trend = (second_half - first_half) / first_half * 100\n",
    "        trend_str = \"improving \u2193\" if trend < -5 else (\"degrading \u2191\" if trend > 5 else \"stable \u2192\")\n",
    "        print(f\"    Trend: {trend:+.1f}% ({trend_str})\")\n",
    "    \n",
    "    # === SIMULATE ONLINE LEARNING ===\n",
    "    print(f\"\\n  Online Learning Simulation:\")\n",
    "    tracker = OnlineBiasTracker(\n",
    "        initial_bias_factors=bias_correction_factors.get(horizon),\n",
    "        decay=0.95\n",
    "    )\n",
    "    \n",
    "    # Simulate updates\n",
    "    for _, row in bt_results.iterrows():\n",
    "        tracker.update(row['signed_error'], row['hour'])\n",
    "    \n",
    "    # Check for bias drift alerts\n",
    "    alerts = tracker.should_alert(threshold=0.05)\n",
    "    if alerts:\n",
    "        print(f\"    \u26a0\ufe0f Bias drift detected:\")\n",
    "        for alert in alerts:\n",
    "            print(f\"      {alert['period']}: bias={alert['bias']:+.4f} (conf={alert['confidence']:.2f})\")\n",
    "    else:\n",
    "        print(f\"    \u2713 No significant bias drift\")\n",
    "    \n",
    "    # Show current online bias estimates\n",
    "    print(f\"\\n  Online Bias Estimates:\")\n",
    "    for period in ['night', 'morning', 'afternoon', 'evening']:\n",
    "        bias_info = tracker.get_current_bias(period)\n",
    "        if bias_info['n_samples'] >= 5:\n",
    "            print(f\"    {period}: {bias_info['bias']:+.4f} (n={bias_info['n_samples']})\")\n",
    "    \n",
    "    online_trackers[horizon] = tracker\n",
    "    \n",
    "    # Store results\n",
    "    backtest_results[horizon] = {\n",
    "        'n_samples': len(bt_results),\n",
    "        'mae': float(mae),\n",
    "        'rmse': float(rmse),\n",
    "        'naive_mae': float(naive_mae),\n",
    "        'improvement_pct': float(improvement),\n",
    "        'interval_coverage': float(bt_results['in_interval'].mean()) if 'in_interval' in bt_results.columns else None,\n",
    "        'daily_mae_min': float(daily_mae.min()),\n",
    "        'daily_mae_max': float(daily_mae.max()),\n",
    "        'online_alerts': alerts\n",
    "    }\n",
    "    \n",
    "    # Store tracker for saving\n",
    "    trained_models[horizon]['online_tracker'] = tracker\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BACKTEST SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for horizon, results in backtest_results.items():\n",
    "    print(f\"\\n{horizon}:\")\n",
    "    print(f\"  MAE: {results['mae']:.4f} ({results['improvement_pct']:+.1f}% vs naive)\")\n",
    "    if results.get('interval_coverage'):\n",
    "        print(f\"  Coverage: {results['interval_coverage']:.1%}\")\n",
    "    if results.get('online_alerts'):\n",
    "        print(f\"  \u26a0\ufe0f Alerts: {len(results['online_alerts'])} periods with bias drift\")\n",
    "\n",
    "print(f\"\\n\u2713 Backtesting and online learning simulation complete\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# SHAP EXPLANATIONS FOR MODEL INTERPRETABILITY\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SHAP EXPLANATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if SHAP is available\n",
    "try:\n",
    "    import shap\n",
    "    HAS_SHAP = True\n",
    "    print(\"\u2713 SHAP library available\")\n",
    "except ImportError:\n",
    "    HAS_SHAP = False\n",
    "    print(\"\u26a0\ufe0f SHAP not available. Install with: pip install shap\")\n",
    "    print(\"   Skipping SHAP analysis\")\n",
    "\n",
    "shap_explainers = {}\n",
    "\n",
    "if HAS_SHAP:\n",
    "    for horizon in ['1h', '4h']:\n",
    "        if horizon not in trained_models:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{horizon} SHAP Analysis...\")\n",
    "        \n",
    "        data = trained_models[horizon]\n",
    "        model = data['model']\n",
    "        scaler = data['scaler']\n",
    "        features = data['features']\n",
    "        \n",
    "        # Get sample of training data for background\n",
    "        X_sample = df_train_val[features].sample(min(500, len(df_train_val)), random_state=42)\n",
    "        X_sample_scaled = scaler.transform(X_sample)\n",
    "        \n",
    "        try:\n",
    "            # Create SHAP explainer based on model type\n",
    "            model_name = data['metrics']['name']\n",
    "            \n",
    "            if 'RF' in model_name or 'GBM' in model_name:\n",
    "                # Tree-based model\n",
    "                explainer = shap.TreeExplainer(model)\n",
    "                shap_values = explainer.shap_values(X_sample_scaled[:100])\n",
    "            else:\n",
    "                # Linear or other model - use KernelExplainer\n",
    "                explainer = shap.KernelExplainer(model.predict, X_sample_scaled[:50])\n",
    "                shap_values = explainer.shap_values(X_sample_scaled[:50])\n",
    "            \n",
    "            # Calculate mean absolute SHAP values\n",
    "            if isinstance(shap_values, list):\n",
    "                shap_values = shap_values[0]\n",
    "            \n",
    "            mean_shap = np.abs(shap_values).mean(axis=0)\n",
    "            \n",
    "            # Create feature importance from SHAP\n",
    "            shap_importance = dict(zip(features, mean_shap))\n",
    "            sorted_importance = sorted(shap_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            print(f\"  Top 5 features by SHAP importance:\")\n",
    "            for feat, imp in sorted_importance[:5]:\n",
    "                print(f\"    {feat}: {imp:.4f}\")\n",
    "            \n",
    "            shap_explainers[horizon] = {\n",
    "                'explainer': explainer,\n",
    "                'background_data': X_sample_scaled[:50],\n",
    "                'feature_names': features,\n",
    "                'shap_importance': dict(sorted_importance)\n",
    "            }\n",
    "            \n",
    "            # Store in trained_models for later use\n",
    "            trained_models[horizon]['shap_explainer'] = shap_explainers[horizon]\n",
    "            \n",
    "            print(f\"  \u2713 SHAP explainer created\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  \u26a0\ufe0f SHAP analysis failed: {e}\")\n",
    "\n",
    "def explain_prediction(horizon, X_single, feature_names):\n",
    "    \"\"\"Generate explanation for a single prediction\"\"\"\n",
    "    if not HAS_SHAP or horizon not in shap_explainers:\n",
    "        return None\n",
    "    \n",
    "    explainer_data = shap_explainers[horizon]\n",
    "    explainer = explainer_data['explainer']\n",
    "    \n",
    "    try:\n",
    "        shap_values = explainer.shap_values(X_single.reshape(1, -1))\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[0]\n",
    "        \n",
    "        # Create explanation\n",
    "        contributions = list(zip(feature_names, shap_values[0]))\n",
    "        contributions = sorted(contributions, key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        explanation = []\n",
    "        for feat, contrib in contributions[:5]:\n",
    "            direction = \"\u2191\" if contrib > 0 else \"\u2193\"\n",
    "            explanation.append(f\"{feat}: {direction}{abs(contrib):.3f}\")\n",
    "        \n",
    "        return explanation\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "if HAS_SHAP and shap_explainers:\n",
    "    # Example explanation\n",
    "    print(f\"\\nExample prediction explanation:\")\n",
    "    for horizon in shap_explainers:\n",
    "        sample_X = df_train_val[trained_models[horizon]['features']].iloc[-1:].values\n",
    "        sample_X_scaled = trained_models[horizon]['scaler'].transform(sample_X)\n",
    "        \n",
    "        explanation = explain_prediction(horizon, sample_X_scaled[0], trained_models[horizon]['features'])\n",
    "        if explanation:\n",
    "            pred = trained_models[horizon]['model'].predict(sample_X_scaled)[0]\n",
    "            print(f\"  {horizon} prediction = {pred:.4f}\")\n",
    "            print(f\"  Top contributors: {', '.join(explanation[:3])}\")\n",
    "\n",
    "print(f\"\\n\u2713 SHAP explainers created for: {list(shap_explainers.keys())}\")\n",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# DQN AGENT TRAINING (OPTIONAL)\n",
    "# This trains a reinforcement learning agent for transaction timing\n",
    "# Skip if you just need prediction models\n",
    "\n",
    "TRAIN_DQN = False  # Set to True to train DQN agent\n",
    "\n",
    "if not TRAIN_DQN:\n",
    "    print(\"=\"*60)\n",
    "    print(\"DQN TRAINING SKIPPED (set TRAIN_DQN = True to enable)\")\n",
    "    print(\"=\"*60)\n",
    "    DQN_TRAINED = False"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Training Implementation (runs only if TRAIN_DQN = True)\n",
    "\n",
    "if TRAIN_DQN:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING DQN AGENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        import torch.optim as optim\n",
    "        from collections import deque\n",
    "        import random\n",
    "        \n",
    "        class DQNNetwork(nn.Module):\n",
    "            def __init__(self, state_dim, action_dim):\n",
    "                super().__init__()\n",
    "                self.net = nn.Sequential(\n",
    "                    nn.Linear(state_dim, 64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(64, 32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(32, action_dim)\n",
    "                )\n",
    "            \n",
    "            def forward(self, x):\n",
    "                return self.net(x)\n",
    "        \n",
    "        class DQNAgent:\n",
    "            def __init__(self, state_dim, action_dim):\n",
    "                self.state_dim = state_dim\n",
    "                self.action_dim = action_dim\n",
    "                self.epsilon = 1.0\n",
    "                self.epsilon_min = 0.05\n",
    "                self.epsilon_decay = 0.995\n",
    "                self.gamma = 0.99\n",
    "                self.lr = 0.001\n",
    "                self.memory = deque(maxlen=10000)\n",
    "                self.batch_size = 32\n",
    "                self.training_steps = 0\n",
    "                \n",
    "                self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                self.model = DQNNetwork(state_dim, action_dim).to(self.device)\n",
    "                self.target_model = DQNNetwork(state_dim, action_dim).to(self.device)\n",
    "                self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "                self.update_target()\n",
    "            \n",
    "            def update_target(self):\n",
    "                self.target_model.load_state_dict(self.model.state_dict())\n",
    "            \n",
    "            def act(self, state):\n",
    "                if random.random() < self.epsilon:\n",
    "                    return random.randint(0, self.action_dim - 1)\n",
    "                state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    q_values = self.model(state_t)\n",
    "                return q_values.argmax().item()\n",
    "            \n",
    "            def remember(self, state, action, reward, next_state, done):\n",
    "                self.memory.append((state, action, reward, next_state, done))\n",
    "            \n",
    "            def replay(self):\n",
    "                if len(self.memory) < self.batch_size:\n",
    "                    return\n",
    "                \n",
    "                batch = random.sample(self.memory, self.batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                \n",
    "                states = torch.FloatTensor(states).to(self.device)\n",
    "                actions = torch.LongTensor(actions).to(self.device)\n",
    "                rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "                next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "                dones = torch.FloatTensor(dones).to(self.device)\n",
    "                \n",
    "                current_q = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "                next_q = self.target_model(next_states).max(1)[0].detach()\n",
    "                target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "                \n",
    "                loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                self.training_steps += 1\n",
    "                if self.training_steps % 100 == 0:\n",
    "                    self.update_target()\n",
    "                \n",
    "                self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "            \n",
    "            def save(self, path):\n",
    "                torch.save(self.model.state_dict(), path)\n",
    "        \n",
    "        # Create simple environment\n",
    "        state_dim = min(30, len(X.columns))  # Limit state size\n",
    "        action_dim = 2  # 0 = wait, 1 = execute\n",
    "        \n",
    "        DQN_AGENT = DQNAgent(state_dim, action_dim)\n",
    "        \n",
    "        # Train for a few episodes\n",
    "        n_episodes = 500\n",
    "        print(f\"Training DQN for {n_episodes} episodes...\")\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            # Simple training loop\n",
    "            for i in range(min(100, len(X) - 1)):\n",
    "                state = X.iloc[i, :state_dim].values\n",
    "                action = DQN_AGENT.act(state)\n",
    "                \n",
    "                # Simple reward: negative gas price change if executing\n",
    "                next_gas = current_gas.iloc[i + 1] if i + 1 < len(current_gas) else current_gas.iloc[i]\n",
    "                reward = -(next_gas - current_gas.iloc[i]) if action == 1 else -0.001  # Small wait penalty\n",
    "                \n",
    "                next_state = X.iloc[i + 1, :state_dim].values if i + 1 < len(X) else state\n",
    "                done = (i >= min(99, len(X) - 2))\n",
    "                \n",
    "                DQN_AGENT.remember(state, action, reward, next_state, done)\n",
    "                DQN_AGENT.replay()\n",
    "            \n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f\"  Episode {episode + 1}/{n_episodes}, Epsilon: {DQN_AGENT.epsilon:.3f}\")\n",
    "        \n",
    "        DQN_TRAINED = True\n",
    "        DQN_METRICS = {\n",
    "            'episodes': n_episodes,\n",
    "            'training_steps': DQN_AGENT.training_steps,\n",
    "            'final_epsilon': float(DQN_AGENT.epsilon)\n",
    "        }\n",
    "        print(f\"\\n\u2713 DQN training complete ({DQN_AGENT.training_steps} steps)\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\u26a0\ufe0f PyTorch not available, skipping DQN training\")\n",
    "        DQN_TRAINED = False\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f DQN training failed: {e}\")\n",
    "        DQN_TRAINED = False\n",
    "else:\n",
    "    DQN_TRAINED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models + ALL IMPROVEMENTS\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json as json_lib\n",
    "\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING MODELS AND ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# === Save prediction models ===\n",
    "for horizon in ['1h', '4h', '24h']:\n",
    "    if horizon not in trained_models:\n",
    "        continue\n",
    "    \n",
    "    data = trained_models[horizon]\n",
    "    model = data['model']\n",
    "    scaler = data['scaler']\n",
    "    metrics = data['metrics']\n",
    "    features = data.get('features', [])\n",
    "    \n",
    "    model_data = {\n",
    "        'model': model,\n",
    "        'model_name': metrics['name'],\n",
    "        'metrics': {\n",
    "            'mae': float(metrics['mae']),\n",
    "            'improvement': float(metrics['improvement']),\n",
    "            'vs_holdout_baseline': float(metrics['vs_holdout_baseline']) if metrics.get('vs_holdout_baseline') else None,\n",
    "            'passed_baseline': bool(metrics.get('passed_baseline', False)),\n",
    "            'is_ensemble': bool(metrics.get('is_ensemble', False)),\n",
    "        },\n",
    "        'trained_at': datetime.now().isoformat(),\n",
    "        'feature_names': list(features),\n",
    "        'feature_scaler': scaler,\n",
    "    }\n",
    "    \n",
    "    if metrics.get('is_ensemble'):\n",
    "        model_data['ensemble_weights'] = metrics.get('ensemble_weights')\n",
    "    \n",
    "    # Add bias correction if available\n",
    "    if data.get('use_bias_correction') and horizon in bias_correction_factors:\n",
    "        model_data['bias_correction'] = bias_correction_factors[horizon]\n",
    "        model_data['use_bias_correction'] = True\n",
    "    \n",
    "    # Add bias-corrected model wrapper\n",
    "    if 'bias_corrected_model' in data:\n",
    "        model_data['bias_corrected_model'] = data['bias_corrected_model']\n",
    "    \n",
    "    # Add time-adaptive predictor\n",
    "    if 'time_adaptive_predict' in data:\n",
    "        model_data['has_time_adaptive'] = True\n",
    "    \n",
    "    # Add confidence scoring info\n",
    "    if data.get('has_confidence_scoring'):\n",
    "        model_data['has_confidence_scoring'] = True\n",
    "    \n",
    "    # Add online tracker initial state\n",
    "    if 'online_tracker' in data:\n",
    "        tracker = data['online_tracker']\n",
    "        model_data['online_bias_state'] = {\n",
    "            'overall': tracker.get_current_bias(),\n",
    "            'by_period': {p: tracker.get_current_bias(p) for p in ['night', 'morning', 'afternoon', 'evening']}\n",
    "        }\n",
    "    \n",
    "    if 'conformal_residuals' in dir() and horizon in conformal_residuals:\n",
    "        model_data['conformal_interval'] = float(conformal_residuals[horizon]['quantile'])\n",
    "        model_data['conformal_calibration_source'] = conformal_residuals[horizon].get('calibration_source', 'training')\n",
    "    \n",
    "    if 'time_period_calibration' in dir() and horizon in time_period_calibration:\n",
    "        model_data['time_calibration'] = time_period_calibration[horizon]\n",
    "    \n",
    "    joblib.dump(model_data, f'saved_models/model_{horizon}.pkl')\n",
    "    print(f\"\u2713 model_{horizon}.pkl ({metrics['name']}, {len(features)} features)\")\n",
    "    joblib.dump(scaler, f'saved_models/scaler_{horizon}.pkl')\n",
    "\n",
    "# === Save time-specific models ===\n",
    "if 'time_specific_models' in dir() and time_specific_models:\n",
    "    os.makedirs('saved_models/time_models', exist_ok=True)\n",
    "    for horizon, periods in time_specific_models.items():\n",
    "        for period, pdata in periods.items():\n",
    "            filename = f'saved_models/time_models/model_{horizon}_{period}.pkl'\n",
    "            joblib.dump(pdata, filename)\n",
    "            print(f\"  \u2192 {horizon}_{period} time model\")\n",
    "\n",
    "# === Save regime-specific models ===\n",
    "if 'regime_specific_models' in dir() and regime_specific_models:\n",
    "    os.makedirs('saved_models/regime_models', exist_ok=True)\n",
    "    for horizon, regime_dict in regime_specific_models.items():\n",
    "        for regime_val, regime_data in regime_dict.items():\n",
    "            filename = f'saved_models/regime_models/model_{horizon}_{regime_data[\"regime_name\"]}.pkl'\n",
    "            joblib.dump(regime_data, filename)\n",
    "            print(f\"  \u2192 {horizon}_{regime_data['regime_name']} regime\")\n",
    "\n",
    "# === Save spike forecast models ===\n",
    "if 'spike_forecast_models' in dir() and spike_forecast_models:\n",
    "    for horizon, sf_data in spike_forecast_models.items():\n",
    "        sf_save = {\n",
    "            'model': sf_data.get('model'),\n",
    "            'scaler': sf_data.get('scaler'),\n",
    "            'features': sf_data.get('features', []),\n",
    "            'threshold_config': sf_data.get('threshold_config'),\n",
    "            'optimal_threshold': sf_data.get('optimal_threshold', 0.5),\n",
    "            'metrics': {k: v for k, v in sf_data.items() if k not in ['model', 'scaler', 'features', 'threshold_config']}\n",
    "        }\n",
    "        joblib.dump(sf_save, f'saved_models/spike_forecast_{horizon}.pkl')\n",
    "        if sf_data.get('model') is not None:\n",
    "            print(f\"\u2713 spike_forecast_{horizon}.pkl (AUC: {sf_data.get('auc', 0.5):.3f})\")\n",
    "        else:\n",
    "            print(f\"\u26a0\ufe0f spike_forecast_{horizon}.pkl (no model)\")\n",
    "\n",
    "# === Save other files ===\n",
    "default_features = trained_models.get('4h', trained_models.get('1h', {})).get('features', [])\n",
    "joblib.dump(list(default_features), 'saved_models/feature_names.pkl')\n",
    "\n",
    "if 'regime_clf' in dir() and regime_clf is not None:\n",
    "    joblib.dump({'model': regime_clf, 'scaler': regime_scaler, 'accuracy': regime_accuracy}, \n",
    "                'saved_models/regime_detector.pkl')\n",
    "\n",
    "if 'quantile_models' in dir() and quantile_models:\n",
    "    for horizon, (q_models, q_scaler) in quantile_models.items():\n",
    "        quantile_data = {'models': q_models, 'scaler': q_scaler, 'quantiles': [0.1, 0.5, 0.9]}\n",
    "        if 'conformal_residuals' in dir() and horizon in conformal_residuals:\n",
    "            quantile_data['conformal'] = {\n",
    "                'interval_width': float(conformal_residuals[horizon]['quantile']),\n",
    "                'calibration_source': conformal_residuals[horizon].get('calibration_source', 'training')\n",
    "            }\n",
    "        if 'time_period_calibration' in dir() and horizon in time_period_calibration:\n",
    "            quantile_data['time_calibration'] = time_period_calibration[horizon]\n",
    "        joblib.dump(quantile_data, f'saved_models/quantile_{horizon}.pkl')\n",
    "\n",
    "# === Save training metadata ===\n",
    "def convert_to_python_types(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_python_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_python_types(v) for v in obj]\n",
    "    elif isinstance(obj, (np.bool_, np.integer)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif hasattr(obj, 'item'):\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "metadata = {\n",
    "    'training_timestamp': datetime.now().isoformat(),\n",
    "    'total_samples': len(df_clean),\n",
    "    'training_samples': len(df_train_val),\n",
    "    'holdout_samples': len(df_holdout) if df_holdout is not None else 0,\n",
    "    'date_range': f\"{df_clean.index.min()} to {df_clean.index.max()}\",\n",
    "    'selection_method': 'holdout-based',\n",
    "    'configuration': {\n",
    "        'target_transform': TARGET_TRANSFORM_USED if 'TARGET_TRANSFORM_USED' in dir() else 'none',\n",
    "        'use_rolling_window': USE_ROLLING_WINDOW if 'USE_ROLLING_WINDOW' in dir() else False,\n",
    "        'rolling_window_days': ROLLING_WINDOW_DAYS if 'ROLLING_WINDOW_DAYS' in dir() else None,\n",
    "        'distribution_shift_detected': DISTRIBUTION_SHIFT_DETECTED if 'DISTRIBUTION_SHIFT_DETECTED' in dir() else False,\n",
    "        'shift_magnitude': SHIFT_MAGNITUDE if 'SHIFT_MAGNITUDE' in dir() else 0,\n",
    "        'feature_pruning_enabled': ENABLE_FEATURE_PRUNING if 'ENABLE_FEATURE_PRUNING' in dir() else False,\n",
    "        'ensemble_blending_enabled': USE_ENSEMBLE_BLENDING if 'USE_ENSEMBLE_BLENDING' in dir() else False,\n",
    "        'regularization_strength': REGULARIZATION_STRENGTH if 'REGULARIZATION_STRENGTH' in dir() else 0,\n",
    "        'time_specific_models_enabled': TRAIN_TIME_SPECIFIC_MODELS if 'TRAIN_TIME_SPECIFIC_MODELS' in dir() else False,\n",
    "    },\n",
    "    'sample_weighting': {\n",
    "        'night_multiplier': 2.0,\n",
    "        'recency_enabled': True,\n",
    "        'recency_range': [0.7, 1.3]\n",
    "    },\n",
    "    'success_criteria': {\n",
    "        'night_coverage_target': 0.75,\n",
    "        'night_coverage_stretch': 0.80,\n",
    "        'baseline_beat_margin': 0.05,\n",
    "        'spike_recall_target': 0.60,\n",
    "    },\n",
    "    'features': {'count': len(default_features), 'list': list(default_features)},\n",
    "    'baselines': BASELINES,\n",
    "    'models': {},\n",
    "    'time_specific_models': {},\n",
    "    'regime_models': {},\n",
    "    'direction_models': {},\n",
    "    'spike_forecast_models': {},\n",
    "    'error_analysis': {},\n",
    "    'bias_correction': {},\n",
    "    'calibration': {},\n",
    "    'retraining_recommendations': {},\n",
    "    'backtest_results': {}\n",
    "}\n",
    "\n",
    "# Model info\n",
    "for horizon, data in trained_models.items():\n",
    "    m = data['metrics']\n",
    "    metadata['models'][horizon] = {\n",
    "        'name': m['name'],\n",
    "        'mae': float(m['mae']),\n",
    "        'improvement_pct': float(m['improvement'] * 100),\n",
    "        'vs_holdout_baseline_pct': float(m['vs_holdout_baseline'] * 100) if m.get('vs_holdout_baseline') else None,\n",
    "        'passed_baseline': bool(m.get('passed_baseline', False)),\n",
    "        'is_fallback': data.get('is_fallback', False),\n",
    "        'is_ensemble': m.get('is_ensemble', False),\n",
    "        'has_bias_correction': data.get('use_bias_correction', False),\n",
    "        'has_time_adaptive': 'time_adaptive_predict' in data,\n",
    "        'n_features': len(data.get('features', []))\n",
    "    }\n",
    "    if 'calibration' in data:\n",
    "        metadata['models'][horizon]['calibration'] = data['calibration']\n",
    "\n",
    "# Time-specific models\n",
    "if 'time_specific_models' in dir() and time_specific_models:\n",
    "    for horizon, periods in time_specific_models.items():\n",
    "        metadata['time_specific_models'][horizon] = {\n",
    "            period: {'mae': float(p['mae']), 'improvement': float(p['improvement'])}\n",
    "            for period, p in periods.items()\n",
    "        }\n",
    "\n",
    "# Regime models\n",
    "if 'regime_specific_models' in dir() and regime_specific_models:\n",
    "    for horizon, regime_dict in regime_specific_models.items():\n",
    "        metadata['regime_models'][horizon] = {}\n",
    "        for regime_val, regime_data in regime_dict.items():\n",
    "            metadata['regime_models'][horizon][regime_data['regime_name']] = {\n",
    "                'mae': float(regime_data['metrics']['mae']),\n",
    "                'n_samples': int(regime_data['n_samples'])\n",
    "            }\n",
    "\n",
    "# Direction models\n",
    "if 'direction_models' in dir() and direction_models:\n",
    "    for horizon, data in direction_models.items():\n",
    "        metadata['direction_models'][horizon] = {\n",
    "            'accuracy': float(data['accuracy']),\n",
    "            'f1_score': float(data['f1_score']),\n",
    "            'baseline_accuracy': float(data.get('baseline_accuracy', 0)),\n",
    "            'improvement_vs_baseline': float(data.get('improvement_vs_baseline', 0)),\n",
    "        }\n",
    "\n",
    "# Spike forecast\n",
    "if 'spike_forecast_models' in dir() and spike_forecast_models:\n",
    "    for horizon, sf_data in spike_forecast_models.items():\n",
    "        metadata['spike_forecast_models'][horizon] = {\n",
    "            'has_model': sf_data.get('model') is not None,\n",
    "            'precision': float(sf_data.get('precision', 0)),\n",
    "            'recall': float(sf_data.get('recall', 0)),\n",
    "            'f1_score': float(sf_data.get('f1_score', 0)),\n",
    "            'auc': float(sf_data.get('auc', 0.5)),\n",
    "            'threshold_config': sf_data.get('threshold_config', {}).get('name') if sf_data.get('threshold_config') else None,\n",
    "        }\n",
    "\n",
    "# Error analysis\n",
    "if 'error_analysis' in dir() and error_analysis:\n",
    "    metadata['error_analysis'] = error_analysis\n",
    "\n",
    "# Bias correction\n",
    "if 'bias_correction_factors' in dir() and bias_correction_factors:\n",
    "    metadata['bias_correction'] = bias_correction_factors\n",
    "\n",
    "# Calibration info\n",
    "if 'conformal_residuals' in dir():\n",
    "    for horizon in conformal_residuals:\n",
    "        if horizon not in metadata['calibration']:\n",
    "            metadata['calibration'][horizon] = {}\n",
    "        metadata['calibration'][horizon]['conformal_width'] = float(conformal_residuals[horizon]['quantile'])\n",
    "        metadata['calibration'][horizon]['source'] = conformal_residuals[horizon].get('calibration_source', 'training')\n",
    "\n",
    "# Tail risk caps for inference\n",
    "if 'tail_risk_caps' in dir() and tail_risk_caps:\n",
    "    metadata['tail_risk_caps'] = tail_risk_caps\n",
    "\n",
    "# Production monitoring config\n",
    "metadata['monitoring_config'] = {\n",
    "    'enable_online_bias': True,\n",
    "    'bias_window_hours': 24,\n",
    "    'bias_threshold': 0.05,  # Apply correction if bias > 5%\n",
    "    'max_bias_correction': 0.15,  # Never correct more than 15%\n",
    "    'track_by_hour': True,\n",
    "    'track_by_regime': True\n",
    "}\n",
    "\n",
    "# Retraining recommendations\n",
    "if 'retraining_recommendations' in dir() and retraining_recommendations:\n",
    "    metadata['retraining_recommendations'] = retraining_recommendations\n",
    "\n",
    "# Backtest results\n",
    "if 'backtest_results' in dir() and backtest_results:\n",
    "    metadata['backtest_results'] = {h: {k: v for k, v in r.items() if k != 'online_alerts'} \n",
    "                                    for h, r in backtest_results.items()}\n",
    "\n",
    "metadata = convert_to_python_types(metadata)\n",
    "\n",
    "with open('saved_models/training_metadata.json', 'w') as f:\n",
    "    json_lib.dump(metadata, f, indent=2)\n",
    "print(f\"\\n\u2713 training_metadata.json\")\n",
    "\n",
    "# Feature importance\n",
    "if 'FEATURE_IMPORTANCE' in dir() and FEATURE_IMPORTANCE:\n",
    "    sorted_importance = dict(sorted(FEATURE_IMPORTANCE.items(), key=lambda x: x[1], reverse=True))\n",
    "    with open('saved_models/feature_importance.json', 'w') as f:\n",
    "        json_lib.dump(convert_to_python_types(sorted_importance), f, indent=2)\n",
    "    print(f\"\u2713 feature_importance.json\")\n",
    "\n",
    "# Training history\n",
    "history_file = 'saved_models/training_history.json'\n",
    "if os.path.exists(history_file):\n",
    "    with open(history_file) as f:\n",
    "        history = json_lib.load(f)\n",
    "else:\n",
    "    history = []\n",
    "\n",
    "current_run = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'models': {h: {\n",
    "        'name': trained_models[h]['metrics']['name'], \n",
    "        'mae': float(trained_models[h]['metrics']['mae']),\n",
    "        'is_ensemble': trained_models[h]['metrics'].get('is_ensemble', False),\n",
    "        'has_bias_correction': trained_models[h].get('use_bias_correction', False)\n",
    "    } for h in trained_models},\n",
    "    'improvements': {\n",
    "        'ensemble_blending': USE_ENSEMBLE_BLENDING if 'USE_ENSEMBLE_BLENDING' in dir() else False,\n",
    "        'time_specific_models': bool(time_specific_models) if 'time_specific_models' in dir() else False,\n",
    "        'bias_correction': any(trained_models.get(h, {}).get('use_bias_correction', False) for h in ['1h', '4h']),\n",
    "        'holdout_calibration': True\n",
    "    }\n",
    "}\n",
    "\n",
    "history.append(current_run)\n",
    "history = history[-10:]\n",
    "\n",
    "with open(history_file, 'w') as f:\n",
    "    json_lib.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for horizon, data in trained_models.items():\n",
    "    m = data['metrics']\n",
    "    extras = []\n",
    "    if m.get('is_ensemble'):\n",
    "        extras.append(\"ensemble\")\n",
    "    if data.get('use_bias_correction'):\n",
    "        extras.append(\"bias-corrected\")\n",
    "    if 'time_adaptive_predict' in data:\n",
    "        extras.append(\"time-adaptive\")\n",
    "    \n",
    "    extras_str = f\" [{', '.join(extras)}]\" if extras else \"\"\n",
    "    print(f\"\u2713 {horizon}: {m['name']} | MAE: {m['mae']:.4f}{extras_str}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL MODELS SAVED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE - FINAL REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_days = len(df_clean) / (120 * 24)\n",
    "\n",
    "print(f\"\\nDATA SUMMARY\")\n",
    "print(f\"   Total samples: {len(df_clean):,} ({total_days:.1f} days)\")\n",
    "print(f\"   Training: {len(df_train_val):,} | Holdout: {len(df_holdout) if df_holdout is not None else 0:,}\")\n",
    "print(f\"   Date range: {df_clean.index.min()} to {df_clean.index.max()}\")\n",
    "print(f\"   ETH price: {'Binance 1-min \u2713' if HAS_ETH_PRICE else 'Not available'}\")\n",
    "print(f\"   Features: 1h={len(numeric_features_1h)}, 4h={len(numeric_features_4h)}, 24h={len(numeric_features_24h)}\")\n",
    "\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(f\"{'MODEL PERFORMANCE':^70}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Horizon':<8} {'Model':<15} {'CV MAE':>10} {'Holdout':>10} {'vs Base':>10} {'Status':>12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for horizon in ['1h', '4h', '24h']:\n",
    "    if horizon in trained_models:\n",
    "        data = trained_models[horizon]\n",
    "        m = data['metrics']\n",
    "        name = m['name'][:14]\n",
    "        if data.get('is_fallback'):\n",
    "            name = name[:10] + '(fb)'\n",
    "        \n",
    "        cv_mae = f\"{m['mae']:.4f}\"\n",
    "        holdout_mae = f\"{data.get('holdout_mae', 0):.4f}\" if 'holdout_mae' in data else \"N/A\"\n",
    "        improvement = f\"{m['improvement']*100:+.1f}%\"\n",
    "        status = \"\u2713 PASS\" if m.get('passed_baseline', False) else \"\u2717 FAIL\"\n",
    "        \n",
    "        print(f\"{horizon:<8} {name:<15} {cv_mae:>10} {holdout_mae:>10} {improvement:>10} {status:>12}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Calibration report\n",
    "if any('calibration' in trained_models.get(h, {}) for h in ['1h', '4h']):\n",
    "    print(f\"\\n\" + \"-\"*70)\n",
    "    print(f\"{'PREDICTION INTERVAL CALIBRATION':^70}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Horizon':<10} {'Conformal 80%':>15} {'Adaptive 80%':>15} {'Width':>15}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for horizon in ['1h', '4h']:\n",
    "        if horizon in trained_models and 'calibration' in trained_models[horizon]:\n",
    "            cal = trained_models[horizon]['calibration']\n",
    "            c_cov = f\"{cal.get('conformal_coverage', 0):.1%}\"\n",
    "            a_cov = f\"{cal.get('adaptive_coverage', 0):.1%}\"\n",
    "            width = f\"\u00b1{cal.get('conformal_width', 0):.4f}\"\n",
    "            print(f\"{horizon:<10} {c_cov:>15} {a_cov:>15} {width:>15}\")\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "\n",
    "# Direction models\n",
    "if 'direction_models' in dir() and direction_models:\n",
    "    print(f\"\\n\" + \"-\"*70)\n",
    "    print(f\"{'DIRECTION PREDICTION':^70}\")\n",
    "    print(\"-\"*70)\n",
    "    for horizon, data in direction_models.items():\n",
    "        print(f\"  {horizon}: Accuracy={data['accuracy']:.1%}, F1={data['f1_score']:.3f}\")\n",
    "\n",
    "# Spike forecast\n",
    "if 'spike_forecast_models' in dir() and spike_forecast_models:\n",
    "    print(f\"\\n\" + \"-\"*70)\n",
    "    print(f\"{'SPIKE FORECASTING':^70}\")\n",
    "    print(\"-\"*70)\n",
    "    for horizon, data in spike_forecast_models.items():\n",
    "        if data.get('model') is not None:\n",
    "            print(f\"  {horizon}: AUC={data.get('auc', 0.5):.3f}, P={data.get('precision', 0):.1%}, R={data.get('recall', 0):.1%}\")\n",
    "        else:\n",
    "            print(f\"  {horizon}: No viable model (AUC < 0.55)\")\n",
    "\n",
    "# Bias correction\n",
    "if 'bias_correction_factors' in dir() and bias_correction_factors:\n",
    "    print(f\"\\n\" + \"-\"*70)\n",
    "    print(f\"{'BIAS CORRECTION':^70}\")\n",
    "    print(\"-\"*70)\n",
    "    for horizon, bcf in bias_correction_factors.items():\n",
    "        use_bc = trained_models.get(horizon, {}).get('use_bias_correction', False)\n",
    "        if use_bc:\n",
    "            print(f\"  {horizon}: ENABLED (correction: {bcf['overall']['correction']:+.4f})\")\n",
    "        else:\n",
    "            print(f\"  {horizon}: disabled\")\n",
    "\n",
    "# 24h model status\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(f\"{'24H MODEL STATUS':^70}\")\n",
    "print(\"-\"*70)\n",
    "if '24h' in trained_models:\n",
    "    if trained_models['24h'].get('is_fallback'):\n",
    "        print(f\"  \u26a0\ufe0f Using 4h model as fallback (need 30+ days of data)\")\n",
    "        print(f\"     Current data: {total_days:.1f} days\")\n",
    "        print(f\"     Recommendation: Collect {30 - total_days:.0f} more days before training true 24h model\")\n",
    "    else:\n",
    "        print(f\"  \u2713 True 24h model trained with {total_days:.1f} days of data\")\n",
    "\n",
    "# Final recommendation\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_passed = all(trained_models.get(h, {}).get('metrics', {}).get('passed_baseline', False) \n",
    "                 for h in trained_models if h in trained_models)\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\u2713 All models beat baseline - READY FOR DEPLOYMENT\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  1. Download saved_models/ folder\")\n",
    "    print(\"  2. Copy to backend/models/saved_models/\")\n",
    "    print(\"  3. Restart backend\")\n",
    "else:\n",
    "    failed = [h for h in trained_models \n",
    "              if not trained_models[h]['metrics'].get('passed_baseline', False)]\n",
    "    print(f\"\u26a0\ufe0f Some models did not pass baseline: {failed}\")\n",
    "    print(\"\\nRecommendations:\")\n",
    "    print(\"  - Collect more data\")\n",
    "    print(\"  - Review feature engineering\")\n",
    "    print(\"  - Consider deploying passing models only\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualizations - IMPROVED with holdout baseline comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Train vs Holdout Distribution Comparison\n",
    "ax1 = axes[0, 0]\n",
    "train_gas = current_gas.values\n",
    "if HAS_HOLDOUT:\n",
    "    holdout_gas = df_holdout['gas'].values\n",
    "    ax1.hist(train_gas, bins=50, alpha=0.6, color='blue', label=f'Train (mean={train_gas.mean():.2f})', density=True)\n",
    "    ax1.hist(holdout_gas, bins=50, alpha=0.6, color='red', label=f'Holdout (mean={holdout_gas.mean():.2f})', density=True)\n",
    "    ax1.legend()\n",
    "    ax1.set_title('Train vs Holdout Distribution')\n",
    "else:\n",
    "    ax1.hist(train_gas, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "    ax1.axvline(train_gas.mean(), color='red', linestyle='--', label=f'Mean: {train_gas.mean():.2f}')\n",
    "    ax1.legend()\n",
    "    ax1.set_title('Gas Price Distribution')\n",
    "ax1.set_xlabel('Gas Price (gwei)')\n",
    "ax1.set_ylabel('Density' if HAS_HOLDOUT else 'Frequency')\n",
    "\n",
    "# 2. Model vs HOLDOUT Baseline (not train baseline!)\n",
    "ax2 = axes[0, 1]\n",
    "horizons = list(trained_models.keys())\n",
    "maes = [trained_models[h]['metrics']['mae'] for h in horizons]\n",
    "\n",
    "# Use holdout baselines if available, otherwise train baselines\n",
    "baselines = []\n",
    "for h in horizons:\n",
    "    h_key = h.replace('24h', '4h')  # 24h uses 4h baseline\n",
    "    if 'holdout_best' in BASELINES.get(h_key, {}):\n",
    "        baselines.append(BASELINES[h_key]['holdout_best'])\n",
    "    else:\n",
    "        baselines.append(BASELINES.get(h_key, BASELINES['4h'])['best'])\n",
    "\n",
    "x = np.arange(len(horizons))\n",
    "width = 0.35\n",
    "bars1 = ax2.bar(x - width/2, maes, width, label='Model MAE', color='steelblue')\n",
    "bars2 = ax2.bar(x + width/2, baselines, width, label='Holdout Baseline', color='coral')\n",
    "ax2.set_xlabel('Horizon')\n",
    "ax2.set_ylabel('MAE (gwei)')\n",
    "ax2.set_title('Model vs Holdout Baseline Performance')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(horizons)\n",
    "ax2.legend()\n",
    "\n",
    "# Add improvement percentages (vs holdout baseline)\n",
    "for i, (h, m, b) in enumerate(zip(horizons, maes, baselines)):\n",
    "    imp = (b - m) / b * 100\n",
    "    color = 'green' if imp > 0 else 'red'\n",
    "    y_pos = max(m, b) + 0.02 * max(max(maes), max(baselines))\n",
    "    ax2.annotate(f'{imp:+.1f}%', xy=(i, y_pos), ha='center', fontsize=10, fontweight='bold', color=color)\n",
    "\n",
    "# 3. Gas price time series with regime markers\n",
    "ax3 = axes[1, 0]\n",
    "sample_size = min(2000, len(df_clean))\n",
    "sample_df = df_clean.iloc[-sample_size:]\n",
    "sample_gas = sample_df['gas']\n",
    "\n",
    "ax3.plot(sample_gas.index, sample_gas.values, linewidth=0.5, alpha=0.8, color='blue')\n",
    "\n",
    "# Mark holdout period\n",
    "if HAS_HOLDOUT:\n",
    "    holdout_start = df_holdout.index[0]\n",
    "    ax3.axvline(holdout_start, color='red', linestyle='--', linewidth=2, label='Holdout start')\n",
    "    ax3.legend()\n",
    "\n",
    "ax3.set_xlabel('Time')\n",
    "ax3.set_ylabel('Gas Price (gwei)')\n",
    "ax3.set_title(f'Recent Gas Prices (last {sample_size} samples)')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Feature importance (top 10)\n",
    "ax4 = axes[1, 1]\n",
    "if FEATURE_IMPORTANCE and any(v != list(FEATURE_IMPORTANCE.values())[0] for v in FEATURE_IMPORTANCE.values()):\n",
    "    # Non-uniform importance\n",
    "    sorted_imp = sorted(FEATURE_IMPORTANCE.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    features_plot = [f[0][:20] for f in sorted_imp]\n",
    "    importances = [f[1] for f in sorted_imp]\n",
    "    \n",
    "    y_pos = np.arange(len(features_plot))\n",
    "    ax4.barh(y_pos, importances, color='teal')\n",
    "    ax4.set_yticks(y_pos)\n",
    "    ax4.set_yticklabels(features_plot)\n",
    "    ax4.invert_yaxis()\n",
    "    ax4.set_xlabel('Importance')\n",
    "    ax4.set_title('Top 10 Feature Importance (Permutation)')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Feature importance uniform\\n(Huber model)', ha='center', va='center', fontsize=12)\n",
    "    ax4.set_title('Feature Importance')\n",
    "    ax4.set_xlim(0, 1)\n",
    "    ax4.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('saved_models/training_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Saved training_results.png\")\n",
    "\n",
    "# === ADDITIONAL: Distribution shift visualization ===\n",
    "if HAS_HOLDOUT and DISTRIBUTION_SHIFT_DETECTED:\n",
    "    fig2, axes2 = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    for i, horizon in enumerate(['1h', '4h']):\n",
    "        ax = axes2[i]\n",
    "        train_target = df_train_val[f'target_{horizon}'].dropna()\n",
    "        holdout_target = df_holdout[f'target_{horizon}'].dropna()\n",
    "        \n",
    "        ax.hist(train_target, bins=50, alpha=0.6, color='blue', label='Train', density=True)\n",
    "        ax.hist(holdout_target, bins=50, alpha=0.6, color='red', label='Holdout', density=True)\n",
    "        ax.set_xlabel(f'{horizon} Target (gwei)')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_title(f'{horizon} Target Distribution')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('saved_models/distribution_shift.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\u2713 Saved distribution_shift.png\")\n",
    ""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip file for download\n",
    "import shutil\n",
    "\n",
    "shutil.make_archive('gweizy_models', 'zip', 'saved_models')\n",
    "print(\"\\n\u2705 Created gweizy_models.zip\")\n",
    "print(\"\\nDownload this file and extract to: backend/models/saved_models/\")\n",
    "\n",
    "# Auto-download\n",
    "files.download('gweizy_models.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}