{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gweizy Model Training Notebook\n",
    "\n",
    "Train all gas prediction models for Gweizy.\n",
    "\n",
    "## Instructions:\n",
    "1. Upload your `gas_data.db` file (from `backend/gas_data.db`)\n",
    "2. Run all cells\n",
    "3. Download the trained models zip file\n",
    "4. Extract to `backend/models/saved_models/` and push to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q scikit-learn pandas numpy joblib lightgbm xgboost matplotlib seaborn optuna"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your gas_data.db file\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"Upload your gas_data.db file from backend/gas_data.db\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "if 'gas_data.db' in uploaded:\n",
    "    print(f\"\\n✅ Uploaded gas_data.db ({len(uploaded['gas_data.db']) / 1024 / 1024:.1f} MB)\")\n",
    "else:\n",
    "    print(\"❌ Please upload gas_data.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sqlite3\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\n# Load data from database\nconn = sqlite3.connect('gas_data.db')\ndf = pd.read_sql(\"\"\"\n    SELECT timestamp, current_gas as gas, base_fee, priority_fee, \n           block_number, gas_used, gas_limit, utilization\n    FROM gas_prices ORDER BY timestamp ASC\n\"\"\", conn)\nconn.close()\n\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf = df.set_index('timestamp').sort_index()\n\nprint(f\"Total records: {len(df):,}\")\nprint(f\"Date range: {df.index.min()} to {df.index.max()}\")\n\n# Resample to 1-minute (reduces noise, easier to work with)\nprint(\"\\nResampling to 1-minute intervals...\")\ndf = df.resample('1min').mean().dropna(subset=['gas'])\nprint(f\"After resample: {len(df):,} records\")\n\n# Find segments (gap > 30 min = new segment)\ndf['time_diff'] = df.index.to_series().diff()\ndf['segment'] = (df['time_diff'] > pd.Timedelta(minutes=30)).cumsum()\n\nsegment_sizes = df.groupby('segment').size()\nprint(f\"\\nSegments found: {len(segment_sizes)}\")\nprint(f\"Segment sizes: {segment_sizes.sort_values(ascending=False).head(10).tolist()}\")\n\n# Keep segments with at least 120 minutes (2 hours) of data\nMIN_SEGMENT_SIZE = 120\ngood_segments = segment_sizes[segment_sizes >= MIN_SEGMENT_SIZE].index.tolist()\ndf = df[df['segment'].isin(good_segments)]\nprint(f\"\\nKeeping {len(good_segments)} segments with >= {MIN_SEGMENT_SIZE} minutes\")\nprint(f\"Total usable records: {len(df):,}\")\n\nRECORDS_PER_HOUR = 60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Feature Engineering - IMPROVED with micro-features for 1h model\n# Key: Use SHORT windows (max 4h) + MICRO windows (5min, 15min, 30min) for 1h\n\nprint(\"Engineering features with MICRO + SHORT windows...\")\n\ndef engineer_features_for_segment(seg_df):\n    \"\"\"Engineer features for a single continuous segment\"\"\"\n    df = seg_df.copy()\n    rph = 60  # records per hour (1-min intervals)\n    \n    # === Log transform gas (helps with skewed distribution) ===\n    df['gas_log'] = np.log1p(df['gas'])\n    \n    # === Time features (ENHANCED) ===\n    df['hour'] = df.index.hour\n    df['minute'] = df.index.minute\n    df['day_of_week'] = df.index.dayofweek\n    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n    df['is_business_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 17)).astype(int)\n    # Peak hours for Ethereum (typically 14:00-22:00 UTC)\n    df['is_peak_hours'] = ((df['hour'] >= 14) & (df['hour'] <= 22)).astype(int)\n    \n    # === MICRO Lag features (for 1h prediction) ===\n    for lag_mins in [5, 10, 15, 30]:\n        df[f'gas_lag_{lag_mins}min'] = df['gas'].shift(lag_mins)\n        df[f'gas_change_{lag_mins}min'] = df['gas'] - df['gas'].shift(lag_mins)\n        df[f'gas_pct_change_{lag_mins}min'] = df['gas'].pct_change(lag_mins)\n    \n    # === MICRO Rolling stats (5min, 15min, 30min windows) ===\n    for window_mins in [5, 15, 30]:\n        df[f'gas_mean_{window_mins}min'] = df['gas'].rolling(window_mins, min_periods=window_mins//2).mean()\n        df[f'gas_std_{window_mins}min'] = df['gas'].rolling(window_mins, min_periods=window_mins//2).std()\n        df[f'gas_min_{window_mins}min'] = df['gas'].rolling(window_mins, min_periods=window_mins//2).min()\n        df[f'gas_max_{window_mins}min'] = df['gas'].rolling(window_mins, min_periods=window_mins//2).max()\n        # Volatility\n        df[f'gas_range_{window_mins}min'] = df[f'gas_max_{window_mins}min'] - df[f'gas_min_{window_mins}min']\n        df[f'gas_cv_{window_mins}min'] = df[f'gas_std_{window_mins}min'] / (df[f'gas_mean_{window_mins}min'] + 1e-8)\n    \n    # === Standard Lag features (hours) ===\n    for lag_hours in [1, 2, 4]:\n        df[f'gas_lag_{lag_hours}h'] = df['gas'].shift(lag_hours * rph)\n        df[f'gas_log_lag_{lag_hours}h'] = df['gas_log'].shift(lag_hours * rph)\n    \n    # === Rolling stats (SHORT windows: 1h, 2h, 4h) ===\n    for window_hours in [1, 2, 4]:\n        window = window_hours * rph\n        df[f'gas_mean_{window_hours}h'] = df['gas'].rolling(window, min_periods=window//2).mean()\n        df[f'gas_std_{window_hours}h'] = df['gas'].rolling(window, min_periods=window//2).std()\n        df[f'gas_min_{window_hours}h'] = df['gas'].rolling(window, min_periods=window//2).min()\n        df[f'gas_max_{window_hours}h'] = df['gas'].rolling(window, min_periods=window//2).max()\n        df[f'gas_median_{window_hours}h'] = df['gas'].rolling(window, min_periods=window//2).median()\n        \n        # EMA (Exponential Moving Average)\n        df[f'gas_ema_{window_hours}h'] = df['gas'].ewm(span=window, min_periods=window//2).mean()\n        \n        # Volatility features\n        df[f'gas_cv_{window_hours}h'] = df[f'gas_std_{window_hours}h'] / (df[f'gas_mean_{window_hours}h'] + 1e-8)\n        df[f'gas_range_{window_hours}h'] = df[f'gas_max_{window_hours}h'] - df[f'gas_min_{window_hours}h']\n        df[f'gas_range_pct_{window_hours}h'] = df[f'gas_range_{window_hours}h'] / (df[f'gas_mean_{window_hours}h'] + 1e-8)\n    \n    # === MICRO Momentum (for 1h) ===\n    for mins in [5, 15, 30]:\n        df[f'momentum_{mins}min'] = df['gas'] - df['gas'].shift(mins)\n        df[f'momentum_pct_{mins}min'] = df['gas'].pct_change(mins)\n        # Acceleration (rate of change of momentum)\n        df[f'acceleration_{mins}min'] = df[f'momentum_{mins}min'] - df[f'momentum_{mins}min'].shift(mins)\n    \n    # === Standard Momentum ===\n    for hours in [1, 2]:\n        periods = hours * rph\n        df[f'momentum_{hours}h'] = df['gas'] - df['gas'].shift(periods)\n        df[f'momentum_pct_{hours}h'] = df['gas'].pct_change(periods)\n        df[f'acceleration_{hours}h'] = df[f'momentum_{hours}h'] - df[f'momentum_{hours}h'].shift(periods)\n        df[f'direction_{hours}h'] = np.sign(df[f'momentum_{hours}h'])\n    \n    # === Z-score ===\n    for hours in [1, 2, 4]:\n        df[f'gas_zscore_{hours}h'] = (df['gas'] - df[f'gas_mean_{hours}h']) / (df[f'gas_std_{hours}h'] + 1e-8)\n    \n    # === Trend indicators ===\n    df['trend_15min_1h'] = df['gas_mean_15min'] / (df['gas_mean_1h'] + 1e-8)\n    df['trend_30min_1h'] = df['gas_mean_30min'] / (df['gas_mean_1h'] + 1e-8)\n    df['trend_1h_2h'] = df['gas_mean_1h'] / (df['gas_mean_2h'] + 1e-8)\n    df['trend_1h_4h'] = df['gas_mean_1h'] / (df['gas_mean_4h'] + 1e-8)\n    df['ema_trend_short'] = df['gas_ema_1h'] / (df['gas_ema_2h'] + 1e-8)\n    df['ema_trend_long'] = df['gas_ema_1h'] / (df['gas_ema_4h'] + 1e-8)\n    \n    # === Price position (where is current price in recent range) ===\n    for window in ['30min', '1h', '2h', '4h']:\n        col_max = f'gas_max_{window}'\n        col_min = f'gas_min_{window}'\n        if col_max in df.columns and col_min in df.columns:\n            range_size = df[col_max] - df[col_min]\n            df[f'price_position_{window}'] = (df['gas'] - df[col_min]) / (range_size + 1e-8)\n    \n    # === Recent volatility regime (for confidence) ===\n    df['volatility_regime'] = pd.cut(\n        df['gas_cv_1h'], \n        bins=[0, 0.05, 0.15, float('inf')], \n        labels=[0, 1, 2]  # 0=Low, 1=Medium, 2=High\n    ).astype(float)\n    \n    # === Targets (absolute) ===\n    df['target_1h'] = df['gas'].shift(-1 * rph)\n    df['target_4h'] = df['gas'].shift(-4 * rph)\n    df['target_24h'] = df['gas'].shift(-4 * rph)  # Actually 4h (honest labeling)\n    \n    # === Targets (percentage change - more stable) ===\n    df['target_pct_1h'] = (df['target_1h'] - df['gas']) / (df['gas'] + 1e-8)\n    df['target_pct_4h'] = (df['target_4h'] - df['gas']) / (df['gas'] + 1e-8)\n    \n    # === Direction targets (for classification) ===\n    # Use threshold to avoid noisy \"stable\" predictions\n    threshold = 0.02  # 2% change threshold\n    \n    def classify_direction(pct_change, threshold):\n        if pct_change < -threshold:\n            return 0  # Down\n        elif pct_change > threshold:\n            return 2  # Up\n        else:\n            return 1  # Stable\n    \n    df['direction_class_1h'] = df['target_pct_1h'].apply(lambda x: classify_direction(x, threshold))\n    df['direction_class_4h'] = df['target_pct_4h'].apply(lambda x: classify_direction(x, threshold))\n    \n    return df\n\n# Process each segment independently\nprint(\"Processing segments independently...\")\nall_features = []\n\nfor seg_id in df['segment'].unique():\n    seg_df = df[df['segment'] == seg_id].drop(columns=['segment', 'time_diff'])\n    if len(seg_df) >= MIN_SEGMENT_SIZE:\n        featured = engineer_features_for_segment(seg_df)\n        all_features.append(featured)\n        print(f\"  Segment {seg_id}: {len(seg_df)} → {len(featured.dropna())} usable rows\")\n\n# Combine all segments\ndf_features = pd.concat(all_features)\ndf_features = df_features.replace([np.inf, -np.inf], np.nan)\n\nprint(f\"\\nTotal featured samples: {len(df_features):,}\")\nprint(f\"After dropping NaN: {len(df_features.dropna()):,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare training data with feature selection\nfrom sklearn.preprocessing import RobustScaler\n\n# Columns to exclude from features\nexclude_cols = ['gas', 'gas_log', 'base_fee', 'priority_fee', 'block_number', \n                'gas_used', 'gas_limit', 'utilization',\n                'target_1h', 'target_4h', 'target_24h',\n                'target_pct_1h', 'target_pct_4h',\n                'direction_class_1h', 'direction_class_4h',\n                'volatility_regime']\n\nfeature_cols = [c for c in df_features.columns if c not in exclude_cols]\nprint(f\"Initial feature columns: {len(feature_cols)}\")\n\n# Drop rows with NaN\ndf_clean = df_features.dropna()\nprint(f\"Clean samples: {len(df_clean):,}\")\n\n# === Feature Selection: Remove highly correlated features (>0.90 for small dataset) ===\nprint(\"\\nRemoving highly correlated features (>0.90)...\")\nX_temp = df_clean[feature_cols]\ncorr_matrix = X_temp.corr().abs()\n\n# Find pairs with correlation > 0.90 (stricter for small dataset)\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\nprint(f\"  Dropping {len(to_drop)} highly correlated features\")\n\nfeature_cols = [c for c in feature_cols if c not in to_drop]\nprint(f\"Final feature columns: {len(feature_cols)}\")\n\n# === Create horizon-specific feature sets ===\n# 1h model benefits from micro-features\nmicro_features = [c for c in feature_cols if 'min' in c or 'micro' in c.lower()]\nhour_features = [c for c in feature_cols if 'h' in c and 'min' not in c]\ntime_features = [c for c in feature_cols if any(t in c for t in ['hour', 'day', 'sin', 'cos', 'weekend', 'business', 'peak'])]\ntrend_features = [c for c in feature_cols if 'trend' in c or 'position' in c or 'zscore' in c]\n\n# 1h: prioritize micro-features + short-term\nfeatures_1h = list(set(micro_features + time_features + trend_features + [c for c in feature_cols if '1h' in c or '2h' in c]))\nfeatures_1h = [c for c in features_1h if c in feature_cols]\n\n# 4h: use all features but weight longer-term\nfeatures_4h = feature_cols  # Use all for 4h\n\nprint(f\"\\n1h model features: {len(features_1h)}\")\nprint(f\"4h model features: {len(features_4h)}\")\n\n# Prepare data\nX = df_clean[feature_cols]\nX_1h = df_clean[[c for c in features_1h if c in df_clean.columns]]\nX_4h = df_clean[[c for c in features_4h if c in df_clean.columns]]\n\ny_1h = df_clean['target_1h']\ny_4h = df_clean['target_4h']\ny_24h = df_clean['target_24h']\n\n# Percentage targets (alternative)\ny_pct_1h = df_clean['target_pct_1h']\ny_pct_4h = df_clean['target_pct_4h']\n\n# Direction targets for classification\ny_dir_1h = df_clean['direction_class_1h']\ny_dir_4h = df_clean['direction_class_4h']\n\n# Volatility regime for confidence\nvolatility_regime = df_clean['volatility_regime']\n\n# Store current gas for baseline\ncurrent_gas = df_clean['gas']\n\n# === Baseline Models ===\nprint(f\"\\n{'='*50}\")\nprint(\"BASELINE COMPARISONS\")\nprint(\"='*50}\")\n\n# Naive baseline: predict last known value\nnaive_pred_1h = current_gas.values\nnaive_mae_1h = np.mean(np.abs(y_1h.values - naive_pred_1h))\nnaive_mae_4h = np.mean(np.abs(y_4h.values - naive_pred_1h))\n\n# Mean baseline: predict historical mean\nmean_pred = np.full_like(y_1h.values, y_1h.mean())\nmean_mae_1h = np.mean(np.abs(y_1h.values - mean_pred))\nmean_mae_4h = np.mean(np.abs(y_4h.values - mean_pred))\n\n# Drift baseline: extrapolate recent trend\ndrift_pred_1h = current_gas.values + df_clean['momentum_1h'].values\ndrift_mae_1h = np.mean(np.abs(y_1h.values - drift_pred_1h))\n\nprint(f\"\\nBaseline MAEs:\")\nprint(f\"  Naive (current price):     MAE_1h={naive_mae_1h:.6f}, MAE_4h={naive_mae_4h:.6f}\")\nprint(f\"  Mean (historical average): MAE_1h={mean_mae_1h:.6f}, MAE_4h={mean_mae_4h:.6f}\")\nprint(f\"  Drift (extrapolate trend): MAE_1h={drift_mae_1h:.6f}\")\n\n# Use best baseline for comparison\nbest_baseline_1h = min(naive_mae_1h, mean_mae_1h, drift_mae_1h)\nbest_baseline_4h = min(naive_mae_4h, mean_mae_4h)\n\nprint(f\"\\n  Best baseline 1h: {best_baseline_1h:.6f}\")\nprint(f\"  Best baseline 4h: {best_baseline_4h:.6f}\")\n\n# Store baselines for comparison\nBASELINES = {\n    '1h': {'naive_mae': naive_mae_1h, 'mean_mae': mean_mae_1h, 'drift_mae': drift_mae_1h, 'best': best_baseline_1h},\n    '4h': {'naive_mae': naive_mae_4h, 'mean_mae': mean_mae_4h, 'best': best_baseline_4h}\n}\n\nprint(f\"\\n{'='*50}\")\nprint(\"TRAINING DATA SUMMARY\")\nprint(\"{'='*50}\")\nprint(f\"Samples: {len(X):,}\")\nprint(f\"Features (all): {len(feature_cols)}\")\nprint(f\"Features (1h specific): {len(features_1h)}\")\nprint(f\"Target 1h range: {y_1h.min():.4f} - {y_1h.max():.4f} gwei\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model Training with Hyperparameter Tuning, Simpler Models, and Quantile Regression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import Ridge, ElasticNet, HuberRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef time_series_cv(model, X, y, n_splits=5):\n    \"\"\"Time-series cross-validation\"\"\"\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    scores = {'mae': [], 'r2': []}\n    \n    for train_idx, val_idx in tscv.split(X):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n        scaler = RobustScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_val_scaled = scaler.transform(X_val)\n        \n        model.fit(X_train_scaled, y_train)\n        pred = model.predict(X_val_scaled)\n        \n        scores['mae'].append(mean_absolute_error(y_val, pred))\n        scores['r2'].append(r2_score(y_val, pred))\n    \n    return {\n        'mae_mean': np.mean(scores['mae']),\n        'mae_std': np.std(scores['mae']),\n        'r2_mean': np.mean(scores['r2']),\n        'r2_std': np.std(scores['r2'])\n    }\n\ndef train_1h_model(X, y, current_gas, baseline_mae):\n    \"\"\"\n    Train 1h model with SIMPLER models optimized for short-term prediction.\n    1h is noisy - simpler models with strong regularization work better.\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(\"Training 1h models (SIMPLER - optimized for short-term)\")\n    print(\"='*60}\")\n    print(f\"Baseline MAE (best): {baseline_mae:.6f}\")\n    \n    # Time-series split\n    split_idx = int(len(X) * 0.8)\n    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n    gas_test = current_gas.iloc[split_idx:]\n    \n    print(f\"Train: {len(X_train):,}, Test: {len(X_test):,}\")\n    \n    # Scale\n    scaler = RobustScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    results = []\n    all_preds = []\n    \n    # === 1. Ridge (strong regularization for noisy data) ===\n    print(\"\\n[1/5] Ridge Regression (high regularization)...\")\n    ridge = Ridge(alpha=10.0, random_state=42)  # High alpha for regularization\n    ridge.fit(X_train_scaled, y_train)\n    ridge_pred = ridge.predict(X_test_scaled)\n    ridge_metrics = evaluate_model(y_test, ridge_pred, baseline_mae)\n    results.append(('Ridge', ridge, ridge_metrics, scaler))\n    all_preds.append(ridge_pred)\n    print(f\"      MAE: {ridge_metrics['mae']:.6f}, vs Baseline: {ridge_metrics['vs_baseline']}\")\n    \n    # === 2. ElasticNet (L1+L2 regularization) ===\n    print(\"[2/5] ElasticNet...\")\n    elastic = ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42, max_iter=5000)\n    elastic.fit(X_train_scaled, y_train)\n    elastic_pred = elastic.predict(X_test_scaled)\n    elastic_metrics = evaluate_model(y_test, elastic_pred, baseline_mae)\n    results.append(('ElasticNet', elastic, elastic_metrics, scaler))\n    all_preds.append(elastic_pred)\n    print(f\"      MAE: {elastic_metrics['mae']:.6f}, vs Baseline: {elastic_metrics['vs_baseline']}\")\n    \n    # === 3. Huber Regressor (robust to outliers) ===\n    print(\"[3/5] Huber Regressor (robust to outliers)...\")\n    huber = HuberRegressor(epsilon=1.35, alpha=1.0, max_iter=1000)\n    huber.fit(X_train_scaled, y_train)\n    huber_pred = huber.predict(X_test_scaled)\n    huber_metrics = evaluate_model(y_test, huber_pred, baseline_mae)\n    results.append(('Huber', huber, huber_metrics, scaler))\n    all_preds.append(huber_pred)\n    print(f\"      MAE: {huber_metrics['mae']:.6f}, vs Baseline: {huber_metrics['vs_baseline']}\")\n    \n    # === 4. Small Random Forest (reduced complexity) ===\n    print(\"[4/5] Small Random Forest...\")\n    rf_small = RandomForestRegressor(\n        n_estimators=50, max_depth=5, min_samples_split=20,\n        min_samples_leaf=10, random_state=42, n_jobs=-1\n    )\n    rf_small.fit(X_train_scaled, y_train)\n    rf_pred = rf_small.predict(X_test_scaled)\n    rf_metrics = evaluate_model(y_test, rf_pred, baseline_mae)\n    results.append(('RF_Small', rf_small, rf_metrics, scaler))\n    all_preds.append(rf_pred)\n    print(f\"      MAE: {rf_metrics['mae']:.6f}, vs Baseline: {rf_metrics['vs_baseline']}\")\n    \n    # === 5. LightGBM with aggressive regularization ===\n    try:\n        import lightgbm as lgb\n        print(\"[5/5] LightGBM (high regularization)...\")\n        \n        val_split = int(len(X_train_scaled) * 0.9)\n        X_tr, X_val = X_train_scaled[:val_split], X_train_scaled[val_split:]\n        y_tr, y_val = y_train.iloc[:val_split], y_train.iloc[val_split:]\n        \n        lgbm = lgb.LGBMRegressor(\n            n_estimators=200, max_depth=4, learning_rate=0.05,\n            num_leaves=15, min_child_samples=30, subsample=0.7,\n            colsample_bytree=0.7, reg_alpha=1.0, reg_lambda=1.0,  # Strong regularization\n            random_state=42, n_jobs=-1, verbose=-1\n        )\n        lgbm.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], \n                 callbacks=[lgb.early_stopping(30, verbose=False)])\n        \n        lgbm_pred = lgbm.predict(X_test_scaled)\n        lgbm_metrics = evaluate_model(y_test, lgbm_pred, baseline_mae)\n        results.append(('LightGBM', lgbm, lgbm_metrics, scaler))\n        all_preds.append(lgbm_pred)\n        print(f\"      MAE: {lgbm_metrics['mae']:.6f}, vs Baseline: {lgbm_metrics['vs_baseline']}\")\n    except Exception as e:\n        print(f\"[5/5] LightGBM failed: {e}\")\n    \n    # === Ensemble ===\n    print(\"\\n[Ensemble] Weighted average (favor simpler models)...\")\n    # Weight simpler models more for 1h\n    weights = [0.25, 0.25, 0.2, 0.15, 0.15] if len(all_preds) == 5 else [1/len(all_preds)] * len(all_preds)\n    ensemble_pred = np.average(all_preds, axis=0, weights=weights[:len(all_preds)])\n    ensemble_metrics = evaluate_model(y_test, ensemble_pred, baseline_mae)\n    print(f\"      MAE: {ensemble_metrics['mae']:.6f}, vs Baseline: {ensemble_metrics['vs_baseline']}\")\n    \n    # === Select best ===\n    all_results = results + [('Ensemble', [r[1] for r in results], ensemble_metrics, scaler)]\n    best = max(all_results, key=lambda x: x[2]['improvement'])\n    \n    print(f\"\\n>>> Best 1h model: {best[0]} (MAE: {best[2]['mae']:.6f}, {best[2]['vs_baseline']})\")\n    \n    # Calculate confidence based on volatility\n    confidence_scores = calculate_confidence(X_test, y_test, best[1] if best[0] != 'Ensemble' else results[0][1], scaler)\n    \n    return best, results, list(X.columns), ensemble_pred, y_test, confidence_scores\n\ndef train_4h_model(X, y, current_gas, baseline_mae):\n    \"\"\"\n    Train 4h model with full model suite + hyperparameter tuning.\n    4h is more predictable - can use more complex models.\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(\"Training 4h models (FULL SUITE with tuning)\")\n    print(\"='*60}\")\n    print(f\"Baseline MAE (best): {baseline_mae:.6f}\")\n    \n    # Time-series split\n    split_idx = int(len(X) * 0.8)\n    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n    \n    print(f\"Train: {len(X_train):,}, Test: {len(X_test):,}\")\n    \n    # Scale\n    scaler = RobustScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    results = []\n    all_preds = []\n    \n    # === 1. Random Forest with tuning ===\n    print(\"\\n[1/4] Random Forest with RandomizedSearchCV...\")\n    rf_params = {\n        'n_estimators': [100, 150, 200],\n        'max_depth': [8, 12, 15],\n        'min_samples_split': [5, 10, 15],\n        'min_samples_leaf': [3, 5, 8]\n    }\n    \n    rf_base = RandomForestRegressor(random_state=42, n_jobs=-1)\n    tscv = TimeSeriesSplit(n_splits=3)\n    rf_search = RandomizedSearchCV(\n        rf_base, rf_params, n_iter=10, cv=tscv, \n        scoring='neg_mean_absolute_error', random_state=42, n_jobs=-1\n    )\n    rf_search.fit(X_train_scaled, y_train)\n    rf = rf_search.best_estimator_\n    \n    rf_pred = rf.predict(X_test_scaled)\n    rf_metrics = evaluate_model(y_test, rf_pred, baseline_mae)\n    results.append(('RandomForest', rf, rf_metrics, scaler))\n    all_preds.append(rf_pred)\n    print(f\"      Best params: {rf_search.best_params_}\")\n    print(f\"      MAE: {rf_metrics['mae']:.6f}, vs Baseline: {rf_metrics['vs_baseline']}\")\n    \n    # === 2. Gradient Boosting ===\n    print(\"[2/4] Gradient Boosting...\")\n    gb = GradientBoostingRegressor(\n        n_estimators=150, max_depth=6, learning_rate=0.05,\n        min_samples_split=10, subsample=0.8, random_state=42\n    )\n    gb.fit(X_train_scaled, y_train)\n    gb_pred = gb.predict(X_test_scaled)\n    gb_metrics = evaluate_model(y_test, gb_pred, baseline_mae)\n    results.append(('GradientBoosting', gb, gb_metrics, scaler))\n    all_preds.append(gb_pred)\n    print(f\"      MAE: {gb_metrics['mae']:.6f}, vs Baseline: {gb_metrics['vs_baseline']}\")\n    \n    # === 3. LightGBM ===\n    try:\n        import lightgbm as lgb\n        print(\"[3/4] LightGBM with early stopping...\")\n        \n        val_split = int(len(X_train_scaled) * 0.9)\n        X_tr, X_val = X_train_scaled[:val_split], X_train_scaled[val_split:]\n        y_tr, y_val = y_train.iloc[:val_split], y_train.iloc[val_split:]\n        \n        lgbm = lgb.LGBMRegressor(\n            n_estimators=500, max_depth=10, learning_rate=0.03,\n            num_leaves=31, min_child_samples=20, subsample=0.8,\n            colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=0.1,\n            random_state=42, n_jobs=-1, verbose=-1\n        )\n        lgbm.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], \n                 callbacks=[lgb.early_stopping(50, verbose=False)])\n        \n        lgbm_pred = lgbm.predict(X_test_scaled)\n        lgbm_metrics = evaluate_model(y_test, lgbm_pred, baseline_mae)\n        results.append(('LightGBM', lgbm, lgbm_metrics, scaler))\n        all_preds.append(lgbm_pred)\n        print(f\"      MAE: {lgbm_metrics['mae']:.6f}, vs Baseline: {lgbm_metrics['vs_baseline']}\")\n    except Exception as e:\n        print(f\"[3/4] LightGBM failed: {e}\")\n    \n    # === 4. XGBoost ===\n    try:\n        import xgboost as xgb\n        print(\"[4/4] XGBoost with early stopping...\")\n        \n        xgbm = xgb.XGBRegressor(\n            n_estimators=500, max_depth=8, learning_rate=0.03,\n            min_child_weight=5, subsample=0.8, colsample_bytree=0.8,\n            reg_alpha=0.1, reg_lambda=1.0, random_state=42, \n            n_jobs=-1, verbosity=0, early_stopping_rounds=50\n        )\n        xgbm.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n        \n        xgbm_pred = xgbm.predict(X_test_scaled)\n        xgbm_metrics = evaluate_model(y_test, xgbm_pred, baseline_mae)\n        results.append(('XGBoost', xgbm, xgbm_metrics, scaler))\n        all_preds.append(xgbm_pred)\n        print(f\"      MAE: {xgbm_metrics['mae']:.6f}, vs Baseline: {xgbm_metrics['vs_baseline']}\")\n    except Exception as e:\n        print(f\"[4/4] XGBoost failed: {e}\")\n    \n    # === Ensemble ===\n    print(\"\\n[Ensemble] Average all models...\")\n    ensemble_pred = np.mean(all_preds, axis=0)\n    ensemble_metrics = evaluate_model(y_test, ensemble_pred, baseline_mae)\n    print(f\"      MAE: {ensemble_metrics['mae']:.6f}, vs Baseline: {ensemble_metrics['vs_baseline']}\")\n    \n    # Select best\n    all_results = results + [('Ensemble', [r[1] for r in results], ensemble_metrics, scaler)]\n    best = max(all_results, key=lambda x: x[2]['improvement'])\n    \n    print(f\"\\n>>> Best 4h model: {best[0]} (MAE: {best[2]['mae']:.6f}, {best[2]['vs_baseline']})\")\n    \n    # Calculate confidence\n    confidence_scores = calculate_confidence(X_test, y_test, best[1] if best[0] != 'Ensemble' else results[0][1], scaler)\n    \n    return best, results, list(X.columns), ensemble_pred, y_test, confidence_scores\n\ndef evaluate_model(y_true, y_pred, baseline_mae):\n    \"\"\"Calculate model metrics with baseline comparison\"\"\"\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    r2 = r2_score(y_true, y_pred)\n    \n    # Directional accuracy\n    if len(y_true) > 1:\n        actual_dir = np.sign(np.diff(y_true.values))\n        pred_dir = np.sign(np.diff(y_pred))\n        dir_acc = np.mean(actual_dir == pred_dir)\n    else:\n        dir_acc = 0.0\n    \n    # Compare to baseline\n    improvement = (baseline_mae - mae) / baseline_mae * 100\n    vs_baseline = f\"{improvement:+.1f}%\" if improvement != 0 else \"0%\"\n    \n    return {\n        'mae': mae, 'rmse': rmse, 'r2': r2, \n        'directional_accuracy': dir_acc,\n        'vs_baseline': vs_baseline, 'improvement': improvement\n    }\n\ndef calculate_confidence(X_test, y_test, model, scaler):\n    \"\"\"\n    Calculate prediction confidence based on:\n    1. Model's prediction variance (if ensemble/tree)\n    2. Distance from training distribution\n    3. Recent volatility\n    \"\"\"\n    X_scaled = scaler.transform(X_test) if not isinstance(X_test, np.ndarray) else X_test\n    \n    confidences = []\n    \n    if hasattr(model, 'estimators_'):\n        # For ensemble models, use prediction variance across trees\n        tree_preds = np.array([tree.predict(X_scaled) for tree in model.estimators_])\n        pred_std = np.std(tree_preds, axis=0)\n        # Lower std = higher confidence\n        max_std = np.percentile(pred_std, 95)\n        confidences = 1 - np.clip(pred_std / (max_std + 1e-8), 0, 1)\n    else:\n        # For other models, use uniform medium confidence\n        confidences = np.full(len(X_test), 0.6)\n    \n    return confidences\n\nprint(\"Training functions defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train all models with separate strategies for 1h vs 4h\nprint(\"=\"*60)\nprint(\"TRAINING ALL MODELS\")\nprint(\"=\"*60)\n\n# Use 1h-specific features for 1h model (micro-features)\nprint(\"\\n>>> Using micro-features for 1h model\")\nbest_1h, all_1h, features_1h_used, pred_1h, actual_1h, conf_1h = train_1h_model(\n    X_1h, y_1h, current_gas, BASELINES['1h']['best']\n)\n\n# Use full features for 4h model\nprint(\"\\n>>> Using full features for 4h model\")\nbest_4h, all_4h, features_4h_used, pred_4h, actual_4h, conf_4h = train_4h_model(\n    X_4h, y_4h, current_gas, BASELINES['4h']['best']\n)\n\n# 24h model (actually 4h - honest labeling)\nprint(\"\\n>>> 24h model = 4h model (data limitation)\")\nprint(\"    Note: '24h' predictions are actually 4h ahead due to insufficient continuous data\")\nbest_24h = best_4h  # Same as 4h\nall_24h = all_4h\npred_24h = pred_4h\nactual_24h = actual_4h\nconf_24h = conf_4h\n\n# Store features used for saving\nfeatures = feature_cols  # Use all features for model file"
  },
  {
   "cell_type": "code",
   "source": "# Direction Prediction (Classification: Down/Stable/Up) - IMPROVED\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, f1_score\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"DIRECTION PREDICTION (Classification)\")\nprint(\"=\"*60)\nprint(\"Classes: 0=Down (>2% drop), 1=Stable (<2% change), 2=Up (>2% rise)\")\n\ndef train_direction_model(X, y_dir, horizon_name, use_class_weights=True):\n    \"\"\"\n    Train direction classifier with:\n    - Class weights to handle imbalance\n    - Multiple model comparison\n    - Probability calibration\n    \"\"\"\n    print(f\"\\n{horizon_name} Direction Classifier:\")\n    \n    # Remove NaN\n    valid_idx = ~y_dir.isna()\n    X_valid = X[valid_idx]\n    y_valid = y_dir[valid_idx].astype(int)\n    \n    # Class distribution\n    class_counts = y_valid.value_counts().sort_index()\n    total = len(y_valid)\n    print(f\"  Class distribution:\")\n    print(f\"    Down (0):   {class_counts.get(0,0):5d} ({class_counts.get(0,0)/total*100:.1f}%)\")\n    print(f\"    Stable (1): {class_counts.get(1,0):5d} ({class_counts.get(1,0)/total*100:.1f}%)\")\n    print(f\"    Up (2):     {class_counts.get(2,0):5d} ({class_counts.get(2,0)/total*100:.1f}%)\")\n    \n    # Calculate class weights (inverse frequency)\n    if use_class_weights:\n        class_weights = {i: total / (3 * count) for i, count in class_counts.items()}\n        print(f\"  Using class weights: {class_weights}\")\n    else:\n        class_weights = None\n    \n    # Split\n    split_idx = int(len(X_valid) * 0.8)\n    X_train, X_test = X_valid.iloc[:split_idx], X_valid.iloc[split_idx:]\n    y_train, y_test = y_valid.iloc[:split_idx], y_valid.iloc[split_idx:]\n    \n    # Scale\n    scaler = RobustScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    results = []\n    \n    # 1. Random Forest with class weights\n    print(f\"\\n  [1/3] Random Forest...\")\n    rf_clf = RandomForestClassifier(\n        n_estimators=150, max_depth=10, min_samples_split=10,\n        class_weight=class_weights, random_state=42, n_jobs=-1\n    )\n    rf_clf.fit(X_train_scaled, y_train)\n    rf_pred = rf_clf.predict(X_test_scaled)\n    rf_acc = accuracy_score(y_test, rf_pred)\n    rf_f1 = f1_score(y_test, rf_pred, average='weighted')\n    results.append(('RandomForest', rf_clf, rf_acc, rf_f1))\n    print(f\"        Accuracy: {rf_acc:.1%}, F1: {rf_f1:.3f}\")\n    \n    # 2. Gradient Boosting\n    print(f\"  [2/3] Gradient Boosting...\")\n    gb_clf = GradientBoostingClassifier(\n        n_estimators=100, max_depth=5, learning_rate=0.1,\n        random_state=42\n    )\n    gb_clf.fit(X_train_scaled, y_train)\n    gb_pred = gb_clf.predict(X_test_scaled)\n    gb_acc = accuracy_score(y_test, gb_pred)\n    gb_f1 = f1_score(y_test, gb_pred, average='weighted')\n    results.append(('GradientBoosting', gb_clf, gb_acc, gb_f1))\n    print(f\"        Accuracy: {gb_acc:.1%}, F1: {gb_f1:.3f}\")\n    \n    # 3. Logistic Regression (probability calibration)\n    print(f\"  [3/3] Logistic Regression...\")\n    lr_clf = LogisticRegression(\n        class_weight=class_weights, max_iter=1000, random_state=42, n_jobs=-1\n    )\n    lr_clf.fit(X_train_scaled, y_train)\n    lr_pred = lr_clf.predict(X_test_scaled)\n    lr_acc = accuracy_score(y_test, lr_pred)\n    lr_f1 = f1_score(y_test, lr_pred, average='weighted')\n    results.append(('LogisticRegression', lr_clf, lr_acc, lr_f1))\n    print(f\"        Accuracy: {lr_acc:.1%}, F1: {lr_f1:.3f}\")\n    \n    # Baseline: always predict most common class\n    most_common = y_train.mode()[0]\n    baseline_acc = (y_test == most_common).mean()\n    print(f\"\\n  Baseline (always predict {['Down', 'Stable', 'Up'][most_common]}): {baseline_acc:.1%}\")\n    \n    # Select best by F1 score (better for imbalanced classes)\n    best = max(results, key=lambda x: x[3])\n    print(f\"\\n  >>> Best: {best[0]} (Accuracy: {best[2]:.1%}, F1: {best[3]:.3f})\")\n    print(f\"      Improvement over baseline: {(best[2] - baseline_acc)*100:+.1f}%\")\n    \n    # Print classification report for best model\n    best_pred = best[1].predict(X_test_scaled)\n    print(f\"\\n  Classification Report ({best[0]}):\")\n    print(classification_report(y_test, best_pred, target_names=['Down', 'Stable', 'Up']))\n    \n    return best[1], scaler, best[2], best[3]\n\n# Train with class weights\ndir_clf_1h, dir_scaler_1h, dir_acc_1h, dir_f1_1h = train_direction_model(X, y_dir_1h, '1h', use_class_weights=True)\ndir_clf_4h, dir_scaler_4h, dir_acc_4h, dir_f1_4h = train_direction_model(X, y_dir_4h, '4h', use_class_weights=True)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Direction classifiers trained successfully\")\nprint(f\"  1h: Accuracy={dir_acc_1h:.1%}, F1={dir_f1_1h:.3f}\")\nprint(f\"  4h: Accuracy={dir_acc_4h:.1%}, F1={dir_f1_4h:.3f}\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train Spike Detectors\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING SPIKE DETECTORS\")\nprint(\"=\"*60)\n\ndef train_spike_detector(X, y_target, current_gas, horizon_name):\n    \"\"\"Train spike classification model\"\"\"\n    print(f\"\\nTraining {horizon_name} spike detector...\")\n    \n    # Classify based on relative change from current\n    price_change_pct = (y_target - current_gas) / (current_gas + 1e-8)\n    \n    # Normal: < 50% change, Elevated: 50-100%, Spike: > 100%\n    def classify(pct):\n        pct = abs(pct)\n        if pct < 0.5:\n            return 0  # Normal\n        elif pct < 1.0:\n            return 1  # Elevated\n        else:\n            return 2  # Spike\n    \n    y_class = price_change_pct.apply(classify)\n    \n    # Class distribution\n    class_counts = y_class.value_counts().sort_index()\n    print(f\"  Classes: Normal={class_counts.get(0,0)}, Elevated={class_counts.get(1,0)}, Spike={class_counts.get(2,0)}\")\n    \n    # Split (time-series)\n    split_idx = int(len(X) * 0.8)\n    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n    y_train, y_test = y_class.iloc[:split_idx], y_class.iloc[split_idx:]\n    \n    # Scale\n    scaler = RobustScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Train\n    clf = GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)\n    clf.fit(X_train_scaled, y_train)\n    \n    accuracy = clf.score(X_test_scaled, y_test)\n    print(f\"  Accuracy: {accuracy:.1%}\")\n    \n    return clf, scaler\n\nspike_1h, spike_scaler_1h = train_spike_detector(X, y_1h, current_gas, '1h')\nspike_4h, spike_scaler_4h = train_spike_detector(X, y_4h, current_gas, '4h')\nspike_24h, spike_scaler_24h = train_spike_detector(X, y_24h, current_gas, '24h')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save all models with feature importance and confidence info\nimport os\nfrom datetime import datetime\n\nos.makedirs('saved_models', exist_ok=True)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"SAVING MODELS\")\nprint(\"=\"*60)\n\n# Get feature importance from best model if available\nfeature_importance = {}\ntry:\n    # Try to get from 4h model (usually RandomForest)\n    for name, model, metrics, scaler in all_4h:\n        if hasattr(model, 'feature_importances_'):\n            feature_importance = dict(zip(features_4h_used, model.feature_importances_))\n            break\nexcept:\n    pass\n\n# Save prediction models\nfor horizon, best, features_used in [('1h', best_1h, features_1h_used), \n                                      ('4h', best_4h, features_4h_used), \n                                      ('24h', best_24h, features_4h_used)]:\n    name, model, metrics, scaler = best\n    \n    model_data = {\n        'model': model,\n        'model_name': name,\n        'metrics': metrics,\n        'trained_at': datetime.now().isoformat(),\n        'feature_names': features_used,\n        'feature_scaler': scaler,\n        'scaler_type': 'RobustScaler',\n        'is_ensemble': name == 'Ensemble',\n        'training_strategy': 'simpler_regularized' if horizon == '1h' else 'full_tuned',\n        'actual_horizon': '1 hour' if horizon == '1h' else '4 hours',\n        'confidence_method': 'tree_variance' if hasattr(model, 'estimators_') else 'fixed'\n    }\n    \n    # Add feature importance for relevant models\n    if horizon == '4h' and feature_importance:\n        model_data['feature_importance'] = feature_importance\n    \n    joblib.dump(model_data, f'saved_models/model_{horizon}.pkl')\n    print(f\"Saved model_{horizon}.pkl ({name}, MAE={metrics['mae']:.6f}, {metrics['vs_baseline']})\")\n    \n    # Save scaler separately\n    joblib.dump(scaler, f'saved_models/scaler_{horizon}.pkl')\n\n# Save spike detectors\nfor horizon, (clf, scaler) in [('1h', (spike_1h, spike_scaler_1h)), \n                                ('4h', (spike_4h, spike_scaler_4h)),\n                                ('24h', (spike_24h, spike_scaler_24h))]:\n    spike_data = {\n        'model': clf,\n        'scaler': scaler,\n        'trained_at': datetime.now().isoformat()\n    }\n    joblib.dump(spike_data, f'saved_models/spike_detector_{horizon}.pkl')\n    print(f\"Saved spike_detector_{horizon}.pkl\")\n\n# Save feature names (all features for compatibility)\njoblib.dump(features, 'saved_models/feature_names.pkl')\nprint(f\"Saved feature_names.pkl ({len(features)} features)\")\n\n# Save training metadata with full info\nimport json\nmetadata = {\n    'training_timestamp': datetime.now().isoformat(),\n    'total_samples': len(df_clean),\n    'date_range': f\"{df_clean.index.min()} to {df_clean.index.max()}\",\n    'num_segments_used': len(good_segments),\n    'features': {\n        'total': len(features),\n        '1h_specific': len(features_1h_used),\n        '4h_specific': len(features_4h_used)\n    },\n    'baselines': BASELINES,\n    'models': {\n        '1h': {\n            'name': best_1h[0], \n            'r2': float(best_1h[2]['r2']), \n            'mae': float(best_1h[2]['mae']),\n            'vs_baseline': best_1h[2]['vs_baseline'],\n            'improvement_pct': float(best_1h[2]['improvement']),\n            'actual_horizon': '1 hour',\n            'training_strategy': 'simpler models with strong regularization (Ridge, ElasticNet, Huber)',\n            'directional_accuracy': float(best_1h[2]['directional_accuracy'])\n        },\n        '4h': {\n            'name': best_4h[0], \n            'r2': float(best_4h[2]['r2']), \n            'mae': float(best_4h[2]['mae']),\n            'vs_baseline': best_4h[2]['vs_baseline'],\n            'improvement_pct': float(best_4h[2]['improvement']),\n            'actual_horizon': '4 hours',\n            'training_strategy': 'full model suite with hyperparameter tuning',\n            'directional_accuracy': float(best_4h[2]['directional_accuracy'])\n        },\n        '24h': {\n            'name': best_24h[0], \n            'r2': float(best_24h[2]['r2']), \n            'mae': float(best_24h[2]['mae']),\n            'vs_baseline': best_24h[2]['vs_baseline'],\n            'improvement_pct': float(best_24h[2]['improvement']),\n            'actual_horizon': '4 hours (labeled as 24h due to data limitations)',\n            'training_strategy': 'same as 4h model',\n            'directional_accuracy': float(best_24h[2]['directional_accuracy'])\n        }\n    },\n    'direction_models': {\n        '1h': {\n            'accuracy': float(dir_acc_1h),\n            'f1_score': float(dir_f1_1h)\n        },\n        '4h': {\n            'accuracy': float(dir_acc_4h),\n            'f1_score': float(dir_f1_4h)\n        }\n    },\n    'improvements_applied': [\n        'Micro-features (5min, 15min, 30min) for 1h prediction',\n        'Time features (hour, day, peak hours)',\n        'Simpler models (Ridge, ElasticNet, Huber) for 1h',\n        'Hyperparameter tuning (RandomizedSearchCV) for 4h',\n        'Class-weighted direction classification',\n        'Confidence scoring based on tree variance',\n        'Percentage-based direction thresholds (2%)',\n        'Drift baseline comparison',\n        'Stricter correlation filtering (0.90)'\n    ]\n}\n\nwith open('saved_models/training_metadata.json', 'w') as f:\n    json.dump(metadata, f, indent=2)\nprint(f\"Saved training_metadata.json\")\n\n# Save feature importance separately\nif feature_importance:\n    with open('saved_models/feature_importance.json', 'w') as f:\n        # Sort by importance\n        sorted_importance = dict(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True))\n        json.dump(sorted_importance, f, indent=2)\n    print(f\"Saved feature_importance.json (top features: {list(sorted_importance.keys())[:5]})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Print final report\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING COMPLETE - FINAL REPORT\")\nprint(\"=\"*70)\n\nprint(f\"\\nDATA SUMMARY\")\nprint(f\"   Total samples used: {len(df_clean):,}\")\nprint(f\"   Segments combined: {len(good_segments)}\")\nprint(f\"   Features (all): {len(features)}\")\nprint(f\"   Features (1h): {len(features_1h_used)}\")\nprint(f\"   Features (4h): {len(features_4h_used)}\")\nprint(f\"   Date range: {df_clean.index.min()} to {df_clean.index.max()}\")\n\nprint(f\"\\nBASELINE COMPARISON\")\nprint(f\"   1h Baselines:\")\nprint(f\"      Naive (current price): {BASELINES['1h']['naive_mae']:.6f}\")\nprint(f\"      Mean (average):        {BASELINES['1h']['mean_mae']:.6f}\")\nprint(f\"      Drift (trend):         {BASELINES['1h']['drift_mae']:.6f}\")\nprint(f\"      Best baseline:         {BASELINES['1h']['best']:.6f}\")\nprint(f\"   4h Baselines:\")\nprint(f\"      Naive (current price): {BASELINES['4h']['naive_mae']:.6f}\")\nprint(f\"      Mean (average):        {BASELINES['4h']['mean_mae']:.6f}\")\nprint(f\"      Best baseline:         {BASELINES['4h']['best']:.6f}\")\n\nprint(f\"\\n\" + \"-\"*70)\nprint(f\"{'PRICE PREDICTION MODELS':^70}\")\nprint(\"-\"*70)\nprint(f\"{'Horizon':<8} {'Model':<18} {'MAE':>10} {'R²':>8} {'vs Baseline':>13} {'Dir Acc':>8}\")\nprint(\"-\"*70)\n\nfor horizon, best in [('1h', best_1h), ('4h', best_4h), ('24h*', best_24h)]:\n    name = best[0][:17]\n    metrics = best[2]\n    print(f\"{horizon:<8} {name:<18} {metrics['mae']:>10.6f} {metrics['r2']:>8.4f} {metrics['vs_baseline']:>13} {metrics['directional_accuracy']:>7.1%}\")\n\nprint(\"-\"*70)\nprint(\"* 24h model = 4h model (insufficient data for true 24h prediction)\")\n\nprint(f\"\\n\" + \"-\"*70)\nprint(f\"{'DIRECTION CLASSIFICATION':^70}\")\nprint(\"-\"*70)\nprint(f\"{'Horizon':<8} {'Accuracy':>10} {'F1 Score':>10} {'Classes':<30}\")\nprint(\"-\"*70)\nprint(f\"{'1h':<8} {dir_acc_1h:>9.1%} {dir_f1_1h:>10.3f} {'Down / Stable / Up':<30}\")\nprint(f\"{'4h':<8} {dir_acc_4h:>9.1%} {dir_f1_4h:>10.3f} {'Down / Stable / Up':<30}\")\nprint(\"-\"*70)\n\nprint(f\"\\nKEY INSIGHTS\")\n# 1h model\nif best_1h[2]['improvement'] > 0:\n    print(f\"   1h: ML beats baseline by {best_1h[2]['improvement']:.1f}% - model is learning!\")\nelse:\n    print(f\"   1h: ML is {abs(best_1h[2]['improvement']):.1f}% worse than baseline\")\n    print(f\"       Short-term gas prices are very noisy. Need more data.\")\n\n# 4h model\nif best_4h[2]['improvement'] > 0:\n    print(f\"   4h: ML beats baseline by {best_4h[2]['improvement']:.1f}% - good performance!\")\nelse:\n    print(f\"   4h: ML is {abs(best_4h[2]['improvement']):.1f}% worse than baseline\")\n\n# Direction\nif dir_acc_1h > 0.4:\n    print(f\"   Direction 1h: {dir_acc_1h:.1%} accuracy (>40% = useful signal)\")\nif dir_acc_4h > 0.4:\n    print(f\"   Direction 4h: {dir_acc_4h:.1%} accuracy (>40% = useful signal)\")\n\nprint(f\"\\nIMPROVEMENTS APPLIED\")\nprint(f\"   - Micro-features (5/15/30min windows) for 1h prediction\")\nprint(f\"   - Simpler models (Ridge, ElasticNet, Huber) for noisy 1h data\")\nprint(f\"   - Hyperparameter tuning (RandomizedSearchCV) for 4h\")\nprint(f\"   - Class-weighted direction classification\")\nprint(f\"   - Peak hours time features (14:00-22:00 UTC)\")\nprint(f\"   - Acceleration features (rate of change of momentum)\")\nprint(f\"   - Confidence scoring based on tree variance\")\n\nprint(f\"\\nFILES SAVED\")\nprint(f\"   - model_1h.pkl, model_4h.pkl, model_24h.pkl (prediction models)\")\nprint(f\"   - scaler_1h.pkl, scaler_4h.pkl, scaler_24h.pkl (feature scalers)\")  \nprint(f\"   - spike_detector_1h/4h/24h.pkl (spike classifiers)\")\nprint(f\"   - feature_names.pkl ({len(features)} features)\")\nprint(f\"   - feature_importance.json (sorted by importance)\")\nprint(f\"   - training_metadata.json (full training report)\")\n\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "code",
   "source": "# Visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\nfig.suptitle('Gweizy Model Training Results', fontsize=14, fontweight='bold')\n\n# 1. Actual vs Predicted (1h)\nax1 = axes[0, 0]\nax1.scatter(actual_1h.values, pred_1h, alpha=0.5, s=10)\nax1.plot([actual_1h.min(), actual_1h.max()], [actual_1h.min(), actual_1h.max()], 'r--', label='Perfect')\nax1.set_xlabel('Actual Gas Price')\nax1.set_ylabel('Predicted')\nax1.set_title(f'1h Prediction (R²={best_1h[2][\"r2\"]:.3f})')\nax1.legend()\n\n# 2. Actual vs Predicted (4h)\nax2 = axes[0, 1]\nax2.scatter(actual_4h.values, pred_4h, alpha=0.5, s=10)\nax2.plot([actual_4h.min(), actual_4h.max()], [actual_4h.min(), actual_4h.max()], 'r--', label='Perfect')\nax2.set_xlabel('Actual Gas Price')\nax2.set_ylabel('Predicted')\nax2.set_title(f'4h Prediction (R²={best_4h[2][\"r2\"]:.3f})')\nax2.legend()\n\n# 3. Model Comparison (MAE)\nax3 = axes[0, 2]\nmodels_1h = [r[0] for r in all_1h]\nmaes_1h = [r[2]['mae'] for r in all_1h]\ncolors = ['green' if m < BASELINES['1h']['best'] else 'red' for m in maes_1h]\nbars = ax3.barh(models_1h, maes_1h, color=colors, alpha=0.7)\nax3.axvline(BASELINES['1h']['best'], color='blue', linestyle='--', label=f'Baseline: {BASELINES[\"1h\"][\"best\"]:.4f}')\nax3.set_xlabel('MAE')\nax3.set_title('1h Model Comparison')\nax3.legend()\n\n# 4. Residuals Distribution (1h)\nax4 = axes[1, 0]\nresiduals_1h = actual_1h.values - pred_1h\nax4.hist(residuals_1h, bins=50, alpha=0.7, edgecolor='black')\nax4.axvline(0, color='red', linestyle='--')\nax4.set_xlabel('Residual (Actual - Predicted)')\nax4.set_ylabel('Frequency')\nax4.set_title(f'1h Residuals (mean={np.mean(residuals_1h):.4f})')\n\n# 5. Time Series Sample\nax5 = axes[1, 1]\nsample_size = min(200, len(actual_1h))\nax5.plot(range(sample_size), actual_1h.values[:sample_size], label='Actual', alpha=0.8)\nax5.plot(range(sample_size), pred_1h[:sample_size], label='Predicted', alpha=0.8)\nax5.set_xlabel('Time (samples)')\nax5.set_ylabel('Gas Price')\nax5.set_title('1h: Actual vs Predicted (Time Series)')\nax5.legend()\n\n# 6. Feature Importance (top 10)\nax6 = axes[1, 2]\nif feature_importance:\n    sorted_imp = dict(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10])\n    ax6.barh(list(sorted_imp.keys()), list(sorted_imp.values()), color='steelblue')\n    ax6.set_xlabel('Importance')\n    ax6.set_title('Top 10 Features (4h model)')\nelse:\n    ax6.text(0.5, 0.5, 'Feature importance\\nnot available', ha='center', va='center')\n    ax6.set_title('Feature Importance')\n\nplt.tight_layout()\nplt.savefig('saved_models/training_results.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"Saved training_results.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip file for download\n",
    "import shutil\n",
    "\n",
    "shutil.make_archive('gweizy_models', 'zip', 'saved_models')\n",
    "print(\"\\n✅ Created gweizy_models.zip\")\n",
    "print(\"\\nDownload this file and extract to: backend/models/saved_models/\")\n",
    "\n",
    "# Auto-download\n",
    "files.download('gweizy_models.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}