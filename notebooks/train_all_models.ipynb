{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gweizy Model Training Notebook\n",
    "\n",
    "Train all gas prediction models for Gweizy.\n",
    "\n",
    "## Instructions:\n",
    "1. Upload your `gas_data.db` file (from `backend/gas_data.db`)\n",
    "2. Run all cells\n",
    "3. Download the trained models zip file\n",
    "4. Extract to `backend/models/saved_models/` and push to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q scikit-learn pandas numpy joblib lightgbm xgboost matplotlib seaborn optuna"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your gas_data.db file\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"Upload your gas_data.db file from backend/gas_data.db\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "if 'gas_data.db' in uploaded:\n",
    "    print(f\"\\n\u2705 Uploaded gas_data.db ({len(uploaded['gas_data.db']) / 1024 / 1024:.1f} MB)\")\n",
    "else:\n",
    "    print(\"\u274c Please upload gas_data.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load data from database\n",
    "conn = sqlite3.connect('gas_data.db')\n",
    "df = pd.read_sql(\"\"\"\n",
    "    SELECT timestamp, current_gas as gas, base_fee, priority_fee, \n",
    "           block_number, gas_used, gas_limit, utilization\n",
    "    FROM gas_prices ORDER BY timestamp ASC\n",
    "\"\"\", conn)\n",
    "conn.close()\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df = df.set_index('timestamp').sort_index()\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "\n",
    "# === IMPROVED: Resample to 30-second intervals (was 1-min, losing too much data) ===\n",
    "print(\"\\nResampling to 30-second intervals (preserves more data)...\")\n",
    "df = df.resample('30s').mean().dropna(subset=['gas'])\n",
    "print(f\"After resample: {len(df):,} records\")\n",
    "\n",
    "# Find segments (gap > 30 min = new segment)\n",
    "df['time_diff'] = df.index.to_series().diff()\n",
    "df['segment'] = (df['time_diff'] > pd.Timedelta(minutes=30)).cumsum()\n",
    "\n",
    "segment_sizes = df.groupby('segment').size()\n",
    "print(f\"\\nSegments found: {len(segment_sizes)}\")\n",
    "print(f\"Segment sizes: {segment_sizes.sort_values(ascending=False).head(10).tolist()}\")\n",
    "\n",
    "# === IMPROVED: Lower threshold from 120 to 30 minutes (keeps more segments) ===\n",
    "MIN_SEGMENT_SIZE = 60  # 30 minutes at 30-sec intervals = 60 records\n",
    "good_segments = segment_sizes[segment_sizes >= MIN_SEGMENT_SIZE].index.tolist()\n",
    "df = df[df['segment'].isin(good_segments)]\n",
    "print(f\"\\nKeeping {len(good_segments)} segments with >= 30 minutes of data\")\n",
    "print(f\"Total usable records: {len(df):,}\")\n",
    "\n",
    "# === DATA SUFFICIENCY CHECK ===\n",
    "MIN_REQUIRED_SAMPLES = 10000\n",
    "if len(df) < MIN_REQUIRED_SAMPLES:\n",
    "    print(f\"\\n\u26a0\ufe0f  WARNING: Only {len(df):,} samples. Recommend at least {MIN_REQUIRED_SAMPLES:,}\")\n",
    "    print(\"   Models may underperform. Consider collecting more data.\")\n",
    "else:\n",
    "    print(f\"\\n\u2713 Data sufficiency check passed: {len(df):,} samples\")\n",
    "\n",
    "RECORDS_PER_HOUR = 120  # 30-sec intervals = 120 records per hour"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Fetch ETH Price Data - IMPROVED with Binance (1-minute data)\nimport requests\n\nprint(\"=\"*60)\nprint(\"FETCHING EXTERNAL DATA\")\nprint(\"=\"*60)\n\ndef fetch_eth_price_binance(start_date, end_date):\n    \"\"\"Fetch ETH price from Binance API (1-minute candles, much better than CoinGecko hourly)\"\"\"\n    try:\n        start_ts = int(start_date.timestamp() * 1000)\n        end_ts = int(end_date.timestamp() * 1000)\n        \n        all_prices = []\n        current_ts = start_ts\n        \n        print(f\"Fetching ETH prices from Binance (1-min candles)...\")\n        \n        while current_ts < end_ts:\n            url = \"https://api.binance.com/api/v3/klines\"\n            params = {\n                'symbol': 'ETHUSDT',\n                'interval': '1m',\n                'startTime': current_ts,\n                'endTime': min(current_ts + 1000 * 60 * 1000, end_ts),  # Max 1000 candles\n                'limit': 1000\n            }\n            \n            response = requests.get(url, params=params, timeout=30)\n            \n            if response.status_code == 200:\n                data = response.json()\n                if not data:\n                    break\n                    \n                for candle in data:\n                    all_prices.append({\n                        'timestamp': pd.to_datetime(candle[0], unit='ms'),\n                        'eth_price': float(candle[4]),  # Close price\n                        'eth_volume': float(candle[5]),  # Volume\n                        'eth_high': float(candle[2]),\n                        'eth_low': float(candle[3])\n                    })\n                \n                current_ts = data[-1][0] + 60000  # Next minute\n                \n                if len(all_prices) % 5000 == 0:\n                    print(f\"  Fetched {len(all_prices):,} candles...\")\n            else:\n                print(f\"  Binance API error: {response.status_code}\")\n                break\n        \n        if all_prices:\n            eth_df = pd.DataFrame(all_prices)\n            eth_df = eth_df.set_index('timestamp')\n            print(f\"  Total: {len(eth_df):,} 1-minute ETH candles\")\n            return eth_df\n        return None\n        \n    except Exception as e:\n        print(f\"  Failed to fetch from Binance: {e}\")\n        return None\n\ndef fetch_eth_price_coingecko(start_date, end_date):\n    \"\"\"Fallback: CoinGecko API (hourly data)\"\"\"\n    try:\n        start_ts = int(start_date.timestamp())\n        end_ts = int(end_date.timestamp())\n        \n        url = \"https://api.coingecko.com/api/v3/coins/ethereum/market_chart/range\"\n        params = {'vs_currency': 'usd', 'from': start_ts, 'to': end_ts}\n        \n        print(f\"Fallback: Fetching from CoinGecko (hourly)...\")\n        response = requests.get(url, params=params, timeout=30)\n        \n        if response.status_code == 200:\n            data = response.json()\n            prices = data.get('prices', [])\n            \n            eth_df = pd.DataFrame(prices, columns=['timestamp', 'eth_price'])\n            eth_df['timestamp'] = pd.to_datetime(eth_df['timestamp'], unit='ms')\n            eth_df = eth_df.set_index('timestamp')\n            eth_df['eth_volume'] = np.nan\n            eth_df['eth_high'] = eth_df['eth_price']\n            eth_df['eth_low'] = eth_df['eth_price']\n            \n            print(f\"  Fetched {len(eth_df)} hourly ETH prices\")\n            return eth_df\n        return None\n    except Exception as e:\n        print(f\"  CoinGecko failed: {e}\")\n        return None\n\n# Try Binance first, fallback to CoinGecko\neth_data = fetch_eth_price_binance(df.index.min(), df.index.max())\nif eth_data is None or len(eth_data) < 100:\n    eth_data = fetch_eth_price_coingecko(df.index.min(), df.index.max())\n\nhas_eth_data = False\nif eth_data is not None and len(eth_data) > 0:\n    # Resample to 30-second intervals\n    eth_data = eth_data.resample('30s').ffill()\n    \n    # Merge with gas data\n    df = df.join(eth_data, how='left')\n    df['eth_price'] = df['eth_price'].ffill().bfill()\n    \n    # Fill other ETH columns\n    for col in ['eth_volume', 'eth_high', 'eth_low']:\n        if col in df.columns:\n            df[col] = df[col].ffill().bfill()\n    \n    eth_coverage = df['eth_price'].notna().mean()\n    print(f\"  ETH price coverage: {eth_coverage:.1%}\")\n    \n    if eth_coverage > 0.5:\n        has_eth_data = True\n        print(\"  \u2713 ETH price data integrated (1-min resolution)\")\nelse:\n    print(\"  \u26a0\ufe0f No ETH price data available\")\n    df['eth_price'] = np.nan\n    df['eth_volume'] = np.nan\n    df['eth_high'] = np.nan\n    df['eth_low'] = np.nan\n\nHAS_ETH_PRICE = has_eth_data"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - SIMPLIFIED v3 + SPIKE-ADJUSTED TARGETS",
    "# Focus: 15-20 high-value features to prevent overfitting",
    "# NEW: Option for log-transformed or winsorized targets",
    "",
    "print(\"Engineering SIMPLIFIED feature set (15-20 features)...\")",
    "",
    "# === CONFIGURATION ===",
    "TARGET_TRANSFORM = \"log\"  # Options: \"none\", \"log\", \"winsorize\"",
    "WINSORIZE_PERCENTILE = 0.95  # For winsorize: cap at this percentile",
    "",
    "def engineer_features_for_segment(seg_df, has_eth=False, horizon='all'):",
    "    \"\"\"Engineer focused feature set - quality over quantity\"\"\"",
    "    df = seg_df.copy()",
    "    rph = 120  # records per hour (30-sec intervals)",
    "    ",
    "    # === TIME FEATURES (3 features) ===",
    "    df['hour'] = df.index.hour",
    "    hour_of_day = df.index.hour + df.index.minute / 60",
    "    df['hour_sin'] = np.sin(2 * np.pi * hour_of_day / 24)",
    "    df['hour_cos'] = np.cos(2 * np.pi * hour_of_day / 24)",
    "    ",
    "    # === ETH FEATURES (2 features) ===",
    "    if has_eth and 'eth_price' in df.columns and df['eth_price'].notna().any():",
    "        df['eth_log'] = np.log1p(df['eth_price'])",
    "        eth_mean = df['eth_price'].rolling(4*rph, min_periods=rph).mean()",
    "        eth_std = df['eth_price'].rolling(4*rph, min_periods=rph).std()",
    "        df['eth_zscore_4h'] = np.where(eth_std > 0.01, (df['eth_price'] - eth_mean) / eth_std, 0)",
    "        df['gas_eth_corr_1h'] = df['gas'].rolling(rph, min_periods=rph//2).corr(df['eth_price']).fillna(0)",
    "    ",
    "    # === NETWORK UTILIZATION (2 features) ===",
    "    if 'utilization' in df.columns:",
    "        df['util_mean_1h'] = df['utilization'].rolling(rph, min_periods=rph//2).mean()",
    "        df['util_mean_2h'] = df['utilization'].rolling(2*rph, min_periods=rph).mean()",
    "    ",
    "    # === GAS LAG FEATURES (5 features) ===",
    "    df['gas_lag_5min'] = df['gas'].shift(10)",
    "    df['gas_lag_15min'] = df['gas'].shift(30)",
    "    df['gas_lag_30min'] = df['gas'].shift(60)",
    "    df['gas_lag_1h'] = df['gas'].shift(rph)",
    "    df['gas_lag_4h'] = df['gas'].shift(4*rph)",
    "    ",
    "    # === ROLLING STATS (6 features) ===",
    "    df['gas_mean_1h'] = df['gas'].rolling(rph, min_periods=rph//2).mean()",
    "    df['gas_std_1h'] = df['gas'].rolling(rph, min_periods=rph//2).std()",
    "    df['gas_cv_1h'] = np.where(df['gas_mean_1h'] > 0.01, ",
    "                                df['gas_std_1h'] / df['gas_mean_1h'], 0)",
    "    df['gas_mean_2h'] = df['gas'].rolling(2*rph, min_periods=rph).mean()",
    "    df['gas_mean_4h'] = df['gas'].rolling(4*rph, min_periods=rph).mean()",
    "    ",
    "    # === MOMENTUM (3 features) ===",
    "    df['momentum_1h'] = df['gas'] - df['gas'].shift(rph)",
    "    shift_2h = df['gas'].shift(2*rph)",
    "    df['momentum_pct_2h'] = np.where(shift_2h > 0.01, (df['gas'] - shift_2h) / shift_2h, 0)",
    "    df['trend_1h_4h'] = np.where(df['gas_mean_4h'] > 0.01, df['gas_mean_1h'] / df['gas_mean_4h'], 1.0)",
    "    ",
    "    # === Z-SCORE AND REGIME (3 features) ===",
    "    df['gas_zscore_1h'] = np.where(df['gas_std_1h'] > 0.001, ",
    "        (df['gas'] - df['gas_mean_1h']) / df['gas_std_1h'], 0)",
    "    df['is_spike'] = (df['gas'] > df['gas_mean_1h'] + 2 * df['gas_std_1h']).astype(int)",
    "    df['is_high_gas'] = (df['gas'] > df['gas'].rolling(4*rph, min_periods=rph).quantile(0.9)).astype(int)",
    "    ",
    "    return df",
    "",
    "# Process each segment",
    "print(\"\\nProcessing segments...\")",
    "segments = df['segment'].unique()",
    "processed_segments = []",
    "",
    "for seg_id in segments:",
    "    seg_df = df[df['segment'] == seg_id].copy()",
    "    processed = engineer_features_for_segment(seg_df, has_eth=has_eth_data, horizon='all')",
    "    processed_segments.append(processed)",
    "",
    "df_features = pd.concat(processed_segments, axis=0)",
    "print(f\"After feature engineering: {len(df_features):,} records\")",
    "",
    "# Create targets",
    "print(\"\\nCreating prediction targets...\")",
    "",
    "def create_targets_for_segment(seg_df, transform=\"none\", winsorize_pct=0.95):",
    "    \"\"\"Create target variables with optional transformation\"\"\"",
    "    df = seg_df.copy()",
    "    rph = 120",
    "    ",
    "    # Raw future prices",
    "    raw_1h = df['gas'].shift(-rph)",
    "    raw_4h = df['gas'].shift(-4*rph)",
    "    raw_24h = df['gas'].shift(-24*rph)",
    "    ",
    "    # Apply transformation",
    "    if transform == \"log\":",
    "        # Log transform - better for multiplicative changes",
    "        df['target_1h'] = np.log1p(raw_1h)",
    "        df['target_4h'] = np.log1p(raw_4h)",
    "        df['target_24h'] = np.log1p(raw_24h)",
    "        # Also store raw for evaluation",
    "        df['target_1h_raw'] = raw_1h",
    "        df['target_4h_raw'] = raw_4h",
    "        df['target_24h_raw'] = raw_24h",
    "    elif transform == \"winsorize\":",
    "        # Winsorize - cap extreme values",
    "        cap_1h = raw_1h.quantile(winsorize_pct)",
    "        cap_4h = raw_4h.quantile(winsorize_pct)",
    "        cap_24h = raw_24h.quantile(winsorize_pct) if raw_24h.notna().sum() > 100 else cap_4h",
    "        df['target_1h'] = raw_1h.clip(upper=cap_1h)",
    "        df['target_4h'] = raw_4h.clip(upper=cap_4h)",
    "        df['target_24h'] = raw_24h.clip(upper=cap_24h)",
    "        df['target_1h_raw'] = raw_1h",
    "        df['target_4h_raw'] = raw_4h",
    "        df['target_24h_raw'] = raw_24h",
    "        print(f\"  Winsorized caps: 1h={cap_1h:.2f}, 4h={cap_4h:.2f}\")",
    "    else:",
    "        # No transform",
    "        df['target_1h'] = raw_1h",
    "        df['target_4h'] = raw_4h",
    "        df['target_24h'] = raw_24h",
    "    ",
    "    # Direction classification (always on raw)",
    "    threshold = 0.02",
    "    for horizon in ['1h', '4h']:",
    "        raw_target = raw_1h if horizon == '1h' else raw_4h",
    "        pct_change = np.where(df['gas'] > 0.001, ",
    "            (raw_target - df['gas']) / df['gas'], 0)",
    "        df[f'direction_class_{horizon}'] = pd.cut(",
    "            pct_change,",
    "            bins=[-float('inf'), -threshold, threshold, float('inf')],",
    "            labels=['down', 'stable', 'up']",
    "        )",
    "    ",
    "    return df",
    "",
    "print(f\"Target transform: {TARGET_TRANSFORM}\")",
    "processed_with_targets = []",
    "for seg_id in df_features['segment'].unique():",
    "    seg_df = df_features[df_features['segment'] == seg_id].copy()",
    "    processed = create_targets_for_segment(seg_df, transform=TARGET_TRANSFORM, winsorize_pct=WINSORIZE_PERCENTILE)",
    "    processed_with_targets.append(processed)",
    "",
    "df_features = pd.concat(processed_with_targets, axis=0)",
    "",
    "# Store transform info for later use",
    "TARGET_TRANSFORM_USED = TARGET_TRANSFORM",
    "",
    "# === CLEAN INF/NAN VALUES ===",
    "print(\"\\nCleaning inf/nan values...\")",
    "numeric_cols = df_features.select_dtypes(include=[np.number]).columns",
    "",
    "for col in numeric_cols:",
    "    df_features[col] = df_features[col].replace([np.inf, -np.inf], np.nan)",
    "    if df_features[col].notna().sum() > 0:",
    "        q_low = df_features[col].quantile(0.001)",
    "        q_high = df_features[col].quantile(0.999)",
    "        df_features[col] = df_features[col].clip(q_low, q_high)",
    "",
    "df_features = df_features.ffill().bfill()",
    "",
    "for col in numeric_cols:",
    "    if df_features[col].isna().any():",
    "        median_val = df_features[col].median()",
    "        if pd.isna(median_val):",
    "            median_val = 0",
    "        df_features[col] = df_features[col].fillna(median_val)",
    "",
    "inf_count = np.isinf(df_features.select_dtypes(include=[np.number])).sum().sum()",
    "nan_count = df_features.select_dtypes(include=[np.number]).isna().sum().sum()",
    "print(f\"  After cleaning: {inf_count} inf, {nan_count} nan values\")",
    "",
    "# === DEFINE FOCUSED FEATURE SET ===",
    "# === EXTERNAL DATA INTEGRATION HOOKS ===",
    "# These features will be used if the corresponding data is available",
    "",
    "def add_external_features(df):",
    "    \"\"\"",
    "    Add external data features when available.",
    "    Currently supports: blob_gas, l1_congestion, day_of_week",
    "    \"\"\"",
    "    external_added = []",
    "",
    "    # === BLOB GAS (EIP-4844) ===",
    "    if 'blob_gas_price' in df.columns:",
    "        df['blob_gas_log'] = np.log1p(df['blob_gas_price'])",
    "        df['blob_vs_gas_ratio'] = df['blob_gas_price'] / (df['gas'] + 1e-8)",
    "        external_added.extend(['blob_gas_log', 'blob_vs_gas_ratio'])",
    "",
    "    # === L1 CONGESTION ===",
    "    if 'l1_gas_price' in df.columns:",
    "        df['l1_gas_log'] = np.log1p(df['l1_gas_price'])",
    "        df['l2_l1_ratio'] = df['gas'] / (df['l1_gas_price'] + 1e-8)",
    "        external_added.extend(['l1_gas_log', 'l2_l1_ratio'])",
    "",
    "    # === DAY OF WEEK (always available from timestamp) ===",
    "    if hasattr(df.index, 'dayofweek'):",
    "        df['day_of_week'] = df.index.dayofweek",
    "        df['is_weekend'] = (df.index.dayofweek >= 5).astype(int)",
    "        # Cyclical encoding",
    "        df['dow_sin'] = np.sin(2 * np.pi * df.index.dayofweek / 7)",
    "        df['dow_cos'] = np.cos(2 * np.pi * df.index.dayofweek / 7)",
    "        external_added.extend(['day_of_week', 'is_weekend', 'dow_sin', 'dow_cos'])",
    "",
    "    # === HOLIDAY INDICATOR (placeholder) ===",
    "    # If holiday data is provided in a 'is_holiday' column",
    "    if 'is_holiday' in df.columns:",
    "        external_added.append('is_holiday')",
    "",
    "    return df, external_added",
    "",
    "# Apply external features to all segments",
    "print(\"\\nAdding external data features...\")",
    "if 'segment' not in df_features.columns:",
    "    print('  Note: No segment column, treating as single segment')",
    "    df_features['segment'] = 0",
    "",
    "external_features_added = []",
    "processed_with_external = []",
    "",
    "for seg_id in df_features['segment'].unique():",
    "    seg_df = df_features[df_features['segment'] == seg_id].copy()",
    "    seg_df, added = add_external_features(seg_df)",
    "    processed_with_external.append(seg_df)",
    "    if not external_features_added:",
    "        external_features_added = added",
    "",
    "df_features = pd.concat(processed_with_external, axis=0)",
    "",
    "if external_features_added:",
    "    print(f\"  Added external features: {external_features_added}\")",
    "else:",
    "    print(\"  No external data columns found, using standard features only\")",
    "",
    "# === DEFINE FOCUSED FEATURE SET ===",
    "CORE_FEATURES = [",
    "    'hour', 'hour_sin', 'hour_cos',",
    "    'eth_log', 'eth_zscore_4h', 'gas_eth_corr_1h',",
    "    'util_mean_1h', 'util_mean_2h',",
    "    'gas_lag_5min', 'gas_lag_15min', 'gas_lag_30min', 'gas_lag_1h', 'gas_lag_4h',",
    "    'gas_mean_1h', 'gas_std_1h', 'gas_cv_1h', 'gas_mean_2h', 'gas_mean_4h',",
    "    'momentum_1h', 'momentum_pct_2h', 'trend_1h_4h',",
    "    'gas_zscore_1h', 'is_spike', 'is_high_gas'",
    "]",
    "",
    "# Add external features to core if available",
    "EXTENDED_FEATURES = CORE_FEATURES + [",
    "    'day_of_week', 'is_weekend', 'dow_sin', 'dow_cos',  # Day of week",
    "    'blob_gas_log', 'blob_vs_gas_ratio',  # Blob gas",
    "    'l1_gas_log', 'l2_l1_ratio',  # L1 congestion",
    "    'is_holiday'  # Holiday",
    "]",
    "",
    "# Use extended features if available, otherwise core",
    "available_features = [f for f in EXTENDED_FEATURES if f in df_features.columns]",
    "if len(available_features) == len([f for f in CORE_FEATURES if f in df_features.columns]):",
    "    print(\"  Using core features only\")",
    "else:",
    "    print(f\"  Using extended features ({len(available_features)} total)\")",
    "features_1h = available_features",
    "features_4h = available_features  ",
    "features_24h = available_features",
    "",
    "print(f\"\\n\u2713 Focused feature set: {len(available_features)} features\")",
    "print(f\"  Features: {', '.join(available_features)}\")",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data - with AUTO-ADAPT to distribution shift\n",
    "",
    "# Prepare training data - with AUTO-ADAPT to distribution shift",
    "from sklearn.preprocessing import RobustScaler",
    "",
    "# === SAFETY CHECK ===",
    "required_vars = ['df_features', 'features_1h', 'features_4h', 'features_24h']",
    "missing = [v for v in required_vars if v not in dir()]",
    "if missing:",
    "    raise RuntimeError(f\"Missing variables from Cell 5: {missing}. Please run Cell 5 (Feature Engineering) first!\")",
    "",
    "print(f\"\u2713 df_features loaded: {len(df_features):,} rows\")",
    "print(f\"\u2713 Features: 1h={len(features_1h)}, 4h={len(features_4h)}, 24h={len(features_24h)}\")",
    "",
    "",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy import stats\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "USE_ROLLING_WINDOW = False  # Set True to use only recent data (AUTO-ENABLED if shift detected)\n",
    "ROLLING_WINDOW_DAYS = 7     # Days of data to use if rolling window enabled\n",
    "HOLDOUT_HOURS = 48          # Hours to reserve for holdout\n",
    "AUTO_ADAPT_ON_SHIFT = True  # Automatically adapt when distribution shift detected\n",
    "\n",
    "# === VOLATILITY ADAPTATION SETTINGS ===\n",
    "# Shorter windows for more aggressive adaptation during volatile periods\n",
    "SEVERE_SHIFT_WINDOW_DAYS = 1.5    # Very aggressive - 36 hours\n",
    "HIGH_SHIFT_WINDOW_DAYS = 2        # Aggressive - 2 days  \n",
    "MODERATE_SHIFT_WINDOW_DAYS = 3    # Moderate - 3 days\n",
    "MILD_SHIFT_WINDOW_DAYS = 5        # Mild - 5 days\n",
    "\n",
    "# Only keep numeric columns\n",
    "numeric_features_1h = df_features[features_1h].select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_features_4h = df_features[features_4h].select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_features_24h = df_features[features_24h].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: 1h={len(numeric_features_1h)}, 4h={len(numeric_features_4h)}, 24h={len(numeric_features_24h)}\")\n",
    "\n",
    "# Drop rows only where TARGET columns are NaN\n",
    "target_cols = ['target_1h', 'target_4h']\n",
    "df_clean = df_features.dropna(subset=target_cols)\n",
    "print(f\"Clean samples (with valid 1h/4h targets): {len(df_clean):,}\")\n",
    "\n",
    "valid_24h = df_features['target_24h'].notna().sum()\n",
    "print(f\"Samples with valid 24h target: {valid_24h:,}\")\n",
    "\n",
    "# === OUT-OF-TIME HOLDOUT (do this FIRST to detect shift) ===\n",
    "rph = 120  # records per hour\n",
    "holdout_size = HOLDOUT_HOURS * rph\n",
    "\n",
    "if len(df_clean) > holdout_size + 5000:\n",
    "    df_train_val_initial = df_clean.iloc[:-holdout_size]\n",
    "    df_holdout = df_clean.iloc[-holdout_size:]\n",
    "    print(f\"\\n\u2713 Out-of-time holdout: {len(df_holdout):,} samples (last {HOLDOUT_HOURS}h)\")\n",
    "    HAS_HOLDOUT = True\n",
    "else:\n",
    "    df_train_val_initial = df_clean\n",
    "    df_holdout = None\n",
    "    print(f\"\\n\u26a0\ufe0f Not enough data for holdout, using all for training\")\n",
    "    HAS_HOLDOUT = False\n",
    "\n",
    "# === DISTRIBUTION SHIFT DETECTION ===\n",
    "def detect_distribution_shift(train_data, holdout_data, name=\"\"):\n",
    "    \"\"\"Detect distribution shift between train and holdout\"\"\"\n",
    "    results = {'name': name, 'warnings': [], 'passed': True, 'shift_magnitude': 0}\n",
    "    \n",
    "    train_mean, train_std = train_data.mean(), train_data.std()\n",
    "    holdout_mean = holdout_data.mean()\n",
    "    mean_shift = abs(holdout_mean - train_mean) / (train_std + 1e-8)\n",
    "    results['mean_shift_std'] = mean_shift\n",
    "    \n",
    "    if mean_shift > 1.0:\n",
    "        results['warnings'].append(f\"Large mean shift: {mean_shift:.2f} std devs\")\n",
    "        results['passed'] = False\n",
    "        results['shift_magnitude'] = max(results['shift_magnitude'], mean_shift)\n",
    "    elif mean_shift > 0.5:\n",
    "        results['warnings'].append(f\"Moderate mean shift: {mean_shift:.2f} std devs\")\n",
    "    \n",
    "    var_ratio = holdout_data.var() / (train_data.var() + 1e-8)\n",
    "    results['var_ratio'] = var_ratio\n",
    "    \n",
    "    if var_ratio > 4 or var_ratio < 0.25:\n",
    "        results['warnings'].append(f\"Large variance change: {var_ratio:.2f}x\")\n",
    "        results['passed'] = False\n",
    "        results['shift_magnitude'] = max(results['shift_magnitude'], abs(np.log(var_ratio)))\n",
    "    \n",
    "    ks_stat, ks_pval = stats.ks_2samp(train_data.values[:5000], holdout_data.values[:5000])\n",
    "    results['ks_statistic'] = ks_stat\n",
    "    results['ks_pvalue'] = ks_pval\n",
    "    \n",
    "    if ks_pval < 0.001 and ks_stat > 0.3:\n",
    "        results['warnings'].append(f\"KS test: distributions differ significantly\")\n",
    "        results['passed'] = False\n",
    "        results['shift_magnitude'] = max(results['shift_magnitude'], ks_stat * 3)\n",
    "    \n",
    "    train_spikes = (train_data > train_data.quantile(0.95)).mean()\n",
    "    holdout_spikes = (holdout_data > train_data.quantile(0.95)).mean()\n",
    "    spike_ratio = holdout_spikes / (train_spikes + 1e-8)\n",
    "    results['spike_ratio'] = spike_ratio\n",
    "    \n",
    "    if spike_ratio > 3:\n",
    "        results['warnings'].append(f\"Spike frequency {spike_ratio:.1f}x higher in holdout\")\n",
    "        results['passed'] = False\n",
    "        results['shift_magnitude'] = max(results['shift_magnitude'], spike_ratio / 2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "DISTRIBUTION_SHIFT_DETECTED = False\n",
    "SHIFT_MAGNITUDE = 0\n",
    "\n",
    "if HAS_HOLDOUT:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DISTRIBUTION SHIFT DETECTION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for horizon in ['1h', '4h']:\n",
    "        target_col = f'target_{horizon}'\n",
    "        train_targets = df_train_val_initial[target_col].dropna()\n",
    "        holdout_targets = df_holdout[target_col].dropna()\n",
    "        \n",
    "        shift_result = detect_distribution_shift(train_targets, holdout_targets, f\"{horizon} target\")\n",
    "        \n",
    "        status = \"\u2713 OK\" if shift_result['passed'] else \"\u26a0\ufe0f SHIFT DETECTED\"\n",
    "        print(f\"\\n{horizon}: {status}\")\n",
    "        print(f\"  Train:   mean={train_targets.mean():.4f}, std={train_targets.std():.4f}\")\n",
    "        print(f\"  Holdout: mean={holdout_targets.mean():.4f}, std={holdout_targets.std():.4f}\")\n",
    "        print(f\"  Mean shift: {shift_result['mean_shift_std']:.2f} std, Var ratio: {shift_result['var_ratio']:.2f}x\")\n",
    "        \n",
    "        if shift_result['warnings']:\n",
    "            for w in shift_result['warnings']:\n",
    "                print(f\"  \u26a0\ufe0f {w}\")\n",
    "            DISTRIBUTION_SHIFT_DETECTED = True\n",
    "            SHIFT_MAGNITUDE = max(SHIFT_MAGNITUDE, shift_result['shift_magnitude'])\n",
    "\n",
    "# === AUTO-ADAPT TO DISTRIBUTION SHIFT (SHORTER WINDOWS) ===\n",
    "if DISTRIBUTION_SHIFT_DETECTED and AUTO_ADAPT_ON_SHIFT:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"AUTO-ADAPTING TO DISTRIBUTION SHIFT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # More aggressive adaptive window based on shift magnitude\n",
    "    if SHIFT_MAGNITUDE > 4:\n",
    "        adaptive_days = SEVERE_SHIFT_WINDOW_DAYS  # 1.5 days (very aggressive)\n",
    "        volatility_level = \"SEVERE\"\n",
    "    elif SHIFT_MAGNITUDE > 2:\n",
    "        adaptive_days = HIGH_SHIFT_WINDOW_DAYS    # 2 days (aggressive)\n",
    "        volatility_level = \"HIGH\"\n",
    "    elif SHIFT_MAGNITUDE > 1:\n",
    "        adaptive_days = MODERATE_SHIFT_WINDOW_DAYS  # 3 days\n",
    "        volatility_level = \"MODERATE\"\n",
    "    else:\n",
    "        adaptive_days = MILD_SHIFT_WINDOW_DAYS    # 5 days\n",
    "        volatility_level = \"MILD\"\n",
    "    \n",
    "    window_samples = int(adaptive_days * 24 * rph)\n",
    "    \n",
    "    if len(df_train_val_initial) > window_samples:\n",
    "        df_train_val = df_train_val_initial.iloc[-window_samples:]\n",
    "        print(f\"\u2713 Auto-enabled rolling window: {adaptive_days} days ({len(df_train_val):,} samples)\")\n",
    "        print(f\"  Volatility: {volatility_level} (shift magnitude: {SHIFT_MAGNITUDE:.2f})\")\n",
    "        print(f\"  Using most recent {adaptive_days} days for training\")\n",
    "        USE_ROLLING_WINDOW = True\n",
    "        ROLLING_WINDOW_DAYS = adaptive_days\n",
    "    else:\n",
    "        df_train_val = df_train_val_initial\n",
    "        print(f\"\u26a0\ufe0f Not enough data for adaptive window, using all training data\")\n",
    "elif USE_ROLLING_WINDOW:\n",
    "    # Manual rolling window\n",
    "    window_samples = int(ROLLING_WINDOW_DAYS * 24 * rph)\n",
    "    if len(df_train_val_initial) > window_samples:\n",
    "        df_train_val = df_train_val_initial.iloc[-window_samples:]\n",
    "        print(f\"\\n\u2713 Rolling window: Using last {ROLLING_WINDOW_DAYS} days ({len(df_train_val):,} samples)\")\n",
    "    else:\n",
    "        df_train_val = df_train_val_initial\n",
    "else:\n",
    "    df_train_val = df_train_val_initial\n",
    "\n",
    "print(f\"\\nFinal training set: {len(df_train_val):,} samples\")\n",
    "\n",
    "# Final safety check\n",
    "for col in df_train_val.select_dtypes(include=[np.float64, np.float32, float]).columns:\n",
    "    df_train_val[col] = df_train_val[col].replace([np.inf, -np.inf], np.nan)\n",
    "    if df_train_val[col].isna().any():\n",
    "        df_train_val[col] = df_train_val[col].fillna(df_train_val[col].median())\n",
    "\n",
    "float_cols = df_train_val.select_dtypes(include=[np.float64, np.float32, float]).columns\n",
    "has_inf = any(np.isinf(df_train_val[col]).any() for col in float_cols)\n",
    "has_nan = any(np.isnan(df_train_val[col]).any() for col in float_cols)\n",
    "assert not has_inf, \"Data still contains inf!\"\n",
    "assert not has_nan, \"Data still contains nan!\"\n",
    "print(\"\u2713 Data validated: no inf/nan values\")\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_1h = df_train_val[numeric_features_1h]\n",
    "X_4h = df_train_val[numeric_features_4h]\n",
    "X_24h = df_train_val[numeric_features_24h]\n",
    "\n",
    "y_1h = df_train_val['target_1h']\n",
    "y_4h = df_train_val['target_4h']\n",
    "y_24h = df_train_val['target_24h']\n",
    "\n",
    "y_dir_1h = df_train_val['direction_class_1h']\n",
    "y_dir_4h = df_train_val['direction_class_4h']\n",
    "\n",
    "current_gas = df_train_val['gas']\n",
    "\n",
    "# === BASELINE MODELS (on both train AND holdout) ===\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BASELINE COMPARISONS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "naive_mae_1h = np.mean(np.abs(y_1h.values - current_gas.values))\n",
    "naive_mae_4h = np.mean(np.abs(y_4h.values - current_gas.values))\n",
    "\n",
    "mean_pred = np.full_like(y_1h.values, y_1h.mean())\n",
    "mean_mae_1h = np.mean(np.abs(y_1h.values - mean_pred))\n",
    "mean_mae_4h = np.mean(np.abs(y_4h.values - mean_pred))\n",
    "\n",
    "print(f\"\\nTRAINING SET Baseline MAEs:\")\n",
    "print(f\"  Naive (current price):     MAE_1h={naive_mae_1h:.6f}, MAE_4h={naive_mae_4h:.6f}\")\n",
    "print(f\"  Mean (historical average): MAE_1h={mean_mae_1h:.6f}, MAE_4h={mean_mae_4h:.6f}\")\n",
    "\n",
    "best_baseline_1h = min(naive_mae_1h, mean_mae_1h)\n",
    "best_baseline_4h = min(naive_mae_4h, mean_mae_4h)\n",
    "\n",
    "BASELINES = {\n",
    "    '1h': {'naive_mae': naive_mae_1h, 'mean_mae': mean_mae_1h, 'best': best_baseline_1h},\n",
    "    '4h': {'naive_mae': naive_mae_4h, 'mean_mae': mean_mae_4h, 'best': best_baseline_4h},\n",
    "}\n",
    "\n",
    "# Holdout baselines (CRITICAL for proper model selection)\n",
    "if HAS_HOLDOUT:\n",
    "    holdout_gas = df_holdout['gas']\n",
    "    holdout_y_1h = df_holdout['target_1h']\n",
    "    holdout_y_4h = df_holdout['target_4h']\n",
    "    \n",
    "    holdout_naive_1h = np.mean(np.abs(holdout_y_1h.values - holdout_gas.values))\n",
    "    holdout_naive_4h = np.mean(np.abs(holdout_y_4h.values - holdout_gas.values))\n",
    "    \n",
    "    holdout_mean_pred = np.full_like(holdout_y_1h.values, y_1h.mean())  # Use training mean\n",
    "    holdout_mean_1h = np.mean(np.abs(holdout_y_1h.values - holdout_mean_pred))\n",
    "    holdout_mean_4h = np.mean(np.abs(holdout_y_4h.values - holdout_mean_pred))\n",
    "    \n",
    "    print(f\"\\nHOLDOUT SET Baseline MAEs:\")\n",
    "    print(f\"  Naive (current price):     MAE_1h={holdout_naive_1h:.6f}, MAE_4h={holdout_naive_4h:.6f}\")\n",
    "    print(f\"  Mean (historical average): MAE_1h={holdout_mean_1h:.6f}, MAE_4h={holdout_mean_4h:.6f}\")\n",
    "    \n",
    "    BASELINES['1h']['holdout_naive_mae'] = holdout_naive_1h\n",
    "    BASELINES['1h']['holdout_mean_mae'] = holdout_mean_1h\n",
    "    BASELINES['1h']['holdout_best'] = min(holdout_naive_1h, holdout_mean_1h)\n",
    "    \n",
    "    BASELINES['4h']['holdout_naive_mae'] = holdout_naive_4h\n",
    "    BASELINES['4h']['holdout_mean_mae'] = holdout_mean_4h\n",
    "    BASELINES['4h']['holdout_best'] = min(holdout_naive_4h, holdout_mean_4h)\n",
    "\n",
    "# === ADAPTIVE CONFIGURATION BASED ON VOLATILITY ===\n",
    "# These will be used in Cell 7 for model training\n",
    "ADAPTIVE_CONFIG = {\n",
    "    'shift_detected': DISTRIBUTION_SHIFT_DETECTED,\n",
    "    'shift_magnitude': SHIFT_MAGNITUDE,\n",
    "    'volatility_level': volatility_level if DISTRIBUTION_SHIFT_DETECTED else 'NORMAL'\n",
    "}\n",
    "\n",
    "if DISTRIBUTION_SHIFT_DETECTED:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ADAPTIVE CONFIGURATION FOR VOLATILE PERIOD\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Volatility level: {ADAPTIVE_CONFIG['volatility_level']}\")\n",
    "    print(f\"  Shift magnitude: {SHIFT_MAGNITUDE:.2f}\")\n",
    "    print(f\"  Rolling window: {ROLLING_WINDOW_DAYS} days\")\n",
    "    print(\"  \u2192 Model training will use relaxed requirements and increased regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training - WITH ADAPTIVE REGULARIZATION & RELAXED BASELINES FOR VOLATILE PERIODS",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor",
    "from sklearn.linear_model import Ridge, HuberRegressor, ElasticNet",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score",
    "from sklearn.inspection import permutation_importance",
    "import joblib",
    "import warnings",
    "warnings.filterwarnings('ignore')",
    "",
    "# === BASE CONFIGURATION ===",
    "TRAIN_REGIME_MODELS = True",
    "COMPUTE_PERMUTATION_IMPORTANCE = True",
    "ENABLE_FEATURE_PRUNING = True",
    "FEATURE_PRUNING_THRESHOLD = 0.005  # Remove features with < 0.5% importance (was 1%)",
    "MIN_FEATURES_TO_KEEP = 12  # NEW: Never prune below this many features for 1h",
    "",
    "# Ensemble settings",
    "USE_ENSEMBLE_BLENDING = True",
    "ENSEMBLE_ASYM_WEIGHT = 0.6",
    "ASYMMETRIC_ALPHA = 0.6",
    "",
    "# === ADAPTIVE CONFIGURATION BASED ON VOLATILITY ===",
    "# Automatically adjust based on distribution shift detected in Cell 6",
    "if 'ADAPTIVE_CONFIG' in dir() and ADAPTIVE_CONFIG.get('shift_detected', False):",
    "    volatility = ADAPTIVE_CONFIG['volatility_level']",
    "    shift_mag = ADAPTIVE_CONFIG['shift_magnitude']",
    "    ",
    "    print(f\"\\n{'='*60}\")",
    "    print(f\"ADAPTIVE MODE: {volatility} VOLATILITY\")",
    "    print(f\"{'='*60}\")",
    "    ",
    "    if volatility == 'SEVERE':",
    "        # SEVERE: Very relaxed requirements, maximum regularization",
    "        MINIMUM_IMPROVEMENT = -0.10        # Accept 10% WORSE than baseline",
    "        HOLDOUT_DEGRADATION_LIMIT = 0.80   # Allow 80% degradation",
    "        REGULARIZATION_STRENGTH = 0.5      # Strong regularization",
    "        MIN_SAMPLES_LEAF_MULT = 3.0        # Much larger leaves",
    "        print(\"  \u2192 Accepting models up to 10% worse than baseline\")",
    "        print(\"  \u2192 Strong regularization (0.5), large leaf sizes (3x)\")",
    "        ",
    "    elif volatility == 'HIGH':",
    "        # HIGH: Relaxed requirements, high regularization",
    "        MINIMUM_IMPROVEMENT = -0.05        # Accept 5% WORSE than baseline",
    "        HOLDOUT_DEGRADATION_LIMIT = 0.60   # Allow 60% degradation",
    "        REGULARIZATION_STRENGTH = 0.3      # Higher regularization",
    "        MIN_SAMPLES_LEAF_MULT = 2.5        # Larger leaves",
    "        print(\"  \u2192 Accepting models up to 5% worse than baseline\")",
    "        print(\"  \u2192 High regularization (0.3), larger leaf sizes (2.5x)\")",
    "        ",
    "    elif volatility == 'MODERATE':",
    "        # MODERATE: Slightly relaxed",
    "        MINIMUM_IMPROVEMENT = 0.0          # Just match baseline",
    "        HOLDOUT_DEGRADATION_LIMIT = 0.50   # Allow 50% degradation",
    "        REGULARIZATION_STRENGTH = 0.2      # Moderate regularization",
    "        MIN_SAMPLES_LEAF_MULT = 2.0        # Larger leaves",
    "        print(\"  \u2192 Accepting models that match baseline (0% improvement)\")",
    "        print(\"  \u2192 Moderate regularization (0.2), leaf sizes (2x)\")",
    "        ",
    "    else:  # MILD",
    "        MINIMUM_IMPROVEMENT = 0.02         # 2% improvement required",
    "        HOLDOUT_DEGRADATION_LIMIT = 0.40   # Allow 40% degradation",
    "        REGULARIZATION_STRENGTH = 0.15     # Light regularization",
    "        MIN_SAMPLES_LEAF_MULT = 1.75       # Slightly larger leaves",
    "        print(\"  \u2192 Requiring 2% improvement over baseline\")",
    "        print(\"  \u2192 Light regularization (0.15), leaf sizes (1.75x)\")",
    "else:",
    "    # NORMAL: Standard requirements",
    "    MINIMUM_IMPROVEMENT = 0.05",
    "    HOLDOUT_DEGRADATION_LIMIT = 0.30",
    "    REGULARIZATION_STRENGTH = 0.1",
    "    MIN_SAMPLES_LEAF_MULT = 1.5",
    "    print(\"\\n[Standard mode - normal volatility requirements]\")",
    "",
    "# === MODULE-LEVEL CLASS FOR PICKLING ===",
    "class EnsembleModel:",
    "    \"\"\"Blended ensemble of symmetric and asymmetric models - defined at module level for pickling\"\"\"",
    "    def __init__(self, sym_model, asym_model, sym_scaler, asym_scaler, sym_weight, asym_weight):",
    "        self.sym_model = sym_model",
    "        self.asym_model = asym_model",
    "        self.sym_scaler = sym_scaler",
    "        self.asym_scaler = asym_scaler",
    "        self.sym_weight = sym_weight",
    "        self.asym_weight = asym_weight",
    "    ",
    "    def predict(self, X):",
    "        sym_pred = self.sym_model.predict(X)",
    "        asym_pred = self.asym_model.predict(X)",
    "        return self.sym_weight * sym_pred + self.asym_weight * asym_pred",
    "    ",
    "    def get_params(self, deep=True):",
    "        return {'sym_weight': self.sym_weight, 'asym_weight': self.asym_weight}",
    "",
    "def check_baseline_gate(model_mae, baseline_mae, model_name):",
    "    \"\"\"Check if model beats baseline by minimum threshold (ADAPTIVE)\"\"\"",
    "    improvement = (baseline_mae - model_mae) / baseline_mae",
    "    passed = improvement >= MINIMUM_IMPROVEMENT",
    "    ",
    "    if passed:",
    "        if MINIMUM_IMPROVEMENT < 0:",
    "            print(f\"  \u2713 PASSED (relaxed): {improvement*100:.1f}% vs baseline (threshold: {MINIMUM_IMPROVEMENT*100:.0f}%)\")",
    "        else:",
    "            print(f\"  \u2713 PASSED baseline gate: {improvement*100:.1f}% improvement\")",
    "    else:",
    "        print(f\"  \u2717 FAILED baseline gate: {improvement*100:.1f}% (need {MINIMUM_IMPROVEMENT*100:.0f}%+)\")",
    "    return passed, improvement",
    "",
    "def check_holdout_gate(cv_mae, holdout_mae, model_name, holdout_baseline=None):",
    "    \"\"\"Check if holdout performance is acceptable (ADAPTIVE)\"\"\"",
    "    if cv_mae <= 0:",
    "        return False, 0",
    "    degradation = (holdout_mae - cv_mae) / cv_mae",
    "    ",
    "    if holdout_baseline is not None:",
    "        holdout_improvement = (holdout_baseline - holdout_mae) / holdout_baseline",
    "        # During volatile periods, accept worse performance",
    "        threshold = MINIMUM_IMPROVEMENT",
    "        if holdout_improvement >= threshold:",
    "            print(f\"  \u2713 Beats holdout baseline by {holdout_improvement*100:.1f}% (threshold: {threshold*100:.0f}%)\")",
    "            return True, degradation",
    "    ",
    "    passed = degradation < HOLDOUT_DEGRADATION_LIMIT",
    "    if passed:",
    "        print(f\"  \u2713 PASSED holdout gate: {degradation*100:+.1f}% degradation (limit: {HOLDOUT_DEGRADATION_LIMIT*100:.0f}%)\")",
    "    else:",
    "        print(f\"  \u2717 FAILED holdout gate: {degradation*100:+.1f}% degradation (limit: {HOLDOUT_DEGRADATION_LIMIT*100:.0f}%)\")",
    "    return passed, degradation",
    "",
    "def asymmetric_loss(y_true, y_pred, alpha=0.6):",
    "    \"\"\"Pinball loss - alpha > 0.5 penalizes under-prediction more\"\"\"",
    "    errors = y_true - y_pred",
    "    loss = np.where(errors >= 0, alpha * np.abs(errors), (1 - alpha) * np.abs(errors))",
    "    return np.mean(loss)",
    "",
    "def walk_forward_validate(model_class, model_params, X, y, baseline_mae, n_splits=5, purge_gap=120):",
    "    \"\"\"Walk-forward validation with purge gap\"\"\"",
    "    n = len(X)",
    "    fold_size = n // (n_splits + 1)",
    "    fold_results = []",
    "    ",
    "    for fold in range(n_splits):",
    "        train_end = fold_size * (fold + 1)",
    "        test_start = train_end + purge_gap",
    "        test_end = test_start + fold_size",
    "        ",
    "        if test_end > n:",
    "            break",
    "            ",
    "        X_train = X.iloc[:train_end]",
    "        X_test = X.iloc[test_start:test_end]",
    "        y_train = y.iloc[:train_end]",
    "        y_test = y.iloc[test_start:test_end]",
    "        ",
    "        scaler = RobustScaler()",
    "        X_train_scaled = scaler.fit_transform(X_train)",
    "        X_test_scaled = scaler.transform(X_test)",
    "        ",
    "        model = model_class(**model_params)",
    "        model.fit(X_train_scaled, y_train)",
    "        ",
    "        y_pred = model.predict(X_test_scaled)",
    "        mae = mean_absolute_error(y_test, y_pred)",
    "        fold_results.append(mae)",
    "    ",
    "    if not fold_results:",
    "        return None",
    "        ",
    "    return {",
    "        'avg_mae': np.mean(fold_results),",
    "        'std_mae': np.std(fold_results),",
    "        'improvement': (baseline_mae - np.mean(fold_results)) / baseline_mae",
    "    }",
    "",
    "def get_models_to_try():",
    "    \"\"\"Get list of models with ADAPTIVE REGULARIZATION\"\"\"",
    "    # Scale regularization based on volatility",
    "    base_min_samples = int(30 * MIN_SAMPLES_LEAF_MULT)",
    "    ridge_alpha = 10.0 * (1 + REGULARIZATION_STRENGTH * 5)",
    "    huber_alpha = 0.5 * (1 + REGULARIZATION_STRENGTH * 3)",
    "    elastic_alpha = 0.1 * (1 + REGULARIZATION_STRENGTH * 3)",
    "    gbm_lr = max(0.03, 0.1 - REGULARIZATION_STRENGTH * 0.15)  # Slower learning with more regularization",
    "    ",
    "    models = [",
    "        ('Ridge', Ridge, {'alpha': ridge_alpha, 'random_state': 42}),",
    "        ('Huber', HuberRegressor, {'epsilon': 1.35, 'alpha': huber_alpha, 'max_iter': 1000}),",
    "        ('ElasticNet', ElasticNet, {'alpha': elastic_alpha, 'l1_ratio': 0.5, 'random_state': 42, 'max_iter': 2000}),",
    "        ('RF', RandomForestRegressor, {",
    "            'n_estimators': 100,  # More trees for stability",
    "            'max_depth': 4,       # Shallower trees",
    "            'min_samples_leaf': base_min_samples,",
    "            'max_features': 0.5,  # Use fewer features per tree for more robustness",
    "            'random_state': 42, ",
    "            'n_jobs': -1",
    "        }),",
    "        ('GBM', GradientBoostingRegressor, {",
    "            'n_estimators': 80, ",
    "            'max_depth': 3,      # Shallower",
    "            'learning_rate': gbm_lr,",
    "            'min_samples_leaf': base_min_samples,",
    "            'subsample': 0.7,    # More subsampling for regularization",
    "            'random_state': 42",
    "        }),",
    "        ('GBM-Asym', GradientBoostingRegressor, {",
    "            'loss': 'quantile', ",
    "            'alpha': ASYMMETRIC_ALPHA,",
    "            'n_estimators': 80, ",
    "            'max_depth': 3,",
    "            'learning_rate': gbm_lr,",
    "            'min_samples_leaf': base_min_samples,",
    "            'subsample': 0.7,",
    "            'random_state': 42",
    "        }),",
    "    ]",
    "    ",
    "    return models",
    "",
    "def compute_permutation_importance(model, X, y, scaler, feature_names, n_repeats=5):",
    "    \"\"\"Compute permutation importance for any model type\"\"\"",
    "    X_scaled = scaler.transform(X)",
    "    ",
    "    result = permutation_importance(",
    "        model, X_scaled, y,",
    "        n_repeats=n_repeats,",
    "        random_state=42,",
    "        scoring='neg_mean_absolute_error',",
    "        n_jobs=-1",
    "    )",
    "    ",
    "    importance_dict = {}",
    "    for i, feat in enumerate(feature_names):",
    "        importance_dict[feat] = -result.importances_mean[i]",
    "    ",
    "    total = sum(importance_dict.values())",
    "    if total > 0:",
    "        importance_dict = {k: v/total for k, v in importance_dict.items()}",
    "    ",
    "    return importance_dict",
    "",
    "def prune_features_by_importance(X_train, X_holdout, feature_names, importance_dict, threshold=0.0):",
    "    \"\"\"Remove features with importance below threshold\"\"\"",
    "    features_to_keep = [f for f in feature_names if importance_dict.get(f, 0) >= threshold]",
    "    features_to_remove = [f for f in feature_names if f not in features_to_keep]",
    "    ",
    "    if not features_to_remove:",
    "        return X_train, X_holdout, feature_names, []",
    "    ",
    "    print(f\"  Feature pruning: removing {len(features_to_remove)} features with importance < {threshold}\")",
    "    for f in features_to_remove[:5]:",
    "        print(f\"    - {f}: {importance_dict.get(f, 0):.4f}\")",
    "    if len(features_to_remove) > 5:",
    "        print(f\"    ... and {len(features_to_remove) - 5} more\")",
    "    ",
    "    X_train_pruned = X_train[features_to_keep]",
    "    X_holdout_pruned = X_holdout[features_to_keep]",
    "    ",
    "    return X_train_pruned, X_holdout_pruned, features_to_keep, features_to_remove",
    "",
    "def create_ensemble_model(sym_model, asym_model, sym_scaler, asym_scaler, sym_weight=0.4, asym_weight=0.6):",
    "    \"\"\"Create blended ensemble of symmetric and asymmetric models\"\"\"",
    "    return EnsembleModel(sym_model, asym_model, sym_scaler, asym_scaler, sym_weight, asym_weight)",
    "",
    "def train_model_with_holdout(X_train, y_train, X_holdout, y_holdout, baseline_mae, ",
    "                             horizon_name, feature_names, holdout_baseline=None):",
    "    \"\"\"Train model with ADAPTIVE requirements\"\"\"",
    "    print(f\"\\n{'='*60}\")",
    "    print(f\"Training {horizon_name} model\")",
    "    print(f\"{'='*60}\")",
    "    print(f\"Train: {len(X_train):,}, Holdout: {len(X_holdout):,}, Features: {X_train.shape[1]}\")",
    "    print(f\"Train baseline: {baseline_mae:.6f}\", end=\"\")",
    "    if holdout_baseline:",
    "        print(f\", Holdout baseline: {holdout_baseline:.6f}\")",
    "    else:",
    "        print()",
    "    ",
    "    # Show adaptive settings",
    "    print(f\"Adaptive settings: min_improvement={MINIMUM_IMPROVEMENT*100:.0f}%, \"",
    "          f\"reg_strength={REGULARIZATION_STRENGTH:.2f}, leaf_mult={MIN_SAMPLES_LEAF_MULT:.1f}x\")",
    "    ",
    "    models_to_try = get_models_to_try()",
    "    results = []",
    "    sym_results = []",
    "    asym_results = []",
    "    ",
    "    for name, model_class, params in models_to_try:",
    "        print(f\"\\n[{name}]\")",
    "        try:",
    "            wf_result = walk_forward_validate(model_class, params, X_train, y_train, baseline_mae, n_splits=4, purge_gap=120)",
    "            if not wf_result:",
    "                continue",
    "                ",
    "            cv_mae = wf_result['avg_mae']",
    "            print(f\"  CV MAE: {cv_mae:.6f} \u00b1 {wf_result['std_mae']:.6f}\")",
    "            ",
    "            scaler = RobustScaler()",
    "            X_train_scaled = scaler.fit_transform(X_train)",
    "            X_holdout_scaled = scaler.transform(X_holdout)",
    "            ",
    "            model = model_class(**params)",
    "            model.fit(X_train_scaled, y_train)",
    "            ",
    "            y_holdout_pred = model.predict(X_holdout_scaled)",
    "            holdout_mae = mean_absolute_error(y_holdout, y_holdout_pred)",
    "            holdout_improvement = (baseline_mae - holdout_mae) / baseline_mae",
    "            ",
    "            asym_loss = asymmetric_loss(y_holdout.values, y_holdout_pred, ASYMMETRIC_ALPHA)",
    "            ",
    "            if holdout_baseline:",
    "                vs_holdout = (holdout_baseline - holdout_mae) / holdout_baseline",
    "                print(f\"  HOLDOUT MAE: {holdout_mae:.6f} ({vs_holdout*100:+.1f}% vs holdout baseline)\")",
    "            else:",
    "                print(f\"  HOLDOUT MAE: {holdout_mae:.6f} ({holdout_improvement*100:+.1f}% vs train baseline)\")",
    "            ",
    "            use_baseline = holdout_baseline if holdout_baseline else baseline_mae",
    "            passed_baseline, _ = check_baseline_gate(holdout_mae, use_baseline, name)",
    "            passed_holdout, degradation = check_holdout_gate(cv_mae, holdout_mae, name, holdout_baseline)",
    "            ",
    "            result_entry = {",
    "                'name': name, 'model_class': model_class, 'params': params,",
    "                'cv_mae': cv_mae, 'holdout_mae': holdout_mae,",
    "                'asymmetric_loss': asym_loss,",
    "                'holdout_improvement': holdout_improvement,",
    "                'vs_holdout_baseline': (holdout_baseline - holdout_mae) / holdout_baseline if holdout_baseline else None,",
    "                'model': model, 'scaler': scaler",
    "            }",
    "            ",
    "            # Relaxed acceptance during volatile periods",
    "            if passed_baseline or passed_holdout:",
    "                results.append(result_entry)",
    "                if 'Asym' in name:",
    "                    asym_results.append(result_entry)",
    "                else:",
    "                    sym_results.append(result_entry)",
    "                print(f\"  \u2192 Accepted\")",
    "            else:",
    "                print(f\"  \u2192 Rejected\")",
    "                ",
    "        except Exception as e:",
    "            print(f\"  Failed: {e}\")",
    "    ",
    "    if not results:",
    "        print(\"\\n\u26a0\ufe0f All models failed! Using Huber fallback...\")",
    "        scaler = RobustScaler()",
    "        X_train_scaled = scaler.fit_transform(X_train)",
    "        model = HuberRegressor(epsilon=1.35, alpha=0.1 * (1 + REGULARIZATION_STRENGTH * 3), max_iter=1000)",
    "        model.fit(X_train_scaled, y_train)",
    "        ",
    "        y_holdout_pred = model.predict(scaler.transform(X_holdout))",
    "        holdout_mae = mean_absolute_error(y_holdout, y_holdout_pred)",
    "        ",
    "        importance = {}",
    "        if COMPUTE_PERMUTATION_IMPORTANCE:",
    "            importance = compute_permutation_importance(model, X_holdout, y_holdout, scaler, feature_names)",
    "        ",
    "        return model, scaler, {",
    "            'name': 'Huber (fallback)',",
    "            'mae': holdout_mae,",
    "            'improvement': (baseline_mae - holdout_mae) / baseline_mae,",
    "            'vs_holdout_baseline': (holdout_baseline - holdout_mae) / holdout_baseline if holdout_baseline else None,",
    "            'passed_baseline': False,",
    "            'is_fallback': True,",
    "            'is_ensemble': False",
    "        }, importance, list(feature_names)",
    "    ",
    "    # === ENSEMBLE BLENDING ===",
    "    best_single = min(results, key=lambda x: x['holdout_mae'])",
    "    best_model = best_single['model']",
    "    best_scaler = best_single['scaler']",
    "    best_metrics = {",
    "        'name': best_single['name'],",
    "        'mae': best_single['holdout_mae'],",
    "        'cv_mae': best_single['cv_mae'],",
    "        'asymmetric_loss': best_single.get('asymmetric_loss'),",
    "        'improvement': best_single['holdout_improvement'],",
    "        'vs_holdout_baseline': best_single['vs_holdout_baseline'],",
    "        'passed_baseline': True,",
    "        'is_fallback': False,",
    "        'is_ensemble': False",
    "    }",
    "    ",
    "    # Try ensemble if we have both symmetric and asymmetric models",
    "    if USE_ENSEMBLE_BLENDING and sym_results and asym_results:",
    "        print(f\"\\n>>> Trying ensemble blend...\")",
    "        best_sym = min(sym_results, key=lambda x: x['holdout_mae'])",
    "        best_asym = min(asym_results, key=lambda x: x['holdout_mae'])",
    "        ",
    "        X_holdout_scaled = best_sym['scaler'].transform(X_holdout)",
    "        sym_pred = best_sym['model'].predict(X_holdout_scaled)",
    "        asym_pred = best_asym['model'].predict(X_holdout_scaled)",
    "        ",
    "        best_blend_mae = float('inf')",
    "        best_blend_weights = (0.4, 0.6)",
    "        ",
    "        for asym_w in [0.5, 0.55, 0.6, 0.65, 0.7]:",
    "            sym_w = 1 - asym_w",
    "            blend_pred = sym_w * sym_pred + asym_w * asym_pred",
    "            blend_mae = mean_absolute_error(y_holdout, blend_pred)",
    "            if blend_mae < best_blend_mae:",
    "                best_blend_mae = blend_mae",
    "                best_blend_weights = (sym_w, asym_w)",
    "        ",
    "        print(f\"  Best single ({best_single['name']}): MAE={best_single['holdout_mae']:.6f}\")",
    "        print(f\"  Best ensemble ({best_sym['name']}+{best_asym['name']}): MAE={best_blend_mae:.6f}\")",
    "        print(f\"  Blend weights: {best_blend_weights[0]:.0%} sym + {best_blend_weights[1]:.0%} asym\")",
    "        ",
    "        if best_blend_mae < best_single['holdout_mae']:",
    "            print(f\"  \u2713 Using ensemble (improves by {(best_single['holdout_mae'] - best_blend_mae) / best_single['holdout_mae'] * 100:.1f}%)\")",
    "            ",
    "            ensemble = create_ensemble_model(",
    "                best_sym['model'], best_asym['model'],",
    "                best_sym['scaler'], best_asym['scaler'],",
    "                best_blend_weights[0], best_blend_weights[1]",
    "            )",
    "            ",
    "            best_model = ensemble",
    "            best_scaler = best_sym['scaler']",
    "            best_metrics = {",
    "                'name': f\"Ensemble({best_sym['name']}+{best_asym['name']})\",",
    "                'mae': best_blend_mae,",
    "                'cv_mae': (best_sym['cv_mae'] + best_asym['cv_mae']) / 2,",
    "                'improvement': (baseline_mae - best_blend_mae) / baseline_mae,",
    "                'vs_holdout_baseline': (holdout_baseline - best_blend_mae) / holdout_baseline if holdout_baseline else None,",
    "                'passed_baseline': True,",
    "                'is_fallback': False,",
    "                'is_ensemble': True,",
    "                'ensemble_weights': best_blend_weights,",
    "                'sym_model': best_sym['name'],",
    "                'asym_model': best_asym['name']",
    "            }",
    "        else:",
    "            print(f\"  \u2717 Single model is better, not using ensemble\")",
    "    ",
    "    print(f\"\\n>>> Best: {best_metrics['name']} (Holdout MAE: {best_metrics['mae']:.6f})\")",
    "    ",
    "    # Compute permutation importance",
    "    importance = {}",
    "    if COMPUTE_PERMUTATION_IMPORTANCE:",
    "        print(\"  Computing permutation importance...\")",
    "        if best_metrics.get('is_ensemble'):",
    "            importance = compute_permutation_importance(",
    "                best_single['model'], X_holdout, y_holdout, best_scaler, list(feature_names)",
    "            )",
    "        else:",
    "            importance = compute_permutation_importance(",
    "                best_model, X_holdout, y_holdout, best_scaler, list(feature_names)",
    "            )",
    "        top_3 = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:3]",
    "        print(f\"  Top features: {', '.join([f'{f[0]}({f[1]:.2f})' for f in top_3])}\")",
    "        ",
    "        # Feature pruning (only if not ensemble)",
    "        if ENABLE_FEATURE_PRUNING and not best_metrics.get('is_ensemble'):",
    "            negative_features = [f for f, v in importance.items() if v < FEATURE_PRUNING_THRESHOLD]",
    "            if negative_features and len(feature_names) - len(negative_features) >= 5:",
    "                X_train_pruned, X_holdout_pruned, pruned_features, removed = prune_features_by_importance(",
    "                    X_train, X_holdout, list(feature_names), importance, FEATURE_PRUNING_THRESHOLD",
    "                )",
    "                ",
    "                if len(pruned_features) >= 5:",
    "                    print(f\"  Re-training with {len(pruned_features)} features...\")",
    "                    ",
    "                    scaler_pruned = RobustScaler()",
    "                    X_train_pruned_scaled = scaler_pruned.fit_transform(X_train_pruned)",
    "                    X_holdout_pruned_scaled = scaler_pruned.transform(X_holdout_pruned)",
    "                    ",
    "                    model_pruned = best_single['model_class'](**best_single['params'])",
    "                    model_pruned.fit(X_train_pruned_scaled, y_train)",
    "                    ",
    "                    y_holdout_pred_pruned = model_pruned.predict(X_holdout_pruned_scaled)",
    "                    holdout_mae_pruned = mean_absolute_error(y_holdout, y_holdout_pred_pruned)",
    "                    ",
    "                    print(f\"  Pruned model MAE: {holdout_mae_pruned:.6f} (original: {best_metrics['mae']:.6f})\")",
    "                    ",
    "                    if holdout_mae_pruned <= best_metrics['mae'] * 1.02:",
    "                        print(f\"  \u2713 Using pruned model ({len(pruned_features)} features)\")",
    "                        best_model = model_pruned",
    "                        best_scaler = scaler_pruned",
    "                        best_metrics['mae'] = holdout_mae_pruned",
    "                        best_metrics['n_features_pruned'] = len(removed)",
    "                        feature_names = pruned_features",
    "                        importance = {f: importance[f] for f in pruned_features}",
    "    ",
    "    return best_model, best_scaler, best_metrics, importance, list(feature_names)",
    "",
    "def train_regime_models(X_train, y_train, X_holdout, y_holdout, regime_train, regime_holdout,",
    "                        baseline_mae, horizon_name, feature_names, holdout_baseline=None):",
    "    \"\"\"Train separate models for each regime\"\"\"",
    "    print(f\"\\n{'='*60}\")",
    "    print(f\"Training REGIME-SPECIFIC {horizon_name} models\")",
    "    print(f\"{'='*60}\")",
    "    ",
    "    regime_models = {}",
    "    ",
    "    for regime_val, regime_name in [(0, 'normal'), (1, 'elevated'), (2, 'spike')]:",
    "        train_mask = regime_train == regime_val",
    "        holdout_mask = regime_holdout == regime_val",
    "        ",
    "        n_train = train_mask.sum()",
    "        n_holdout = holdout_mask.sum()",
    "        ",
    "        print(f\"\\n[{regime_name.upper()}] Train: {n_train}, Holdout: {n_holdout}\")",
    "        ",
    "        if n_train < 500 or n_holdout < 100:",
    "            print(f\"  Insufficient data, skipping\")",
    "            continue",
    "        ",
    "        X_r_train = X_train[train_mask]",
    "        y_r_train = y_train[train_mask]",
    "        X_r_holdout = X_holdout[holdout_mask]",
    "        y_r_holdout = y_holdout[holdout_mask]",
    "        ",
    "        model, scaler, metrics, importance, final_features = train_model_with_holdout(",
    "            X_r_train, y_r_train, X_r_holdout, y_r_holdout,",
    "            baseline_mae, f\"{horizon_name}_{regime_name}\", feature_names, holdout_baseline",
    "        )",
    "        ",
    "        if model:",
    "            regime_models[regime_val] = {",
    "                'model': model, 'scaler': scaler, 'metrics': metrics,",
    "                'regime_name': regime_name, 'n_samples': n_train,",
    "                'features': final_features",
    "            }",
    "    ",
    "    return regime_models",
    "",
    "def print_distribution_diagnostics(y_train, y_holdout, name=\"\"):",
    "    \"\"\"Print diagnostics\"\"\"",
    "    print(f\"\\n[Distribution - {name}]\")",
    "    print(f\"  Train:   mean={y_train.mean():.4f}, std={y_train.std():.4f}\")",
    "    print(f\"  Holdout: mean={y_holdout.mean():.4f}, std={y_holdout.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models with ENSEMBLE REGIME SWITCHING\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING ALL PREDICTION MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trained_models = {}\n",
    "regime_specific_models = {}\n",
    "all_feature_importance = {}\n",
    "pruned_features_log = {}  # NEW: Track which features were pruned\n",
    "\n",
    "if not HAS_HOLDOUT or df_holdout is None or len(df_holdout) < 1000:\n",
    "    print(\"\\n\u26a0\ufe0f WARNING: Limited holdout data\")\n",
    "\n",
    "print(f\"\\nTraining set: {len(df_train_val):,} samples\")\n",
    "print(f\"Holdout set:  {len(df_holdout) if df_holdout is not None else 0:,} samples\")\n",
    "if DISTRIBUTION_SHIFT_DETECTED:\n",
    "    print(f\"\u26a0\ufe0f Distribution shift detected (magnitude: {SHIFT_MAGNITUDE:.2f})\")\n",
    "    if USE_ROLLING_WINDOW:\n",
    "        print(f\"   Auto-adapted to {ROLLING_WINDOW_DAYS}-day rolling window\")\n",
    "\n",
    "# === CREATE REGIME LABELS ===\n",
    "print(\"\\nCreating regime labels...\")\n",
    "regime_train = pd.Series(0, index=df_train_val.index)\n",
    "if 'gas_zscore_1h' in df_train_val.columns:\n",
    "    regime_train[df_train_val['gas_zscore_1h'] > 1] = 1\n",
    "if 'is_spike' in df_train_val.columns:\n",
    "    regime_train[df_train_val['is_spike'] == 1] = 2\n",
    "\n",
    "regime_holdout = None\n",
    "if HAS_HOLDOUT:\n",
    "    regime_holdout = pd.Series(0, index=df_holdout.index)\n",
    "    if 'gas_zscore_1h' in df_holdout.columns:\n",
    "        regime_holdout[df_holdout['gas_zscore_1h'] > 1] = 1\n",
    "    if 'is_spike' in df_holdout.columns:\n",
    "        regime_holdout[df_holdout['is_spike'] == 1] = 2\n",
    "\n",
    "print(f\"Regime distribution (train): {dict(regime_train.value_counts().sort_index())}\")\n",
    "if regime_holdout is not None:\n",
    "    print(f\"Regime distribution (holdout): {dict(regime_holdout.value_counts().sort_index())}\")\n",
    "\n",
    "# === ENSEMBLE PREDICTION FUNCTION ===\n",
    "def create_ensemble_predictor(global_model, global_scaler, regime_models, features):\n",
    "    \"\"\"Create a predictor that uses regime-specific models when available.\n",
    "    Handles feature mismatches when regime models use different (pruned) features.\n",
    "    \"\"\"\n",
    "    def predict(X, current_regime=None):\n",
    "        # Handle both DataFrame and array inputs\n",
    "        if hasattr(X, 'columns'):\n",
    "            # DataFrame input - select features by name\n",
    "            X_global = X[features] if all(f in X.columns for f in features) else X\n",
    "            X_scaled = global_scaler.transform(X_global)\n",
    "        else:\n",
    "            # Array input - assume correct feature order\n",
    "            X_scaled = global_scaler.transform(X)\n",
    "        \n",
    "        global_pred = global_model.predict(X_scaled)\n",
    "        \n",
    "        if current_regime is not None and regime_models and current_regime in regime_models:\n",
    "            regime_data = regime_models[current_regime]\n",
    "            regime_features = regime_data.get('features', features)\n",
    "            \n",
    "            if hasattr(X, 'columns'):\n",
    "                # DataFrame - select regime-specific features\n",
    "                available_features = [f for f in regime_features if f in X.columns]\n",
    "                if len(available_features) == len(regime_features):\n",
    "                    X_regime = X[regime_features]\n",
    "                    X_regime_scaled = regime_data['scaler'].transform(X_regime)\n",
    "                    regime_pred = regime_data['model'].predict(X_regime_scaled)\n",
    "                    # Weighted average: 70% regime, 30% global\n",
    "                    return 0.7 * regime_pred + 0.3 * global_pred\n",
    "            else:\n",
    "                # Array input - only use if feature counts match\n",
    "                if X.shape[1] == len(regime_features):\n",
    "                    X_regime_scaled = regime_data['scaler'].transform(X)\n",
    "                    regime_pred = regime_data['model'].predict(X_regime_scaled)\n",
    "                    return 0.7 * regime_pred + 0.3 * global_pred\n",
    "        \n",
    "        return global_pred\n",
    "    \n",
    "    return predict\n",
    "\n",
    "# === 1H MODEL ===\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"1-HOUR MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_holdout_1h = df_holdout[numeric_features_1h] if HAS_HOLDOUT else X_1h.iloc[-1000:]\n",
    "y_holdout_1h = df_holdout['target_1h'] if HAS_HOLDOUT else y_1h.iloc[-1000:]\n",
    "mask_1h = y_holdout_1h.notna()\n",
    "X_holdout_1h = X_holdout_1h[mask_1h]\n",
    "y_holdout_1h = y_holdout_1h[mask_1h]\n",
    "\n",
    "print_distribution_diagnostics(y_1h, y_holdout_1h, \"1h targets\")\n",
    "\n",
    "holdout_baseline_1h = BASELINES['1h'].get('holdout_best', None)\n",
    "\n",
    "# Updated: now returns 5 values (model, scaler, metrics, importance, final_features)\n",
    "model_1h, scaler_1h, metrics_1h, importance_1h, features_1h = train_model_with_holdout(\n",
    "    X_1h, y_1h, X_holdout_1h, y_holdout_1h,\n",
    "    BASELINES['1h']['best'], '1h', list(numeric_features_1h), holdout_baseline_1h\n",
    ")\n",
    "if model_1h:\n",
    "    trained_models['1h'] = {\n",
    "        'model': model_1h, 'scaler': scaler_1h, \n",
    "        'metrics': metrics_1h, 'features': features_1h  # Use possibly-pruned features\n",
    "    }\n",
    "    if importance_1h:\n",
    "        all_feature_importance['1h'] = importance_1h\n",
    "    \n",
    "    # Log if features were pruned\n",
    "    if len(features_1h) < len(numeric_features_1h):\n",
    "        pruned_features_log['1h'] = {\n",
    "            'original_count': len(numeric_features_1h),\n",
    "            'pruned_count': len(features_1h),\n",
    "            'removed': list(set(numeric_features_1h) - set(features_1h))\n",
    "        }\n",
    "        print(f\"  \u2713 Features pruned: {len(numeric_features_1h)} \u2192 {len(features_1h)}\")\n",
    "\n",
    "# Train regime-specific models\n",
    "if TRAIN_REGIME_MODELS and regime_holdout is not None:\n",
    "    regime_holdout_1h = regime_holdout[mask_1h]\n",
    "    regime_models_1h = train_regime_models(\n",
    "        X_1h, y_1h, X_holdout_1h, y_holdout_1h,\n",
    "        regime_train, regime_holdout_1h,\n",
    "        BASELINES['1h']['best'], '1h', list(numeric_features_1h), holdout_baseline_1h\n",
    "    )\n",
    "    if regime_models_1h:\n",
    "        regime_specific_models['1h'] = regime_models_1h\n",
    "        # Create ensemble predictor with the actual features used\n",
    "        trained_models['1h']['ensemble_predict'] = create_ensemble_predictor(\n",
    "            model_1h, scaler_1h, regime_models_1h, features_1h\n",
    "        )\n",
    "\n",
    "# === 4H MODEL ===\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"4-HOUR MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_holdout_4h = df_holdout[numeric_features_4h] if HAS_HOLDOUT else X_4h.iloc[-1000:]\n",
    "y_holdout_4h = df_holdout['target_4h'] if HAS_HOLDOUT else y_4h.iloc[-1000:]\n",
    "mask_4h = y_holdout_4h.notna()\n",
    "X_holdout_4h = X_holdout_4h[mask_4h]\n",
    "y_holdout_4h = y_holdout_4h[mask_4h]\n",
    "\n",
    "print_distribution_diagnostics(y_4h, y_holdout_4h, \"4h targets\")\n",
    "\n",
    "holdout_baseline_4h = BASELINES['4h'].get('holdout_best', None)\n",
    "\n",
    "model_4h, scaler_4h, metrics_4h, importance_4h, features_4h = train_model_with_holdout(\n",
    "    X_4h, y_4h, X_holdout_4h, y_holdout_4h,\n",
    "    BASELINES['4h']['best'], '4h', list(numeric_features_4h), holdout_baseline_4h\n",
    ")\n",
    "if model_4h:\n",
    "    trained_models['4h'] = {\n",
    "        'model': model_4h, 'scaler': scaler_4h,\n",
    "        'metrics': metrics_4h, 'features': features_4h\n",
    "    }\n",
    "    if importance_4h:\n",
    "        all_feature_importance['4h'] = importance_4h\n",
    "    \n",
    "    if len(features_4h) < len(numeric_features_4h):\n",
    "        pruned_features_log['4h'] = {\n",
    "            'original_count': len(numeric_features_4h),\n",
    "            'pruned_count': len(features_4h),\n",
    "            'removed': list(set(numeric_features_4h) - set(features_4h))\n",
    "        }\n",
    "        print(f\"  \u2713 Features pruned: {len(numeric_features_4h)} \u2192 {len(features_4h)}\")\n",
    "\n",
    "if TRAIN_REGIME_MODELS and regime_holdout is not None:\n",
    "    regime_holdout_4h = regime_holdout[mask_4h]\n",
    "    regime_models_4h = train_regime_models(\n",
    "        X_4h, y_4h, X_holdout_4h, y_holdout_4h,\n",
    "        regime_train, regime_holdout_4h,\n",
    "        BASELINES['4h']['best'], '4h', list(numeric_features_4h), holdout_baseline_4h\n",
    "    )\n",
    "    if regime_models_4h:\n",
    "        regime_specific_models['4h'] = regime_models_4h\n",
    "        trained_models['4h']['ensemble_predict'] = create_ensemble_predictor(\n",
    "            model_4h, scaler_4h, regime_models_4h, features_4h\n",
    "        )\n",
    "\n",
    "# === 24H MODEL ===\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"24-HOUR MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rph = 120\n",
    "total_hours = len(df_clean) / rph\n",
    "total_days = total_hours / 24\n",
    "print(f\"Total data: {total_days:.1f} days\")\n",
    "\n",
    "if total_days >= 30:\n",
    "    mask_24h_train = y_24h.notna()\n",
    "    X_24h_valid = X_24h[mask_24h_train]\n",
    "    y_24h_valid = y_24h[mask_24h_train]\n",
    "    \n",
    "    if HAS_HOLDOUT:\n",
    "        y_holdout_24h = df_holdout['target_24h']\n",
    "        mask_24h_holdout = y_holdout_24h.notna()\n",
    "        X_holdout_24h = df_holdout[numeric_features_24h][mask_24h_holdout]\n",
    "        y_holdout_24h = y_holdout_24h[mask_24h_holdout]\n",
    "    else:\n",
    "        X_holdout_24h = X_24h_valid.iloc[-500:]\n",
    "        y_holdout_24h = y_24h_valid.iloc[-500:]\n",
    "    \n",
    "    if len(y_holdout_24h) > 100:\n",
    "        model_24h, scaler_24h, metrics_24h, _, features_24h = train_model_with_holdout(\n",
    "            X_24h_valid, y_24h_valid, X_holdout_24h, y_holdout_24h,\n",
    "            BASELINES['4h']['best'], '24h', list(numeric_features_24h)\n",
    "        )\n",
    "        if model_24h:\n",
    "            trained_models['24h'] = {\n",
    "                'model': model_24h, 'scaler': scaler_24h,\n",
    "                'metrics': metrics_24h, 'features': features_24h,\n",
    "                'is_fallback': False\n",
    "            }\n",
    "    else:\n",
    "        print(f\"\u26a0\ufe0f Using 4h model as 24h fallback\")\n",
    "        if model_4h:\n",
    "            trained_models['24h'] = {\n",
    "                'model': model_4h, 'scaler': scaler_4h,\n",
    "                'metrics': {'name': metrics_4h['name'] + ' (4h fallback)', 'mae': metrics_4h['mae'],\n",
    "                           'improvement': metrics_4h['improvement'], \n",
    "                           'vs_holdout_baseline': metrics_4h.get('vs_holdout_baseline'),\n",
    "                           'passed_baseline': metrics_4h.get('passed_baseline', False)},\n",
    "                'features': features_4h,\n",
    "                'is_fallback': True\n",
    "            }\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f Using 4h model as 24h fallback ({total_days:.1f} days < 30)\")\n",
    "    if model_4h:\n",
    "        trained_models['24h'] = {\n",
    "            'model': model_4h, 'scaler': scaler_4h,\n",
    "            'metrics': {'name': metrics_4h['name'] + ' (4h fallback)', 'mae': metrics_4h['mae'],\n",
    "                       'improvement': metrics_4h['improvement'],\n",
    "                       'vs_holdout_baseline': metrics_4h.get('vs_holdout_baseline'),\n",
    "                       'passed_baseline': metrics_4h.get('passed_baseline', False)},\n",
    "            'features': features_4h,\n",
    "            'is_fallback': True\n",
    "        }\n",
    "\n",
    "# === SUMMARY ===\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for horizon, data in trained_models.items():\n",
    "    m = data['metrics']\n",
    "    status = \"\u2713\" if m.get('passed_baseline', False) else \"\u26a0\"\n",
    "    fallback = \" (fallback)\" if data.get('is_fallback') else \"\"\n",
    "    \n",
    "    # Show vs holdout baseline if available\n",
    "    if m.get('vs_holdout_baseline') is not None:\n",
    "        vs_baseline = f\"{m['vs_holdout_baseline']*100:+.1f}% vs holdout baseline\"\n",
    "    else:\n",
    "        vs_baseline = f\"{m['improvement']*100:+.1f}% vs train baseline\"\n",
    "    \n",
    "    has_ensemble = \" [+ensemble]\" if 'ensemble_predict' in data else \"\"\n",
    "    n_features = len(data.get('features', []))\n",
    "    print(f\"{status} {horizon}: {m['name']}{fallback} | MAE: {m['mae']:.4f} | {vs_baseline} | {n_features} features{has_ensemble}\")\n",
    "\n",
    "if regime_specific_models:\n",
    "    print(f\"\\nRegime-specific models:\")\n",
    "    for horizon, regime_dict in regime_specific_models.items():\n",
    "        for regime_val, regime_data in regime_dict.items():\n",
    "            print(f\"  {horizon}_{regime_data['regime_name']}: MAE={regime_data['metrics']['mae']:.4f}\")\n",
    "\n",
    "if pruned_features_log:\n",
    "    print(f\"\\nFeature pruning summary:\")\n",
    "    for horizon, log in pruned_features_log.items():\n",
    "        print(f\"  {horizon}: {log['original_count']} \u2192 {log['pruned_count']} features\")\n",
    "\n",
    "FEATURE_IMPORTANCE = all_feature_importance.get('4h', all_feature_importance.get('1h', {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME-OF-DAY SPECIFIC MODELS\n",
    "# Train lightweight models for each time period (afternoon has 2x higher errors)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TIME-OF-DAY SPECIFIC MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "TRAIN_TIME_SPECIFIC_MODELS = True\n",
    "MIN_SAMPLES_PER_PERIOD = 500\n",
    "\n",
    "time_specific_models = {}\n",
    "\n",
    "if TRAIN_TIME_SPECIFIC_MODELS:\n",
    "    time_periods = {\n",
    "        'night': (0, 6),\n",
    "        'morning': (6, 12),\n",
    "        'afternoon': (12, 18),  # Highest errors\n",
    "        'evening': (18, 24)\n",
    "    }\n",
    "    \n",
    "    for horizon in ['1h', '4h']:\n",
    "        if horizon not in trained_models:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{horizon} Time-Specific Models:\")\n",
    "        \n",
    "        features = trained_models[horizon]['features']\n",
    "        global_model = trained_models[horizon]['model']\n",
    "        global_scaler = trained_models[horizon]['scaler']\n",
    "        global_mae = trained_models[horizon]['metrics']['mae']\n",
    "        \n",
    "        time_specific_models[horizon] = {}\n",
    "        \n",
    "        for period_name, (start_hour, end_hour) in time_periods.items():\n",
    "            # Filter data for this time period\n",
    "            train_hours = df_train_val.index.hour\n",
    "            train_mask = (train_hours >= start_hour) & (train_hours < end_hour)\n",
    "            \n",
    "            X_train_period = df_train_val.loc[train_mask, features]\n",
    "            y_train_period = df_train_val.loc[train_mask, f'target_{horizon}']\n",
    "            \n",
    "            # Remove NaN\n",
    "            valid_mask = y_train_period.notna()\n",
    "            X_train_period = X_train_period[valid_mask]\n",
    "            y_train_period = y_train_period[valid_mask]\n",
    "            \n",
    "            if len(X_train_period) < MIN_SAMPLES_PER_PERIOD:\n",
    "                print(f\"  {period_name}: Insufficient data ({len(X_train_period)} samples)\")\n",
    "                continue\n",
    "            \n",
    "            # Holdout data for this period\n",
    "            if HAS_HOLDOUT:\n",
    "                holdout_hours = df_holdout.index.hour\n",
    "                holdout_mask = (holdout_hours >= start_hour) & (holdout_hours < end_hour)\n",
    "                \n",
    "                X_holdout_period = df_holdout.loc[holdout_mask, features]\n",
    "                y_holdout_period = df_holdout.loc[holdout_mask, f'target_{horizon}']\n",
    "                \n",
    "                valid_mask_h = y_holdout_period.notna()\n",
    "                X_holdout_period = X_holdout_period[valid_mask_h]\n",
    "                y_holdout_period = y_holdout_period[valid_mask_h]\n",
    "            else:\n",
    "                # Split training data\n",
    "                split_idx = int(len(X_train_period) * 0.8)\n",
    "                X_holdout_period = X_train_period.iloc[split_idx:]\n",
    "                y_holdout_period = y_train_period.iloc[split_idx:]\n",
    "                X_train_period = X_train_period.iloc[:split_idx]\n",
    "                y_train_period = y_train_period.iloc[:split_idx]\n",
    "            \n",
    "            if len(X_holdout_period) < 50:\n",
    "                print(f\"  {period_name}: Insufficient holdout data\")\n",
    "                continue\n",
    "            \n",
    "            # Train a simple model for this period\n",
    "            scaler = RobustScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train_period)\n",
    "            X_holdout_scaled = scaler.transform(X_holdout_period)\n",
    "            \n",
    "            # Use GBM with moderate complexity\n",
    "            model = GradientBoostingRegressor(\n",
    "                n_estimators=30, max_depth=4, learning_rate=0.1,\n",
    "                min_samples_leaf=30, subsample=0.8, random_state=42\n",
    "            )\n",
    "            model.fit(X_train_scaled, y_train_period)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_holdout_scaled)\n",
    "            period_mae = mean_absolute_error(y_holdout_period, y_pred)\n",
    "            \n",
    "            # Compare to global model\n",
    "            global_pred = global_model.predict(global_scaler.transform(X_holdout_period))\n",
    "            global_period_mae = mean_absolute_error(y_holdout_period, global_pred)\n",
    "            \n",
    "            improvement = (global_period_mae - period_mae) / global_period_mae * 100\n",
    "            \n",
    "            print(f\"  {period_name}: Global MAE={global_period_mae:.4f}, Period MAE={period_mae:.4f} ({improvement:+.1f}%)\")\n",
    "            \n",
    "            # Only use period-specific model if it's better\n",
    "            if period_mae < global_period_mae:\n",
    "                time_specific_models[horizon][period_name] = {\n",
    "                    'model': model,\n",
    "                    'scaler': scaler,\n",
    "                    'mae': float(period_mae),\n",
    "                    'global_mae': float(global_period_mae),\n",
    "                    'improvement': float(improvement),\n",
    "                    'n_samples': len(X_train_period),\n",
    "                    'features': features\n",
    "                }\n",
    "                print(f\"    \u2713 Using period-specific model\")\n",
    "            else:\n",
    "                print(f\"    \u2717 Global model is better, skipping\")\n",
    "        \n",
    "        # Create time-adaptive predictor\n",
    "        if time_specific_models.get(horizon):\n",
    "            def create_time_adaptive_predictor(global_model, global_scaler, time_models, features):\n",
    "                def predict(X, hour=None):\n",
    "                    if hour is None:\n",
    "                        # Use global\n",
    "                        X_scaled = global_scaler.transform(X[features] if hasattr(X, 'columns') else X)\n",
    "                        return global_model.predict(X_scaled)\n",
    "                    \n",
    "                    # Determine period\n",
    "                    if 0 <= hour < 6:\n",
    "                        period = 'night'\n",
    "                    elif 6 <= hour < 12:\n",
    "                        period = 'morning'\n",
    "                    elif 12 <= hour < 18:\n",
    "                        period = 'afternoon'\n",
    "                    else:\n",
    "                        period = 'evening'\n",
    "                    \n",
    "                    # Use period-specific model if available\n",
    "                    if period in time_models:\n",
    "                        tm = time_models[period]\n",
    "                        X_scaled = tm['scaler'].transform(X[features] if hasattr(X, 'columns') else X)\n",
    "                        return tm['model'].predict(X_scaled)\n",
    "                    else:\n",
    "                        # Fall back to global\n",
    "                        X_scaled = global_scaler.transform(X[features] if hasattr(X, 'columns') else X)\n",
    "                        return global_model.predict(X_scaled)\n",
    "                \n",
    "                return predict\n",
    "            \n",
    "            trained_models[horizon]['time_adaptive_predict'] = create_time_adaptive_predictor(\n",
    "                global_model, global_scaler, time_specific_models[horizon], features\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n  \u2713 Time-adaptive predictor created with {len(time_specific_models[horizon])} period models\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TIME-SPECIFIC MODELS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for horizon, periods in time_specific_models.items():\n",
    "    if periods:\n",
    "        print(f\"\\n{horizon}:\")\n",
    "        for period, data in periods.items():\n",
    "            print(f\"  {period}: MAE={data['mae']:.4f} ({data['improvement']:+.1f}% vs global)\")\n",
    "    else:\n",
    "        print(f\"\\n{horizon}: No period-specific models (global is best)\")\n",
    "\n",
    "print(f\"\\n\u2713 Time-of-day model training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# PREDICTION INTERVALS - FIXED CALIBRATION on HOLDOUT DATA",
    "# IMPROVED: Night-specific asymmetric intervals + Tail risk capping",
    "from sklearn.ensemble import GradientBoostingRegressor, IsolationForest",
    "",
    "print(\"\\n\" + \"=\"*60)",
    "print(\"TRAINING PREDICTION INTERVALS (HOLDOUT-CALIBRATED)\")",
    "print(\"=\"*60)",
    "",
    "quantile_models = {}",
    "conformal_residuals = {}",
    "uncertainty_scalers = {}",
    "time_period_calibration = {}",
    "tail_risk_config = {}  # NEW: Store tail risk capping config",
    "",
    "def compute_holdout_conformal_interval(X_holdout, y_holdout, model, scaler, alpha=0.2):",
    "    \"\"\"",
    "    Compute conformal interval on HOLDOUT data for proper calibration.",
    "    alpha=0.2 means 80% interval.",
    "    \"\"\"",
    "    X_scaled = scaler.transform(X_holdout)",
    "    y_pred = model.predict(X_scaled)",
    "",
    "    residuals = np.abs(y_holdout.values - y_pred)",
    "",
    "    # Use holdout residuals directly for calibration",
    "    q = np.quantile(residuals, 1 - alpha)",
    "",
    "    # Verify coverage on holdout",
    "    coverage = np.mean(residuals <= q)",
    "",
    "    return {",
    "        'quantile': q,",
    "        'residuals': residuals,",
    "        'coverage_target': 1 - alpha,",
    "        'actual_coverage': coverage,",
    "        'calibration_source': 'holdout'",
    "    }",
    "",
    "def compute_adaptive_conformal_intervals_v2(X_holdout, y_holdout, model, scaler, hours, regimes=None):",
    "    \"\"\"",
    "    IMPROVED: Compute separate conformal intervals with asymmetric handling for night.",
    "    Ensures 80% coverage across all conditions including problematic night period.",
    "    \"\"\"",
    "    X_scaled = scaler.transform(X_holdout)",
    "    y_pred = model.predict(X_scaled)",
    "    residuals = np.abs(y_holdout.values - y_pred)",
    "    signed_residuals = y_holdout.values - y_pred  # For asymmetric analysis",
    "",
    "    # Time-period specific intervals",
    "    time_intervals = {}",
    "    time_periods = {",
    "        'night': (0, 6),",
    "        'morning': (6, 12),",
    "        'afternoon': (12, 18),",
    "        'evening': (18, 24)",
    "    }",
    "",
    "    overall_q80 = np.quantile(residuals, 0.8)",
    "",
    "    for period_name, (start, end) in time_periods.items():",
    "        mask = (hours >= start) & (hours < end)",
    "        if mask.sum() >= 50:",
    "            period_residuals = residuals[mask]",
    "            period_signed = signed_residuals[mask]",
    "",
    "            q80 = np.quantile(period_residuals, 0.8)",
    "            q90 = np.quantile(period_residuals, 0.9)",
    "",
    "            # NEW: Asymmetric intervals for night (spikes tend to be upward)",
    "            # Calculate separate up/down intervals",
    "            up_residuals = period_signed[period_signed > 0]  # Under-predictions",
    "            down_residuals = np.abs(period_signed[period_signed < 0])  # Over-predictions",
    "",
    "            # For night: use wider upside interval (gas can spike up)",
    "            if period_name == 'night' and len(up_residuals) > 10:",
    "                up_q80 = np.quantile(up_residuals, 0.85) if len(up_residuals) > 10 else q80",
    "                down_q80 = np.quantile(down_residuals, 0.75) if len(down_residuals) > 10 else q80 * 0.7",
    "",
    "                # If night coverage is poor, widen further",
    "                actual_coverage = np.mean(period_residuals <= q80)",
    "                if actual_coverage < 0.75:",
    "                    coverage_shortfall = (0.80 - actual_coverage) / 0.80",
    "                    q80 = q80 * (1 + coverage_shortfall * 0.5)  # Widen by up to 50%",
    "                    q90 = q90 * (1 + coverage_shortfall * 0.5)",
    "                    up_q80 = up_q80 * (1 + coverage_shortfall * 0.3)",
    "                    print(f\"  Night coverage fix: widening intervals by {coverage_shortfall*50:.0f}%\")",
    "            else:",
    "                up_q80 = q80",
    "                down_q80 = q80",
    "",
    "            multiplier = q80 / overall_q80 if overall_q80 > 0 else 1.0",
    "",
    "            time_intervals[period_name] = {",
    "                'interval_80': float(q80),",
    "                'interval_90': float(q90),",
    "                'interval_up_80': float(up_q80),    # NEW: Asymmetric up",
    "                'interval_down_80': float(down_q80),  # NEW: Asymmetric down",
    "                'multiplier': float(max(multiplier, 1.0)),",
    "                'mae': float(np.mean(period_residuals)),",
    "                'n_samples': int(mask.sum()),",
    "                'actual_coverage_80': float(np.mean(period_residuals <= q80)),",
    "                'is_asymmetric': period_name == 'night'  # Flag for asymmetric intervals",
    "            }",
    "",
    "    # Regime-specific intervals",
    "    regime_intervals = {}",
    "    if regimes is not None:",
    "        for regime_val, regime_name in [(0, 'normal'), (1, 'elevated'), (2, 'spike')]:",
    "            mask = regimes == regime_val",
    "            if mask.sum() >= 50:",
    "                regime_residuals = residuals[mask]",
    "                q80 = np.quantile(regime_residuals, 0.8)",
    "                q90 = np.quantile(regime_residuals, 0.9)",
    "",
    "                multiplier = q80 / overall_q80 if overall_q80 > 0 else 1.0",
    "",
    "                regime_intervals[regime_name] = {",
    "                    'interval_80': float(q80),",
    "                    'interval_90': float(q90),",
    "                    'multiplier': float(max(multiplier, 1.0)),",
    "                    'mae': float(np.mean(regime_residuals)),",
    "                    'n_samples': int(mask.sum()),",
    "                    'actual_coverage_80': float(np.mean(regime_residuals <= q80))",
    "                }",
    "",
    "    return time_intervals, regime_intervals",
    "",
    "def compute_tail_risk_caps(y_true, y_pred, percentile=0.95):",
    "    \"\"\"",
    "    NEW: Compute prediction capping values to handle tail risk.",
    "    Returns caps that limit extreme predictions.",
    "    \"\"\"",
    "    errors = y_true - y_pred",
    "",
    "    # Cap extreme predictions based on historical distribution",
    "    pred_min = np.percentile(y_true, 1)",
    "    pred_max = np.percentile(y_true, 99)",
    "",
    "    # Error-based caps: limit to P95 error magnitude",
    "    error_p95 = np.percentile(np.abs(errors), percentile)",
    "",
    "    # Range-based cap: limit change from recent mean",
    "    recent_mean = np.mean(y_true[-min(500, len(y_true)):])",
    "    recent_std = np.std(y_true[-min(500, len(y_true)):])",
    "",
    "    return {",
    "        'pred_min': float(pred_min),",
    "        'pred_max': float(pred_max),",
    "        'error_p95': float(error_p95),",
    "        'recent_mean': float(recent_mean),",
    "        'recent_std': float(recent_std),",
    "        'max_deviation_from_mean': float(3.0 * recent_std),  # 3 sigma limit",
    "        'winsorize_percentile': percentile",
    "    }",
    "",
    "def apply_tail_risk_capping(predictions, caps, current_gas=None):",
    "    \"\"\"",
    "    Apply tail risk capping to predictions.",
    "    Prevents extreme predictions that are unlikely to be correct.",
    "    \"\"\"",
    "    capped = np.array(predictions).copy()",
    "",
    "    # Method 1: Hard min/max from historical",
    "    capped = np.clip(capped, caps['pred_min'], caps['pred_max'])",
    "",
    "    # Method 2: Limit deviation from recent mean",
    "    if current_gas is not None:",
    "        max_up = current_gas + caps['max_deviation_from_mean']",
    "        max_down = current_gas - caps['max_deviation_from_mean']",
    "        max_down = max(max_down, caps['pred_min'])  # Don't go below historical min",
    "        capped = np.clip(capped, max_down, max_up)",
    "",
    "    return capped",
    "",
    "def train_uncertainty_scaler(X_train, scaler, residuals, features):",
    "    \"\"\"Train OOD detector for uncertainty scaling\"\"\"",
    "    X_scaled = scaler.transform(X_train)",
    "",
    "    iso_forest = IsolationForest(",
    "        n_estimators=50, contamination=0.1,",
    "        random_state=42, n_jobs=-1",
    "    )",
    "    iso_forest.fit(X_scaled)",
    "",
    "    feature_means = X_scaled.mean(axis=0)",
    "    feature_stds = X_scaled.std(axis=0) + 1e-8",
    "    base_interval = np.quantile(residuals, 0.8)",
    "",
    "    return {",
    "        'iso_forest': iso_forest,",
    "        'feature_means': feature_means,",
    "        'feature_stds': feature_stds,",
    "        'base_interval': base_interval,",
    "        'features': features",
    "    }",
    "",
    "def get_adaptive_interval_v2(base_interval, hour=None, regime=None,",
    "                             time_intervals=None, regime_intervals=None,",
    "                             direction='symmetric'):",
    "    \"\"\"",
    "    IMPROVED: Get the appropriate interval width with asymmetric support.",
    "    direction: 'up', 'down', or 'symmetric'",
    "    \"\"\"",
    "    multiplier = 1.0",
    "",
    "    # Time-based adjustment",
    "    if hour is not None and time_intervals:",
    "        period = None",
    "        if 0 <= hour < 6:",
    "            period = 'night'",
    "        elif 6 <= hour < 12:",
    "            period = 'morning'",
    "        elif 12 <= hour < 18:",
    "            period = 'afternoon'",
    "        else:",
    "            period = 'evening'",
    "",
    "        if period in time_intervals:",
    "            ti = time_intervals[period]",
    "            if ti.get('is_asymmetric', False) and direction != 'symmetric':",
    "                # Use asymmetric intervals for night",
    "                if direction == 'up':",
    "                    return ti.get('interval_up_80', ti['interval_80'])",
    "                else:",
    "                    return ti.get('interval_down_80', ti['interval_80'])",
    "            multiplier = max(multiplier, ti['multiplier'])",
    "",
    "    # Regime-based adjustment",
    "    if regime is not None and regime_intervals:",
    "        regime_name = {0: 'normal', 1: 'elevated', 2: 'spike'}.get(regime)",
    "        if regime_name in regime_intervals:",
    "            multiplier = max(multiplier, regime_intervals[regime_name]['multiplier'])",
    "",
    "    return base_interval * multiplier",
    "",
    "for horizon in ['1h', '4h']:",
    "    if horizon not in trained_models:",
    "        continue",
    "",
    "    print(f\"\\n{horizon} prediction intervals...\")",
    "",
    "    data = trained_models[horizon]",
    "    model = data['model']",
    "    scaler = data['scaler']",
    "    features = data['features']",
    "",
    "    # === Use HOLDOUT data for calibration ===",
    "    if not HAS_HOLDOUT:",
    "        print(f\"  \u26a0\ufe0f No holdout data, using training data (may be miscalibrated)\")",
    "        X_cal = df_train_val[features]",
    "        y_cal = df_train_val[f'target_{horizon}']",
    "    else:",
    "        X_cal = df_holdout[features]",
    "        y_cal = df_holdout[f'target_{horizon}']",
    "",
    "    mask = y_cal.notna()",
    "    X_cal = X_cal[mask]",
    "    y_cal = y_cal[mask]",
    "",
    "    if len(X_cal) < 500:",
    "        print(f\"  \u26a0\ufe0f Insufficient data for {horizon} intervals, skipping\")",
    "        continue",
    "",
    "    # Get hours and regimes for adaptive calibration",
    "    hours = X_cal.index.hour if hasattr(X_cal.index, 'hour') else pd.Series(12, index=X_cal.index)",
    "",
    "    regimes = pd.Series(0, index=X_cal.index)",
    "    if 'gas_zscore_1h' in df_holdout.columns if HAS_HOLDOUT else df_train_val.columns:",
    "        source_df = df_holdout if HAS_HOLDOUT else df_train_val",
    "        regimes[source_df.loc[X_cal.index, 'gas_zscore_1h'] > 1] = 1",
    "    if 'is_spike' in df_holdout.columns if HAS_HOLDOUT else df_train_val.columns:",
    "        source_df = df_holdout if HAS_HOLDOUT else df_train_val",
    "        regimes[source_df.loc[X_cal.index, 'is_spike'] == 1] = 2",
    "",
    "    # === Compute HOLDOUT-CALIBRATED conformal intervals ===",
    "    conformal = compute_holdout_conformal_interval(X_cal, y_cal, model, scaler, alpha=0.2)",
    "    conformal_residuals[horizon] = conformal",
    "    print(f\"  \u2713 Conformal interval (holdout-calibrated): \u00b1{conformal['quantile']:.4f}\")",
    "    print(f\"    Actual coverage on holdout: {conformal['actual_coverage']:.1%}\")",
    "",
    "    # === Compute ADAPTIVE intervals with ASYMMETRIC night support ===",
    "    time_intervals, regime_intervals = compute_adaptive_conformal_intervals_v2(",
    "        X_cal, y_cal, model, scaler, hours.values, regimes.values",
    "    )",
    "    time_period_calibration[horizon] = {",
    "        'time': time_intervals,",
    "        'regime': regime_intervals",
    "    }",
    "",
    "    print(f\"  \u2713 Time-adaptive intervals computed:\")",
    "    for period, info in time_intervals.items():",
    "        asym_flag = \" [ASYMMETRIC]\" if info.get('is_asymmetric', False) else \"\"",
    "        print(f\"    {period}: \u00b1{info['interval_80']:.4f} (coverage: {info['actual_coverage_80']:.1%}){asym_flag}\")",
    "        if info.get('is_asymmetric', False):",
    "            print(f\"      Up: {info.get('interval_up_80', 0):.4f}, Down: {info.get('interval_down_80', 0):.4f}\")",
    "",
    "    # === NEW: Compute tail risk caps ===",
    "    X_scaled = scaler.transform(X_cal)",
    "    y_pred = model.predict(X_scaled)",
    "    tail_caps = compute_tail_risk_caps(y_cal.values, y_pred, percentile=0.95)",
    "    tail_risk_config[horizon] = tail_caps",
    "    print(f\"  \u2713 Tail risk caps: pred range [{tail_caps['pred_min']:.2f}, {tail_caps['pred_max']:.2f}]\")",
    "    print(f\"    Max deviation from mean: \u00b1{tail_caps['max_deviation_from_mean']:.2f}\")",
    "",
    "    # === Train quantile models on training data ===",
    "    X_train_q = df_train_val[features]",
    "    y_train_q = df_train_val[f'target_{horizon}']",
    "    mask_q = y_train_q.notna()",
    "    X_train_q = X_train_q[mask_q]",
    "    y_train_q = y_train_q[mask_q]",
    "",
    "    split_idx = int(len(X_train_q) * 0.8)",
    "    X_train, X_test = X_train_q.iloc[:split_idx], X_train_q.iloc[split_idx:]",
    "    y_train, y_test = y_train_q.iloc[:split_idx], y_train_q.iloc[split_idx:]",
    "",
    "    q_scaler = RobustScaler()",
    "    X_train_scaled = q_scaler.fit_transform(X_train)",
    "",
    "    q_models = {}",
    "    for q in [0.1, 0.5, 0.9]:",
    "        qmodel = GradientBoostingRegressor(",
    "            loss='quantile', alpha=q,",
    "            n_estimators=50, max_depth=4,",
    "            learning_rate=0.1, random_state=42",
    "        )",
    "        qmodel.fit(X_train_scaled, y_train)",
    "        q_models[q] = qmodel",
    "",
    "    quantile_models[horizon] = (q_models, q_scaler)",
    "    print(f\"  \u2713 Quantile models trained\")",
    "",
    "    # === Train uncertainty scaler ===",
    "    unc_scaler = train_uncertainty_scaler(X_train, q_scaler, conformal['residuals'], features)",
    "    uncertainty_scalers[horizon] = unc_scaler",
    "    print(f\"  \u2713 Uncertainty scaler trained\")",
    "",
    "    # === Verify calibration on holdout ===",
    "    abs_errors = np.abs(y_cal.values - y_pred)",
    "    conf_coverage = np.mean(abs_errors <= conformal['quantile'])",
    "",
    "    # Adaptive coverage (using time/regime specific intervals)",
    "    adaptive_coverages = []",
    "    for i, (idx, row) in enumerate(X_cal.iterrows()):",
    "        h = hours.iloc[i] if hasattr(hours, 'iloc') else hours[i]",
    "        r = regimes.iloc[i] if hasattr(regimes, 'iloc') else regimes[i]",
    "",
    "        adaptive_interval = get_adaptive_interval_v2(",
    "            conformal['quantile'], hour=h, regime=r,",
    "            time_intervals=time_intervals, regime_intervals=regime_intervals",
    "        )",
    "        adaptive_coverages.append(abs_errors[i] <= adaptive_interval)",
    "",
    "    adaptive_coverage = np.mean(adaptive_coverages)",
    "",
    "    print(f\"\\n  Calibration Verification (on holdout):\")",
    "    print(f\"    Conformal 80% interval: actual = {conf_coverage:.1%}\")",
    "    print(f\"    Adaptive 80% interval: actual = {adaptive_coverage:.1%}\")",
    "",
    "    # Check night specifically",
    "    night_mask = (hours.values >= 0) & (hours.values < 6)",
    "    if night_mask.sum() > 50:",
    "        night_coverage = np.mean(np.array(adaptive_coverages)[night_mask])",
    "        print(f\"    Night period coverage: {night_coverage:.1%}\")",
    "        if night_coverage < 0.75:",
    "            print(f\"    \u26a0\ufe0f Night coverage still low - may need wider intervals\")",
    "",
    "    # Store calibration in model data",
    "    trained_models[horizon]['calibration'] = {",
    "        'conformal_coverage': float(conf_coverage),",
    "        'adaptive_coverage': float(adaptive_coverage),",
    "        'conformal_width': float(conformal['quantile']),",
    "        'time_intervals': time_intervals,",
    "        'regime_intervals': regime_intervals,",
    "        'calibration_source': 'holdout',",
    "        'tail_risk_caps': tail_caps  # NEW: Include tail risk config",
    "    }",
    "",
    "    # Store warnings",
    "    warnings_list = []",
    "    for period, info in time_intervals.items():",
    "        if info['actual_coverage_80'] < 0.75:",
    "            warnings_list.append(f\"{period} period under-covered: {info['actual_coverage_80']:.1%} (target 80%)\")",
    "    trained_models[horizon]['calibration']['warnings'] = warnings_list",
    "",
    "# Copy 4h to 24h",
    "if '4h' in quantile_models:",
    "    quantile_models['24h'] = quantile_models['4h']",
    "    print(\"\\n24h: Using 4h quantile models\")",
    "",
    "if '4h' in conformal_residuals:",
    "    conformal_residuals['24h'] = conformal_residuals['4h']",
    "",
    "if '4h' in uncertainty_scalers:",
    "    uncertainty_scalers['24h'] = uncertainty_scalers['4h']",
    "",
    "if '4h' in time_period_calibration:",
    "    time_period_calibration['24h'] = time_period_calibration['4h']",
    "",
    "if '4h' in tail_risk_config:",
    "    tail_risk_config['24h'] = tail_risk_config['4h']",
    "",
    "print(f\"\\n\u2713 Prediction intervals (holdout-calibrated) ready for: {list(quantile_models.keys())}\")",
    "print(f\"\u2713 Tail risk capping configured for: {list(tail_risk_config.keys())}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI-REGIME CALIBRATION VERIFICATION\n",
    "# Verify that prediction intervals are well-calibrated across different conditions\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-REGIME CALIBRATION VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "regime_calibration = {}\n",
    "\n",
    "def verify_calibration_by_regime(X, y, model, scaler, conformal_quantile, \n",
    "                                  regime_labels, time_calibration=None, \n",
    "                                  uncertainty_scaler=None):\n",
    "    \"\"\"\n",
    "    Verify prediction interval coverage for each regime and time period.\n",
    "    Returns detailed calibration metrics.\n",
    "    \"\"\"\n",
    "    X_scaled = scaler.transform(X)\n",
    "    y_pred = model.predict(X_scaled)\n",
    "    errors = np.abs(y.values - y_pred)\n",
    "    \n",
    "    results = {\n",
    "        'overall': {},\n",
    "        'by_regime': {},\n",
    "        'by_time': {},\n",
    "        'warnings': []\n",
    "    }\n",
    "    \n",
    "    # Overall coverage at different interval widths\n",
    "    for coverage_target in [0.8, 0.9, 0.95]:\n",
    "        interval_width = np.quantile(errors, coverage_target)\n",
    "        actual_coverage = np.mean(errors <= interval_width)\n",
    "        results['overall'][f'coverage_{int(coverage_target*100)}'] = {\n",
    "            'target': coverage_target,\n",
    "            'actual': float(actual_coverage),\n",
    "            'interval_width': float(interval_width),\n",
    "            'calibrated': abs(actual_coverage - coverage_target) < 0.05\n",
    "        }\n",
    "    \n",
    "    # Coverage by regime\n",
    "    for regime_val, regime_name in [(0, 'normal'), (1, 'elevated'), (2, 'spike')]:\n",
    "        mask = regime_labels == regime_val\n",
    "        if mask.sum() < 50:\n",
    "            continue\n",
    "        \n",
    "        regime_errors = errors[mask]\n",
    "        regime_y = y.values[mask]\n",
    "        regime_pred = y_pred[mask]\n",
    "        \n",
    "        # Standard conformal coverage\n",
    "        in_interval = regime_errors <= conformal_quantile\n",
    "        regime_coverage = np.mean(in_interval)\n",
    "        \n",
    "        # Adaptive coverage (if time calibration available)\n",
    "        if time_calibration and uncertainty_scaler:\n",
    "            hours = X.index.hour[mask] if hasattr(X.index, 'hour') else np.full(mask.sum(), 12)\n",
    "            adaptive_coverages = []\n",
    "            for i, (h, err) in enumerate(zip(hours, regime_errors)):\n",
    "                period = 'afternoon' if 12 <= h < 18 else ('morning' if 6 <= h < 12 else ('evening' if 18 <= h < 24 else 'night'))\n",
    "                mult = time_calibration.get(period, {}).get('multiplier', 1.0)\n",
    "                adapted_interval = conformal_quantile * mult\n",
    "                adaptive_coverages.append(err <= adapted_interval)\n",
    "            adaptive_coverage = np.mean(adaptive_coverages)\n",
    "        else:\n",
    "            adaptive_coverage = regime_coverage\n",
    "        \n",
    "        results['by_regime'][regime_name] = {\n",
    "            'n_samples': int(mask.sum()),\n",
    "            'mae': float(np.mean(regime_errors)),\n",
    "            'std': float(np.std(regime_errors)),\n",
    "            'conformal_coverage': float(regime_coverage),\n",
    "            'adaptive_coverage': float(adaptive_coverage),\n",
    "            'target_coverage': 0.8,\n",
    "            'calibrated': abs(regime_coverage - 0.8) < 0.1\n",
    "        }\n",
    "        \n",
    "        if regime_coverage < 0.7:\n",
    "            results['warnings'].append(f\"{regime_name} regime under-covered: {regime_coverage:.1%} (target 80%)\")\n",
    "    \n",
    "    # Coverage by time period\n",
    "    hours = X.index.hour if hasattr(X.index, 'hour') else pd.Series(12, index=X.index)\n",
    "    time_periods = {\n",
    "        'night': (0, 6),\n",
    "        'morning': (6, 12),\n",
    "        'afternoon': (12, 18),\n",
    "        'evening': (18, 24)\n",
    "    }\n",
    "    \n",
    "    for period_name, (start, end) in time_periods.items():\n",
    "        mask = (hours >= start) & (hours < end)\n",
    "        if mask.sum() < 50:\n",
    "            continue\n",
    "        \n",
    "        period_errors = errors[mask]\n",
    "        \n",
    "        # Standard conformal coverage\n",
    "        in_interval = period_errors <= conformal_quantile\n",
    "        period_coverage = np.mean(in_interval)\n",
    "        \n",
    "        # Adaptive coverage\n",
    "        if time_calibration and period_name in time_calibration:\n",
    "            mult = time_calibration[period_name].get('multiplier', 1.0)\n",
    "            adapted_interval = conformal_quantile * mult\n",
    "            adaptive_coverage = np.mean(period_errors <= adapted_interval)\n",
    "        else:\n",
    "            adaptive_coverage = period_coverage\n",
    "        \n",
    "        results['by_time'][period_name] = {\n",
    "            'n_samples': int(mask.sum()),\n",
    "            'mae': float(np.mean(period_errors)),\n",
    "            'conformal_coverage': float(period_coverage),\n",
    "            'adaptive_coverage': float(adaptive_coverage),\n",
    "            'calibrated': abs(adaptive_coverage - 0.8) < 0.1\n",
    "        }\n",
    "        \n",
    "        if period_coverage < 0.7:\n",
    "            results['warnings'].append(f\"{period_name} period under-covered: {period_coverage:.1%} (target 80%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "for horizon in ['1h', '4h']:\n",
    "    if horizon not in trained_models:\n",
    "        continue\n",
    "    if horizon not in conformal_residuals:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{horizon} Calibration Verification\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    data = trained_models[horizon]\n",
    "    model = data['model']\n",
    "    scaler = data['scaler']\n",
    "    features = data['features']\n",
    "    conformal = conformal_residuals[horizon]\n",
    "    time_cal = time_period_calibration.get(horizon, {})\n",
    "    unc_scaler = uncertainty_scalers.get(horizon, None)\n",
    "    \n",
    "    if not HAS_HOLDOUT:\n",
    "        print(\"  \u26a0\ufe0f No holdout data for calibration verification\")\n",
    "        continue\n",
    "    \n",
    "    # Get holdout data\n",
    "    X_test = df_holdout[features]\n",
    "    y_test = df_holdout[f'target_{horizon}']\n",
    "    mask = y_test.notna()\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    \n",
    "    # Create regime labels\n",
    "    regime_test = pd.Series(0, index=X_test.index)\n",
    "    if 'gas_zscore_1h' in df_holdout.columns:\n",
    "        regime_test[df_holdout.loc[X_test.index, 'gas_zscore_1h'] > 1] = 1\n",
    "    if 'is_spike' in df_holdout.columns:\n",
    "        regime_test[df_holdout.loc[X_test.index, 'is_spike'] == 1] = 2\n",
    "    \n",
    "    # Verify calibration\n",
    "    cal_results = verify_calibration_by_regime(\n",
    "        X_test, y_test, model, scaler,\n",
    "        conformal['quantile'], regime_test.values,\n",
    "        time_cal, unc_scaler\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n  Overall Calibration:\")\n",
    "    for level, metrics in cal_results['overall'].items():\n",
    "        status = \"\u2713\" if metrics['calibrated'] else \"\u26a0\"\n",
    "        print(f\"    {status} {int(metrics['target']*100)}% target: actual={metrics['actual']:.1%}, width={metrics['interval_width']:.4f}\")\n",
    "    \n",
    "    print(\"\\n  Calibration by Regime:\")\n",
    "    for regime_name, metrics in cal_results['by_regime'].items():\n",
    "        status = \"\u2713\" if metrics['calibrated'] else \"\u26a0\"\n",
    "        print(f\"    {status} {regime_name}: conformal={metrics['conformal_coverage']:.1%}, adaptive={metrics['adaptive_coverage']:.1%} ({metrics['n_samples']} samples)\")\n",
    "    \n",
    "    print(\"\\n  Calibration by Time Period:\")\n",
    "    for period_name, metrics in cal_results['by_time'].items():\n",
    "        status = \"\u2713\" if metrics['calibrated'] else \"\u26a0\"\n",
    "        print(f\"    {status} {period_name}: conformal={metrics['conformal_coverage']:.1%}, adaptive={metrics['adaptive_coverage']:.1%}\")\n",
    "    \n",
    "    if cal_results['warnings']:\n",
    "        print(\"\\n  \u26a0\ufe0f Calibration Warnings:\")\n",
    "        for warning in cal_results['warnings']:\n",
    "            print(f\"    - {warning}\")\n",
    "    \n",
    "    # Store results\n",
    "    regime_calibration[horizon] = cal_results\n",
    "    \n",
    "    # Update trained_models with detailed calibration\n",
    "    if 'calibration' not in trained_models[horizon]:\n",
    "        trained_models[horizon]['calibration'] = {}\n",
    "    trained_models[horizon]['calibration']['regime_breakdown'] = cal_results['by_regime']\n",
    "    trained_models[horizon]['calibration']['time_breakdown'] = cal_results['by_time']\n",
    "    trained_models[horizon]['calibration']['warnings'] = cal_results['warnings']\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CALIBRATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_calibrated = True\n",
    "for horizon, cal in regime_calibration.items():\n",
    "    n_warnings = len(cal['warnings'])\n",
    "    if n_warnings == 0:\n",
    "        print(f\"  \u2713 {horizon}: All regimes and time periods well-calibrated\")\n",
    "    else:\n",
    "        all_calibrated = False\n",
    "        print(f\"  \u26a0 {horizon}: {n_warnings} calibration warning(s)\")\n",
    "\n",
    "if all_calibrated:\n",
    "    print(\"\\n\u2713 Prediction intervals are well-calibrated across all conditions\")\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f Some conditions show poor calibration - consider regime-specific intervals\")\n",
    "\n",
    "print(f\"\\n\u2713 Multi-regime calibration verification complete\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Direction Prediction - IMPROVED\n",
    "# Changes: Binary up/down, class weights, holdout evaluation, adaptive threshold\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING DIRECTION MODELS (IMPROVED)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "direction_models = {}\n",
    "\n",
    "# Configuration\n",
    "USE_BINARY = True  # Binary (up/down) vs 3-class (down/stable/up)\n",
    "DIRECTION_THRESHOLD = 0.01  # 1% threshold for direction change\n",
    "\n",
    "def create_binary_direction(target, current, threshold=0.01):\n",
    "    \"\"\"Create binary direction labels: 1=up, 0=down/stable\"\"\"\n",
    "    pct_change = (target - current) / (current + 1e-8)\n",
    "    return (pct_change > threshold).astype(int)\n",
    "\n",
    "def create_ternary_direction(target, current, threshold=0.02):\n",
    "    \"\"\"Create 3-class direction labels\"\"\"\n",
    "    pct_change = (target - current) / (current + 1e-8)\n",
    "    direction = pd.Series('stable', index=target.index)\n",
    "    direction[pct_change > threshold] = 'up'\n",
    "    direction[pct_change < -threshold] = 'down'\n",
    "    return direction\n",
    "\n",
    "for horizon in ['1h', '4h']:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{horizon.upper()} DIRECTION MODEL\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Get features and targets\n",
    "    X_h = X_1h if horizon == '1h' else X_4h\n",
    "    features = numeric_features_1h if horizon == '1h' else numeric_features_4h\n",
    "    \n",
    "    # Get raw target for direction calculation\n",
    "    if 'target_1h_raw' in df_train_val.columns:\n",
    "        target_raw = df_train_val[f'target_{horizon}_raw']\n",
    "    else:\n",
    "        target_raw = df_train_val[f'target_{horizon}']\n",
    "    \n",
    "    current = df_train_val['gas']\n",
    "    \n",
    "    # Create direction labels\n",
    "    if USE_BINARY:\n",
    "        y_dir = create_binary_direction(target_raw, current, DIRECTION_THRESHOLD)\n",
    "        print(f\"Binary classification (threshold: {DIRECTION_THRESHOLD*100}%)\")\n",
    "    else:\n",
    "        y_dir = create_ternary_direction(target_raw, current)\n",
    "        print(f\"3-class classification (threshold: {DIRECTION_THRESHOLD*100}%)\")\n",
    "    \n",
    "    mask = y_dir.notna() & target_raw.notna()\n",
    "    X_d = X_h[mask]\n",
    "    y_d = y_dir[mask]\n",
    "    \n",
    "    if len(X_d) < 1000:\n",
    "        print(f\"  \u26a0\ufe0f Insufficient data, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Class distribution\n",
    "    class_counts = y_d.value_counts()\n",
    "    print(f\"Class distribution: {dict(class_counts)}\")\n",
    "    \n",
    "    # Compute class weights\n",
    "    classes = np.unique(y_d)\n",
    "    weights = compute_class_weight('balanced', classes=classes, y=y_d)\n",
    "    class_weight_dict = dict(zip(classes, weights))\n",
    "    print(f\"Class weights: {class_weight_dict}\")\n",
    "    \n",
    "    # Split - use holdout if available\n",
    "    if HAS_HOLDOUT:\n",
    "        X_train, X_test = X_d, df_holdout[features]\n",
    "        y_train = y_d\n",
    "        \n",
    "        # Create holdout labels\n",
    "        if 'target_1h_raw' in df_holdout.columns:\n",
    "            holdout_target = df_holdout[f'target_{horizon}_raw']\n",
    "        else:\n",
    "            holdout_target = df_holdout[f'target_{horizon}']\n",
    "        holdout_current = df_holdout['gas']\n",
    "        \n",
    "        if USE_BINARY:\n",
    "            y_test = create_binary_direction(holdout_target, holdout_current, DIRECTION_THRESHOLD)\n",
    "        else:\n",
    "            y_test = create_ternary_direction(holdout_target, holdout_current)\n",
    "        \n",
    "        test_mask = y_test.notna() & holdout_target.notna()\n",
    "        X_test = X_test[test_mask]\n",
    "        y_test = y_test[test_mask]\n",
    "        print(f\"Using holdout for evaluation ({len(y_test)} samples)\")\n",
    "    else:\n",
    "        split_idx = int(len(X_d) * 0.8)\n",
    "        X_train, X_test = X_d.iloc[:split_idx], X_d.iloc[split_idx:]\n",
    "        y_train, y_test = y_d.iloc[:split_idx], y_d.iloc[split_idx:]\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Try multiple classifiers\n",
    "    classifiers = [\n",
    "        ('LogReg', LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)),\n",
    "        ('RF', RandomForestClassifier(n_estimators=30, max_depth=4, class_weight='balanced', random_state=42, n_jobs=-1)),\n",
    "        ('GBM', GradientBoostingClassifier(n_estimators=30, max_depth=3, learning_rate=0.1, random_state=42)),\n",
    "    ]\n",
    "    \n",
    "    best_clf = None\n",
    "    best_acc = 0\n",
    "    best_name = None\n",
    "    \n",
    "    for name, clf in classifiers:\n",
    "        try:\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            y_pred = clf.predict(X_test_scaled)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "            print(f\"  {name}: Acc={acc:.1%}, F1={f1:.3f}\")\n",
    "            \n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_clf = clf\n",
    "                best_name = name\n",
    "                best_f1 = f1\n",
    "        except Exception as e:\n",
    "            print(f\"  {name}: Failed - {e}\")\n",
    "    \n",
    "    if best_clf is None:\n",
    "        print(f\"  \u26a0\ufe0f All classifiers failed\")\n",
    "        continue\n",
    "    \n",
    "    # Baseline: always predict majority class\n",
    "    majority_class = y_train.mode()[0]\n",
    "    baseline_acc = (y_test == majority_class).mean()\n",
    "    improvement = (best_acc - baseline_acc) / baseline_acc * 100\n",
    "    \n",
    "    print(f\"\\n  >>> Best: {best_name} (Acc: {best_acc:.1%}, vs baseline {baseline_acc:.1%}: {improvement:+.1f}%)\")\n",
    "    \n",
    "    direction_models[horizon] = {\n",
    "        'model': best_clf,\n",
    "        'scaler': scaler,\n",
    "        'accuracy': float(best_acc),\n",
    "        'f1_score': float(best_f1),\n",
    "        'baseline_accuracy': float(baseline_acc),\n",
    "        'improvement_vs_baseline': float(improvement),\n",
    "        'model_name': best_name,\n",
    "        'is_binary': USE_BINARY,\n",
    "        'threshold': DIRECTION_THRESHOLD\n",
    "    }\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DIRECTION MODEL SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "for horizon, data in direction_models.items():\n",
    "    imp = data['improvement_vs_baseline']\n",
    "    status = \"\u2713\" if imp > 5 else \"\u26a0\" if imp > 0 else \"\u2717\"\n",
    "    print(f\"{status} {horizon}: {data['model_name']} | Acc: {data['accuracy']:.1%} | vs baseline: {imp:+.1f}%\")\n",
    ""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# REGIME DETECTION\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING REGIME DETECTION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create regime labels from gas statistics (instead of volatility_regime)\n",
    "# 0 = Normal, 1 = Elevated, 2 = Spike\n",
    "if 'gas_zscore_1h' in df_train_val.columns and 'is_spike' in df_train_val.columns:\n",
    "    # Create regime from z-score: low (<-0.5), normal (-0.5 to 1), elevated (1 to 2), spike (>2)\n",
    "    zscore = df_train_val['gas_zscore_1h']\n",
    "    is_spike = df_train_val['is_spike']\n",
    "    \n",
    "    regime_labels = pd.Series(0, index=df_train_val.index)  # Default: Normal\n",
    "    regime_labels[zscore > 1] = 1  # Elevated\n",
    "    regime_labels[is_spike == 1] = 2  # Spike\n",
    "    \n",
    "    X_r = X_4h.copy()\n",
    "    y_r = regime_labels\n",
    "    \n",
    "    if len(X_r) < 500:\n",
    "        print(\"\u26a0\ufe0f Insufficient data for regime detection\")\n",
    "        regime_clf = None\n",
    "        regime_scaler = None\n",
    "        regime_accuracy = 0\n",
    "    else:\n",
    "        # Train/test split\n",
    "        split_idx = int(len(X_r) * 0.8)\n",
    "        X_train, X_test = X_r.iloc[:split_idx], X_r.iloc[split_idx:]\n",
    "        y_train, y_test = y_r.iloc[:split_idx], y_r.iloc[split_idx:]\n",
    "        \n",
    "        regime_scaler = RobustScaler()\n",
    "        X_train_scaled = regime_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = regime_scaler.transform(X_test)\n",
    "        \n",
    "        # Train classifier (simple, reduced complexity)\n",
    "        regime_clf = RandomForestClassifier(\n",
    "            n_estimators=30, max_depth=4,\n",
    "            min_samples_leaf=20,\n",
    "            random_state=42, n_jobs=-1\n",
    "        )\n",
    "        regime_clf.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = regime_clf.predict(X_test_scaled)\n",
    "        regime_accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Regime classes: Normal (0), Elevated (1), Spike (2)\")\n",
    "        print(f\"Class distribution: {dict(y_r.value_counts().sort_index())}\")\n",
    "        print(f\"Accuracy: {regime_accuracy:.1%}\")\n",
    "        \n",
    "        if regime_accuracy > 0.95:\n",
    "            print(\"\u26a0\ufe0f Warning: Very high accuracy may indicate class imbalance or overfitting\")\n",
    "else:\n",
    "    regime_clf = None\n",
    "    regime_scaler = None\n",
    "    regime_accuracy = 0\n",
    "    print(\"\u26a0\ufe0f Missing gas_zscore_1h or is_spike, skipping regime detection\")\n",
    ""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Spike Detectors\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nprint(\"\\n",
    "\" + \"=\"*60)\nprint(\"TRAINING SPIKE DETECTORS\")\nprint(\"=\"*60)\n\nspike_models = {}\n\nfor horizon, X_h, y_target in [('1h', X_1h, y_1h), ('4h', X_4h, y_4h)]:\n    print(f\"\\n",
    "{horizon} spike detector...\")\n    \n    # Create spike labels (>2 std from mean is a spike)\n    mask = y_target.notna()\n    X_s = X_h[mask]\n    y_s = y_target[mask]\n    current = current_gas[mask]\n    \n    # Define spike threshold\n    price_change = y_s - current\n    threshold = price_change.std() * 2\n    spike_labels = (price_change > threshold).astype(int)\n    \n    spike_rate = spike_labels.mean()\n    print(f\"  Spike rate: {spike_rate:.1%}\")\n    \n    if spike_rate < 0.01 or spike_rate > 0.5:\n        print(f\"  \u26a0\ufe0f Unusual spike rate, skipping\")\n        continue\n    \n    if len(X_s) < 1000:\n        print(f\"  \u26a0\ufe0f Insufficient data, skipping\")\n        continue\n    \n    # Train/test split\n    split_idx = int(len(X_s) * 0.8)\n    X_train, X_test = X_s.iloc[:split_idx], X_s.iloc[split_idx:]\n    y_train, y_test = spike_labels.iloc[:split_idx], spike_labels.iloc[split_idx:]\n    \n    scaler = RobustScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Train with class weights\n    clf = GradientBoostingClassifier(\n        n_estimators=50, max_depth=4,\n        learning_rate=0.1, random_state=42\n    )\n    clf.fit(X_train_scaled, y_train)\n    \n    # Evaluate\n    y_pred = clf.predict(X_test_scaled)\n    acc = accuracy_score(y_test, y_pred)\n    \n    spike_models[horizon] = (clf, scaler)\n    print(f\"  Accuracy: {acc:.1%}\")\n\n# Copy 4h to 24h if available\nif '4h' in spike_models:\n    spike_models['24h'] = spike_models['4h']\n    print(\"\\n",
    "24h: Using 4h spike detector (fallback)\")\n\nprint(f\"\\n",
    "\u2713 Spike detectors trained for: {list(spike_models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# SPIKE FORECASTING - IMPROVED with Multiple Definitions & Volatility Features",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier",
    "from sklearn.linear_model import LogisticRegression",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve",
    "import warnings",
    "warnings.filterwarnings('ignore')",
    "",
    "print(\"\\n\" + \"=\"*60)",
    "print(\"SPIKE FORECASTING (IMPROVED)\")",
    "print(\"=\"*60)",
    "",
    "# Check for SMOTE",
    "try:",
    "    from imblearn.over_sampling import SMOTE",
    "    HAS_SMOTE = True",
    "except ImportError:",
    "    HAS_SMOTE = False",
    "",
    "spike_forecast_models = {}",
    "",
    "def create_volatility_features(df):",
    "    \"\"\"",
    "    Create comprehensive volatility and regime-transition features.",
    "    Focus on patterns that precede spikes, not just rate-of-change.",
    "    \"\"\"",
    "    rph = 120  # rows per hour",
    "    features = {}",
    "    ",
    "    # === VOLATILITY CLUSTERING (GARCH-like) ===",
    "    # Squared returns as volatility proxy",
    "    returns = df['gas'].pct_change()",
    "    features['volatility_5min'] = returns.rolling(10).std()",
    "    features['volatility_15min'] = returns.rolling(30).std()",
    "    features['volatility_1h'] = returns.rolling(rph).std()",
    "    ",
    "    # Volatility of volatility (regime change indicator)",
    "    features['vol_of_vol'] = features['volatility_15min'].rolling(rph).std()",
    "    ",
    "    # Volatility ratio (short/long) - high ratio = volatility increasing",
    "    features['vol_ratio'] = features['volatility_5min'] / (features['volatility_1h'] + 1e-8)",
    "    ",
    "    # === REGIME TRANSITION FEATURES ===",
    "    # Moving average crossovers",
    "    ma_5min = df['gas'].rolling(10).mean()",
    "    ma_30min = df['gas'].rolling(60).mean()",
    "    ma_1h = df['gas'].rolling(rph).mean()",
    "    features['ma_cross_5_30'] = (ma_5min - ma_30min) / (ma_30min + 1e-8)",
    "    features['ma_cross_30_60'] = (ma_30min - ma_1h) / (ma_1h + 1e-8)",
    "    ",
    "    # Distance from recent low (potential for mean reversion spike)",
    "    rolling_low_1h = df['gas'].rolling(rph).min()",
    "    features['dist_from_low'] = (df['gas'] - rolling_low_1h) / (rolling_low_1h + 1e-8)",
    "    ",
    "    # Distance from recent high",
    "    rolling_high_1h = df['gas'].rolling(rph).max()",
    "    features['dist_from_high'] = (rolling_high_1h - df['gas']) / (df['gas'] + 1e-8)",
    "    ",
    "    # Range as % of price (consolidation vs expansion)",
    "    features['range_pct'] = (rolling_high_1h - rolling_low_1h) / (df['gas'] + 1e-8)",
    "    ",
    "    # === ABSOLUTE LEVEL FEATURES ===",
    "    # Current gas relative to historical percentiles",
    "    rolling_median_4h = df['gas'].rolling(4 * rph).median()",
    "    features['above_median_4h'] = (df['gas'] > rolling_median_4h).astype(float)",
    "    features['pct_above_median'] = (df['gas'] - rolling_median_4h) / (rolling_median_4h + 1e-8)",
    "    ",
    "    # Absolute gas level buckets (spikes often start from low base)",
    "    features['gas_level'] = df['gas']",
    "    features['is_low_gas'] = (df['gas'] < df['gas'].rolling(4 * rph).quantile(0.25)).astype(float)",
    "    features['is_high_gas'] = (df['gas'] > df['gas'].rolling(4 * rph).quantile(0.75)).astype(float)",
    "    ",
    "    # === MOMENTUM FEATURES ===",
    "    features['momentum_15min'] = df['gas'].diff(30)",
    "    features['momentum_1h'] = df['gas'].diff(rph)",
    "    features['momentum_accel'] = features['momentum_15min'].diff(30)",
    "    ",
    "    # Rate of change",
    "    features['roc_5min'] = df['gas'].pct_change(10)",
    "    features['roc_15min'] = df['gas'].pct_change(30)",
    "    features['roc_1h'] = df['gas'].pct_change(rph)",
    "    ",
    "    # === TIME FEATURES ===",
    "    if hasattr(df.index, 'hour'):",
    "        features['hour'] = df.index.hour",
    "        features['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 24)",
    "        features['hour_cos'] = np.cos(2 * np.pi * df.index.hour / 24)",
    "        # High activity hours (when spikes are more likely)",
    "        features['is_active_hours'] = ((df.index.hour >= 13) & (df.index.hour <= 21)).astype(float)",
    "    ",
    "    return pd.DataFrame(features, index=df.index)",
    "",
    "def create_spike_target_multi(df, horizon_hours, definitions):",
    "    \"\"\"",
    "    Create spike target using multiple definitions.",
    "    Returns dict of targets for each definition.",
    "    \"\"\"",
    "    rph = 120",
    "    horizon_periods = horizon_hours * rph",
    "    targets = {}",
    "    ",
    "    # Calculate forward max",
    "    future_max = df['gas'].shift(-1).rolling(horizon_periods, min_periods=1).max()",
    "    future_max = future_max.shift(-horizon_periods + 1)",
    "    current_gas = df['gas']",
    "    pct_change = (future_max - current_gas) / (current_gas + 1e-8)",
    "    abs_change = future_max - current_gas",
    "    ",
    "    for defn in definitions:",
    "        name = defn['name']",
    "        if defn['type'] == 'pct':",
    "            targets[name] = (pct_change > defn['threshold']).astype(int)",
    "        elif defn['type'] == 'abs':",
    "            targets[name] = (abs_change > defn['threshold']).astype(int)",
    "        elif defn['type'] == 'combined':",
    "            targets[name] = ((pct_change > defn['pct_threshold']) | ",
    "                            (abs_change > defn['abs_threshold'])).astype(int)",
    "        elif defn['type'] == 'percentile':",
    "            # Spike = future max is in top X percentile",
    "            threshold = df['gas'].rolling(4 * rph, min_periods=rph).quantile(defn['percentile'])",
    "            targets[name] = (future_max > threshold).astype(int)",
    "        ",
    "        targets[name] = targets[name].fillna(0).astype(int)",
    "    ",
    "    return targets",
    "",
    "def find_optimal_threshold(y_true, y_prob):",
    "    \"\"\"Find threshold that maximizes F1\"\"\"",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)",
    "    f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-8)",
    "    if len(f1_scores) == 0:",
    "        return 0.5, 0.0",
    "    best_idx = np.argmax(f1_scores)",
    "    return thresholds[best_idx], f1_scores[best_idx]",
    "",
    "# === CREATE FEATURES ===",
    "print(\"\\nCreating volatility features...\")",
    "vol_features_train = create_volatility_features(df_train_val)",
    "if HAS_HOLDOUT:",
    "    vol_features_holdout = create_volatility_features(df_holdout)",
    "",
    "# Add to dataframes",
    "for col in vol_features_train.columns:",
    "    if col not in df_train_val.columns:",
    "        df_train_val[col] = vol_features_train[col]",
    "        if HAS_HOLDOUT:",
    "            df_holdout[col] = vol_features_holdout[col]",
    "",
    "# Select features for spike prediction",
    "# IMPROVED: Horizon-specific feature sets",
    "# Shorter horizons need shorter lookback windows",
    "SPIKE_FEATURES_SHORT = [  # For 1h/2h horizons",
    "    'hour_sin', 'hour_cos', 'is_active_hours',",
    "    'volatility_5min', 'volatility_15min',  # Short-term vol only",
    "    'vol_ratio',",
    "    'ma_cross_5_30',",
    "    'dist_from_low', 'dist_from_high',",
    "    'is_low_gas', 'is_high_gas',",
    "    'momentum_15min', 'momentum_accel',  # Fast momentum",
    "    'roc_5min', 'roc_15min'  # Fast ROC",
    "]",
    "",
    "SPIKE_FEATURES_LONG = [  # For 4h+ horizons",
    "    'hour_sin', 'hour_cos', 'is_active_hours',",
    "    'volatility_5min', 'volatility_15min', 'volatility_1h',",
    "    'vol_of_vol', 'vol_ratio',",
    "    'ma_cross_5_30', 'ma_cross_30_60',",
    "    'dist_from_low', 'dist_from_high', 'range_pct',",
    "    'is_low_gas', 'is_high_gas', 'pct_above_median',",
    "    'momentum_15min', 'momentum_1h', 'momentum_accel',",
    "    'roc_5min', 'roc_15min', 'roc_1h'",
    "]",
    "",
    "# Select features based on horizon",
    "def get_spike_features_for_horizon(horizon_hours):",
    "    if horizon_hours <= 2:",
    "        return SPIKE_FEATURES_SHORT",
    "    else:",
    "        return SPIKE_FEATURES_LONG",
    "",
    "spike_features = SPIKE_FEATURES_LONG  # Default for compatibility",
    "",
    "available_features = [f for f in spike_features if f in df_train_val.columns]",
    "print(f\"Using {len(available_features)} features for spike forecasting\")",
    "",
    "# === SPIKE DEFINITIONS TO TRY ===",
    "SPIKE_DEFINITIONS = [",
    "    {'name': 'pct_15', 'type': 'pct', 'threshold': 0.15},",
    "    {'name': 'pct_25', 'type': 'pct', 'threshold': 0.25},",
    "    {'name': 'pct_50', 'type': 'pct', 'threshold': 0.50},",
    "    {'name': 'abs_5gwei', 'type': 'abs', 'threshold': 5.0},",
    "    {'name': 'abs_10gwei', 'type': 'abs', 'threshold': 10.0},",
    "    {'name': 'combined', 'type': 'combined', 'pct_threshold': 0.20, 'abs_threshold': 5.0},",
    "    {'name': 'top10pct', 'type': 'percentile', 'percentile': 0.90},",
    "]",
    "",
    "# === TRAIN SPIKE FORECASTERS ===",
    "for horizon_name, horizon_hours in [('1h', 1), ('2h', 2), ('4h', 4)]:  # Ordered by horizon",
    "    # Get horizon-specific features",
    "    horizon_spike_features = get_spike_features_for_horizon(horizon_hours)",
    "    available_features = [f for f in horizon_spike_features if f in df_train_val.columns]",
    "    print(f\"  Using {len(available_features)} features for {horizon_name} spike forecast\")",
    "    print(f\"\\n{'='*50}\")",
    "    print(f\"{horizon_name} Spike Forecast Model\")",
    "    print(f\"{'='*50}\")",
    "    ",
    "    best_model_result = None",
    "    best_auc = 0",
    "    ",
    "    # Create all spike targets",
    "    spike_targets_train = create_spike_target_multi(df_train_val, horizon_hours, SPIKE_DEFINITIONS)",
    "    if HAS_HOLDOUT:",
    "        spike_targets_holdout = create_spike_target_multi(df_holdout, horizon_hours, SPIKE_DEFINITIONS)",
    "    ",
    "    for defn in SPIKE_DEFINITIONS:",
    "        defn_name = defn['name']",
    "        print(f\"\\n  Testing {defn_name}...\")",
    "        ",
    "        y_sf = spike_targets_train[defn_name]",
    "        X_sf = df_train_val[available_features].copy()  # Uses horizon-specific features",
    "        ",
    "        # Remove NaN",
    "        mask = y_sf.notna() & X_sf.notna().all(axis=1)",
    "        X_sf = X_sf[mask]",
    "        y_sf = y_sf[mask]",
    "        ",
    "        if len(X_sf) < 1000:",
    "            print(f\"    \u26a0\ufe0f Insufficient data ({len(X_sf)} samples)\")",
    "            continue",
    "        ",
    "        spike_rate = y_sf.mean()",
    "        print(f\"    Spike rate: {spike_rate:.1%} ({y_sf.sum():.0f} spikes)\")",
    "        ",
    "        # Skip extreme imbalance",
    "        if spike_rate > 0.7 or spike_rate < 0.01:",
    "            print(f\"    \u26a0\ufe0f Class imbalance too extreme\")",
    "            continue",
    "        ",
    "        # Prepare test set",
    "        if HAS_HOLDOUT:",
    "            X_train = X_sf",
    "            y_train = y_sf",
    "            ",
    "            y_test = spike_targets_holdout[defn_name]",
    "            X_test = df_holdout[available_features].copy()",
    "            ",
    "            mask_test = y_test.notna() & X_test.notna().all(axis=1)",
    "            X_test = X_test[mask_test]",
    "            y_test = y_test[mask_test]",
    "        else:",
    "            split_idx = int(len(X_sf) * 0.8)",
    "            X_train, X_test = X_sf.iloc[:split_idx], X_sf.iloc[split_idx:]",
    "            y_train, y_test = y_sf.iloc[:split_idx], y_sf.iloc[split_idx:]",
    "        ",
    "        if len(X_test) < 100 or y_test.sum() < 5:",
    "            print(f\"    \u26a0\ufe0f Insufficient test data\")",
    "            continue",
    "        ",
    "        scaler = RobustScaler()",
    "        X_train_scaled = scaler.fit_transform(X_train)",
    "        X_test_scaled = scaler.transform(X_test)",
    "        ",
    "        # Handle class imbalance",
    "        use_smote = HAS_SMOTE and y_train.sum() >= 10 and (y_train == 0).sum() >= 10",
    "        ",
    "        if use_smote:",
    "            try:",
    "                k_neighbors = min(5, int(min(y_train.sum(), (y_train == 0).sum())) - 1)",
    "                k_neighbors = max(1, k_neighbors)",
    "                smote = SMOTE(random_state=42, k_neighbors=k_neighbors)",
    "                X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)",
    "            except:",
    "                X_train_resampled, y_train_resampled = X_train_scaled, y_train",
    "                use_smote = False",
    "        else:",
    "            X_train_resampled, y_train_resampled = X_train_scaled, y_train",
    "        ",
    "        # Try classifiers",
    "        classifiers = [",
    "            ('GBM', GradientBoostingClassifier(n_estimators=100, max_depth=4, learning_rate=0.1, ",
    "                                               min_samples_leaf=20, subsample=0.8, random_state=42)),",
    "            ('RF', RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_leaf=20,",
    "                                          class_weight='balanced', random_state=42, n_jobs=-1)),",
    "            ('LogReg', LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000, random_state=42))",
    "        ]",
    "        ",
    "        for clf_name, clf in classifiers:",
    "            try:",
    "                if use_smote:",
    "                    clf.fit(X_train_resampled, y_train_resampled)",
    "                else:",
    "                    clf.fit(X_train_scaled, y_train)",
    "                ",
    "                y_prob = clf.predict_proba(X_test_scaled)[:, 1]",
    "                ",
    "                # Calculate AUC",
    "                try:",
    "                    auc = roc_auc_score(y_test, y_prob)",
    "                except:",
    "                    auc = 0.5",
    "                ",
    "                # Only consider if AUC > 0.55 (better than random)",
    "                if auc > best_auc and auc > 0.55:",
    "                    optimal_threshold, optimal_f1 = find_optimal_threshold(y_test, y_prob)",
    "                    y_pred = (y_prob >= optimal_threshold).astype(int)",
    "                    ",
    "                    precision = precision_score(y_test, y_pred, zero_division=0)",
    "                    recall = recall_score(y_test, y_pred, zero_division=0)",
    "                    f1 = f1_score(y_test, y_pred, zero_division=0)",
    "                    ",
    "                    # Must have reasonable precision (>50%) to be useful",
    "                    if precision >= 0.5:",
    "                        best_auc = auc",
    "                        best_model_result = {",
    "                            'model': clf,",
    "                            'scaler': scaler,",
    "                            'spike_definition': defn,",
    "                            'classifier': clf_name,",
    "                            'optimal_threshold': float(optimal_threshold),",
    "                            'precision': float(precision),",
    "                            'recall': float(recall),",
    "                            'f1_score': float(f1),",
    "                            'auc': float(auc),",
    "                            'spike_rate_train': float(spike_rate),",
    "                            'spike_rate_test': float(y_test.mean()),",
    "                            'features': available_features,",
    "                            'horizon': horizon_name",
    "                        }",
    "                        print(f\"    {clf_name}: AUC={auc:.3f}, P={precision:.1%}, R={recall:.1%}, F1={f1:.3f} \u2713\")",
    "                    else:",
    "                        print(f\"    {clf_name}: AUC={auc:.3f}, P={precision:.1%} (too low)\")",
    "                        ",
    "            except Exception as e:",
    "                pass",
    "    ",
    "    # Store results",
    "    if best_model_result:",
    "        spike_forecast_models[horizon_name] = best_model_result",
    "        print(f\"\\n  \u2713 Best: {best_model_result['classifier']} with {best_model_result['spike_definition']['name']}\")",
    "        print(f\"    AUC={best_model_result['auc']:.3f}, P={best_model_result['precision']:.1%}, R={best_model_result['recall']:.1%}\")",
    "    else:",
    "        print(f\"\\n  \u26a0\ufe0f No viable {horizon_name} spike model (need AUC>0.55 and P>50%)\")",
    "        spike_forecast_models[horizon_name] = {",
    "            'model': None,",
    "            'scaler': None,",
    "            'precision': 0.0,",
    "            'recall': 0.0,",
    "            'f1_score': 0.0,",
    "            'auc': 0.5,",
    "            'has_model': False,",
    "            'note': 'No model met quality thresholds'",
    "        }",
    "",
    "print(f\"\\n{'='*60}\")",
    "print(\"SPIKE FORECAST SUMMARY\")",
    "print(\"=\"*60)",
    "for h, data in spike_forecast_models.items():",
    "    if data.get('model') is not None:",
    "        print(f\"  {h}: AUC={data['auc']:.3f}, P={data['precision']:.1%}, R={data['recall']:.1%}\")",
    "        print(f\"      Definition: {data['spike_definition']['name']}\")",
    "    else:",
    "        print(f\"  {h}: No viable model\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# DYNAMIC ENSEMBLE WEIGHTING + CONFIDENCE SCORINGprint(\"\\n\" + \"=\"*60)print(\"DYNAMIC ENSEMBLE WEIGHTING + CONFIDENCE SCORING\")print(\"=\"*60)def calculate_dynamic_weights(global_errors, regime_errors, decay=0.9):    \"\"\"Calculate dynamic weights based on exponentially weighted recent errors\"\"\"    if len(global_errors) == 0 or len(regime_errors) == 0:        return 0.5, 0.5        n = len(global_errors)    exp_weights = np.array([decay ** (n - i - 1) for i in range(n)])    exp_weights = exp_weights / exp_weights.sum()        global_weighted_error = np.sum(global_errors * exp_weights)    regime_weighted_error = np.sum(regime_errors * exp_weights)        total_inv_error = 1/(global_weighted_error + 1e-8) + 1/(regime_weighted_error + 1e-8)    global_weight = (1/(global_weighted_error + 1e-8)) / total_inv_error    regime_weight = (1/(regime_weighted_error + 1e-8)) / total_inv_error        return global_weight, regime_weightdef compute_prediction_confidence(predictions, base_interval, volatility_ratio=1.0):    \"\"\"    Compute confidence score based on model agreement and volatility.    Returns confidence (0-1) and suggested interval multiplier.    \"\"\"    if len(predictions) <= 1:        return 1.0, 1.0  # High confidence, no multiplier        # Model disagreement as spread    pred_std = np.std(predictions)    pred_mean = np.mean(predictions)    pred_cv = pred_std / (np.abs(pred_mean) + 1e-8)  # Coefficient of variation        # Normalize disagreement (higher = less confident)    # pred_cv > 0.5 means models disagree by more than 50%    disagreement = min(pred_cv / 0.5, 1.0)        # Confidence = 1 - disagreement, adjusted for volatility    confidence = (1 - disagreement) / (1 + volatility_ratio * 0.5)    confidence = max(0.2, min(1.0, confidence))        # Interval multiplier: widen when confidence is low    # confidence=1.0 -> multiplier=1.0, confidence=0.5 -> multiplier=1.5    multiplier = 1.0 + (1.0 - confidence)        return confidence, multiplierclass ConfidenceScoringPredictor:    \"\"\"Predictor that outputs confidence scores alongside predictions.\"\"\"        def __init__(self, main_model, main_scaler, features,                  quantile_models=None, quantile_scaler=None,                 regime_models=None, base_interval=0.3):        self.main_model = main_model        self.main_scaler = main_scaler        self.features = features        self.quantile_models = quantile_models        self.quantile_scaler = quantile_scaler        self.regime_models = regime_models        self.base_interval = base_interval        self.recent_errors = []        self.max_history = 100        def predict_with_confidence(self, X, hour=None, regime=None, volatility_ratio=1.0):",
    "        \"\"\"",
    "        ENHANCED: Make prediction with comprehensive confidence scoring.",
    "        Returns dict with prediction, confidence, intervals, and diagnostics.",
    "        \"\"\"",
    "        # Prepare input",
    "        if hasattr(X, 'columns'):",
    "            X_subset = X[self.features] if all(f in X.columns for f in self.features) else X",
    "        else:",
    "            X_subset = X",
    "",
    "        # Get main prediction",
    "        X_scaled = self.main_scaler.transform(X_subset)",
    "        main_pred = self.main_model.predict(X_scaled)",
    "",
    "        # Collect predictions from different models",
    "        all_predictions = [main_pred]",
    "        model_sources = ['main']",
    "",
    "        # Add quantile median if available",
    "        if self.quantile_models and self.quantile_scaler:",
    "            try:",
    "                X_q_scaled = self.quantile_scaler.transform(X_subset)",
    "                if 0.5 in self.quantile_models:",
    "                    q_pred = self.quantile_models[0.5].predict(X_q_scaled)",
    "                    all_predictions.append(q_pred)",
    "                    model_sources.append('quantile_median')",
    "            except:",
    "                pass",
    "",
    "        # Add regime model prediction if available",
    "        if regime is not None and self.regime_models and regime in self.regime_models:",
    "            try:",
    "                regime_data = self.regime_models[regime]",
    "                regime_features = regime_data.get('features', self.features)",
    "                if hasattr(X, 'columns'):",
    "                    available = [f for f in regime_features if f in X.columns]",
    "                    if len(available) == len(regime_features):",
    "                        X_regime = X[regime_features]",
    "                        X_regime_scaled = regime_data['scaler'].transform(X_regime)",
    "                        regime_pred = regime_data['model'].predict(X_regime_scaled)",
    "                        all_predictions.append(regime_pred)",
    "                        model_sources.append(f'regime_{regime}')",
    "            except:",
    "                pass",
    "",
    "        # Compute model disagreement",
    "        all_preds_array = np.array([p.flatten()[0] if hasattr(p, 'flatten') else p for p in all_predictions])",
    "        model_disagreement = float(np.std(all_preds_array)) if len(all_preds_array) > 1 else 0.0",
    "        pred_cv = model_disagreement / (np.abs(np.mean(all_preds_array)) + 1e-8)",
    "",
    "        # === ENHANCED CONFIDENCE CALCULATION ===",
    "        # 1. Model agreement component (0-1, higher = more agreement)",
    "        agreement_score = max(0, 1 - (pred_cv / 0.3))",
    "",
    "        # 2. Rolling performance component (based on recent hit rate)",
    "        performance_score = (self.hit_rate_80 + self.hit_rate_90) / 2",
    "",
    "        # 3. Volatility component (lower confidence in high volatility)",
    "        volatility_score = 1.0 / (1.0 + volatility_ratio * 0.5)",
    "",
    "        # 4. Regime component (lower confidence in spike/elevated regimes)",
    "        regime_score = 1.0",
    "        if regime is not None:",
    "            regime_scores = {0: 1.0, 1: 0.8, 2: 0.7}",
    "            regime_score = regime_scores.get(regime, 0.9)",
    "",
    "        # 5. Time component (lower confidence in volatile hours)",
    "        time_score = 1.0",
    "        if hour is not None:",
    "            if 12 <= hour < 18:",
    "                time_score = 0.85",
    "            elif 0 <= hour < 6:",
    "                time_score = 0.9",
    "",
    "        # Combine scores (weighted geometric mean)",
    "        weights = [0.35, 0.25, 0.15, 0.15, 0.10]",
    "        scores = [agreement_score, performance_score, volatility_score, regime_score, time_score]",
    "        confidence = np.prod([s ** w for s, w in zip(scores, weights)])",
    "        confidence = max(0.2, min(1.0, confidence))",
    "",
    "        # === ADAPTIVE INTERVAL CALCULATION ===",
    "        interval_multiplier = 1.0 + (1.0 - confidence) * 0.5",
    "",
    "        if hour is not None and 12 <= hour < 18:",
    "            interval_multiplier *= 1.15",
    "        if regime == 2:",
    "            interval_multiplier *= 1.2",
    "",
    "        interval_width = self.base_interval * interval_multiplier",
    "",
    "        if len(self.recent_errors) >= 10:",
    "            recent_mae = np.mean(self.recent_errors[-10:])",
    "            interval_width = max(interval_width, recent_mae * 1.5)",
    "",
    "        pred_value = main_pred.flatten()[0] if hasattr(main_pred, 'flatten') else float(main_pred)",
    "",
    "        return {",
    "            'prediction': pred_value,",
    "            'confidence': float(confidence),",
    "            'interval_low': pred_value - interval_width,",
    "            'interval_high': pred_value + interval_width,",
    "            'interval_width': float(interval_width),",
    "            'model_disagreement': float(model_disagreement),",
    "            'n_models': len(all_predictions),",
    "            'model_sources': model_sources,",
    "            'confidence_components': {",
    "                'agreement': float(agreement_score),",
    "                'performance': float(performance_score),",
    "                'volatility': float(volatility_score),",
    "                'regime': float(regime_score),",
    "                'time': float(time_score)",
    "            },",
    "            'rolling_hit_rate_80': float(self.hit_rate_80),",
    "            'rolling_mae': float(self.rolling_mae)",
    "        }",
    "",
    "    def update_with_actual(self, prediction, actual, interval_width=None):        \"\"\"        ENHANCED: Update rolling metrics for adaptive confidence.        Tracks hit rates for 80% and 90% intervals.        \"\"\"        error = abs(actual - prediction)        self.recent_errors.append(error)        self.recent_predictions.append(prediction)        self.recent_actuals.append(actual)        # Maintain history size        if len(self.recent_errors) > self.max_history:            self.recent_errors.pop(0)            self.recent_predictions.pop(0)            self.recent_actuals.pop(0)        # Update rolling MAE        if len(self.recent_errors) >= 10:            self.rolling_mae = np.mean(self.recent_errors[-min(50, len(self.recent_errors)):])        # Update hit rates if interval was provided        if interval_width is not None:            interval_80 = interval_width            interval_90 = interval_width * 1.3            # Calculate rolling hit rates            recent_n = min(50, len(self.recent_errors))            recent_errors = np.array(self.recent_errors[-recent_n:])            self.hit_rate_80 = np.mean(recent_errors <= interval_80)            self.hit_rate_90 = np.mean(recent_errors <= interval_90)def create_dynamic_ensemble_predictor(global_model, global_scaler, regime_models, features, decay=0.9):    \"\"\"Create ensemble predictor with dynamic weighting and confidence scoring.\"\"\"    recent_errors = {'global': [], 'regime': {}}        def predict(X, current_regime=None, actual_value=None):        nonlocal recent_errors                if hasattr(X, 'columns'):            X_global = X[features] if all(f in X.columns for f in features) else X            X_scaled = global_scaler.transform(X_global)        else:            X_scaled = global_scaler.transform(X)                global_pred = global_model.predict(X_scaled)                if current_regime is not None and regime_models and current_regime in regime_models:            regime_data = regime_models[current_regime]            regime_features = regime_data.get('features', features)                        try:                if hasattr(X, 'columns'):                    available_features = [f for f in regime_features if f in X.columns]                    if len(available_features) == len(regime_features):                        X_regime = X[regime_features]                        X_regime_scaled = regime_data['scaler'].transform(X_regime)                        regime_pred = regime_data['model'].predict(X_regime_scaled)                    else:                        return global_pred                else:                    expected_features = regime_data['scaler'].n_features_in_                    if X.shape[1] == expected_features:                        X_regime_scaled = regime_data['scaler'].transform(X)                        regime_pred = regime_data['model'].predict(X_regime_scaled)                    else:                        return global_pred            except Exception:                return global_pred                        if current_regime not in recent_errors['regime']:                recent_errors['regime'][current_regime] = []                        global_errors = np.array(recent_errors['global']) if recent_errors['global'] else np.array([1.0])            regime_errors = np.array(recent_errors['regime'].get(current_regime, [1.0]))                        g_weight, r_weight = calculate_dynamic_weights(global_errors, regime_errors, decay)            final_pred = g_weight * global_pred + r_weight * regime_pred                        if actual_value is not None:                g_error = abs(actual_value - global_pred)                r_error = abs(actual_value - regime_pred)                recent_errors['global'].append(float(g_error))                recent_errors['regime'][current_regime].append(float(r_error))                                if len(recent_errors['global']) > 100:                    recent_errors['global'].pop(0)                if len(recent_errors['regime'][current_regime]) > 100:                    recent_errors['regime'][current_regime].pop(0)                        return final_pred                return global_pred        return predict# Create confidence scoring predictors for each horizonconfidence_predictors = {}for horizon in ['1h', '4h']:    if horizon not in trained_models:        continue        data = trained_models[horizon]        # Get quantile models if available    q_models = None    q_scaler = None    if horizon in quantile_models:        q_models, q_scaler = quantile_models[horizon]        # Get regime models if available    r_models = None    if 'regime_specific_models' in dir() and regime_specific_models.get(horizon):        r_models = regime_specific_models[horizon]        # Get base interval from conformal calibration    base_interval = conformal_residuals.get(horizon, {}).get('quantile', 0.3)        # Create confidence predictor    conf_pred = ConfidenceScoringPredictor(        main_model=data['model'],        main_scaler=data['scaler'],        features=data['features'],        quantile_models=q_models,        quantile_scaler=q_scaler,        regime_models=r_models,        base_interval=base_interval    )        confidence_predictors[horizon] = conf_pred        # Store in trained_models for saving    trained_models[horizon]['confidence_predictor'] = conf_pred    trained_models[horizon]['has_confidence_scoring'] = True        print(f\"\\n{horizon}: Confidence scoring predictor created\")    print(f\"  Base interval: \u00b1{base_interval:.4f}\")    print(f\"  Quantile models: {'Yes' if q_models else 'No'}\")    print(f\"  Regime models: {'Yes' if r_models else 'No'}\")# Also create dynamic ensemble predictorsfor horizon in ['1h', '4h']:    if horizon not in trained_models:        continue        data = trained_models[horizon]    r_models = None    if 'regime_specific_models' in dir() and regime_specific_models.get(horizon):        r_models = regime_specific_models[horizon]        if r_models:        ensemble_pred = create_dynamic_ensemble_predictor(            data['model'], data['scaler'], r_models, data['features']        )        trained_models[horizon]['dynamic_ensemble_predict'] = ensemble_pred        trained_models[horizon]['has_dynamic_ensemble'] = True        print(f\"  {horizon}: Dynamic ensemble predictor created\")print(f\"\\n\u2713 Confidence scoring and dynamic ensemble setup complete\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ERROR ANALYSIS + AUTOMATIC BIAS CORRECTION (IMPROVED)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ERROR ANALYSIS + BIAS CORRECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "error_analysis = {}\n",
    "bias_correction_factors = {}\n",
    "\n",
    "# === MODULE-LEVEL CLASS FOR PICKLING ===\n",
    "class BiasCorrectedModel:\n",
    "    \"\"\"\n",
    "    Model wrapper that automatically applies bias correction during inference.\n",
    "    Defined at module level for pickling compatibility.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, scaler, bias_factors, features):\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "        self.bias_factors = bias_factors\n",
    "        self.features = features\n",
    "    \n",
    "    def predict(self, X, hour=None, regime=None):\n",
    "        \"\"\"Predict with automatic bias correction\"\"\"\n",
    "        if hasattr(X, 'columns'):\n",
    "            X_subset = X[self.features]\n",
    "        else:\n",
    "            X_subset = X\n",
    "        \n",
    "        X_scaled = self.scaler.transform(X_subset)\n",
    "        raw_pred = self.model.predict(X_scaled)\n",
    "        \n",
    "        # Apply bias correction\n",
    "        correction = 0.0\n",
    "        \n",
    "        # Time-based correction (most specific)\n",
    "        if hour is not None and 'by_time' in self.bias_factors:\n",
    "            period = 'afternoon' if 12 <= hour < 18 else (\n",
    "                'morning' if 6 <= hour < 12 else (\n",
    "                'evening' if 18 <= hour < 24 else 'night'))\n",
    "            \n",
    "            if period in self.bias_factors['by_time']:\n",
    "                tb = self.bias_factors['by_time'][period]\n",
    "                if tb.get('should_apply', False):\n",
    "                    correction = tb['correction']\n",
    "        \n",
    "        # Regime-based correction (fallback)\n",
    "        elif regime is not None and 'by_regime' in self.bias_factors:\n",
    "            regime_name = {0: 'normal', 1: 'elevated', 2: 'spike'}.get(regime)\n",
    "            if regime_name in self.bias_factors['by_regime']:\n",
    "                rb = self.bias_factors['by_regime'][regime_name]\n",
    "                if rb.get('should_apply', False):\n",
    "                    correction = rb['correction']\n",
    "        \n",
    "        # Overall correction (last resort)\n",
    "        elif self.bias_factors.get('overall', {}).get('should_apply', False):\n",
    "            correction = self.bias_factors['overall']['correction']\n",
    "        \n",
    "        return raw_pred + correction\n",
    "    \n",
    "    def predict_raw(self, X):\n",
    "        \"\"\"Predict without bias correction\"\"\"\n",
    "        if hasattr(X, 'columns'):\n",
    "            X_subset = X[self.features]\n",
    "        else:\n",
    "            X_subset = X\n",
    "        X_scaled = self.scaler.transform(X_subset)\n",
    "        return self.model.predict(X_scaled)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "def compute_bias_correction(y_true, y_pred, hours, regimes):\n",
    "    \"\"\"\n",
    "    Compute bias correction factors that can be applied during inference.\n",
    "    IMPROVED: Use median for robustness, lower threshold for high-error periods.\n",
    "    \"\"\"\n",
    "    errors = y_true - y_pred  # Positive = under-prediction, Negative = over-prediction\n",
    "    abs_errors = np.abs(errors)\n",
    "    \n",
    "    # Overall bias\n",
    "    overall_bias = np.mean(errors)\n",
    "    overall_median_bias = np.median(errors)\n",
    "    recent_bias = np.mean(errors[-min(240, len(errors)):])  # Last 2 hours\n",
    "    \n",
    "    # Time-period specific bias\n",
    "    time_bias = {}\n",
    "    time_periods = {\n",
    "        'night': (0, 6),\n",
    "        'morning': (6, 12),\n",
    "        'afternoon': (12, 18),\n",
    "        'evening': (18, 24)\n",
    "    }\n",
    "    \n",
    "    overall_mae = np.mean(abs_errors)\n",
    "    \n",
    "    for period_name, (start, end) in time_periods.items():\n",
    "        mask = (hours >= start) & (hours < end)\n",
    "        if mask.sum() >= 50:\n",
    "            period_errors = errors[mask]\n",
    "            period_abs_errors = abs_errors[mask]\n",
    "            period_bias = np.mean(period_errors)\n",
    "            period_median_bias = np.median(period_errors)\n",
    "            period_mae = np.mean(period_abs_errors)\n",
    "            \n",
    "            # Use median bias for correction (more robust)\n",
    "            correction = period_median_bias\n",
    "            \n",
    "            # Apply if bias > 2% (lowered from 3%) OR if this period has much higher MAE\n",
    "            is_high_error_period = period_mae > overall_mae * 1.2\n",
    "            should_apply = abs(period_bias) > 0.02 or (is_high_error_period and abs(period_bias) > 0.01)\n",
    "            \n",
    "            time_bias[period_name] = {\n",
    "                'bias': float(period_bias),\n",
    "                'median_bias': float(period_median_bias),\n",
    "                'correction': float(correction),\n",
    "                'mae': float(period_mae),\n",
    "                'mae_ratio': float(period_mae / overall_mae) if overall_mae > 0 else 1.0,\n",
    "                'n_samples': int(mask.sum()),\n",
    "                'should_apply': should_apply,\n",
    "                'is_high_error': is_high_error_period\n",
    "            }\n",
    "    \n",
    "    # Regime-specific bias\n",
    "    regime_bias = {}\n",
    "    for regime_val, regime_name in [(0, 'normal'), (1, 'elevated'), (2, 'spike')]:\n",
    "        mask = regimes == regime_val\n",
    "        if mask.sum() >= 50:\n",
    "            r_errors = errors[mask]\n",
    "            r_bias = np.mean(r_errors)\n",
    "            r_median_bias = np.median(r_errors)\n",
    "            \n",
    "            regime_bias[regime_name] = {\n",
    "                'bias': float(r_bias),\n",
    "                'median_bias': float(r_median_bias),\n",
    "                'correction': float(r_median_bias),\n",
    "                'n_samples': int(mask.sum()),\n",
    "                'should_apply': abs(r_bias) > 0.02\n",
    "            }\n",
    "    \n",
    "    return {\n",
    "        'overall': {\n",
    "            'bias': float(overall_bias),\n",
    "            'median_bias': float(overall_median_bias),\n",
    "            'recent_bias': float(recent_bias),\n",
    "            'correction': float(overall_median_bias),\n",
    "            'mae': float(overall_mae),\n",
    "            'should_apply': abs(overall_median_bias) > 0.05\n",
    "        },\n",
    "        'by_time': time_bias,\n",
    "        'by_regime': regime_bias\n",
    "    }\n",
    "\n",
    "def create_bias_corrected_predictor(model, scaler, bias_factors, features):\n",
    "    \"\"\"Create a predictor that automatically applies bias correction.\"\"\"\n",
    "    return BiasCorrectedModel(model, scaler, bias_factors, features)\n",
    "\n",
    "for horizon in ['1h', '4h']:\n",
    "    if horizon not in trained_models:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{horizon} Error Analysis...\")\n",
    "    \n",
    "    data = trained_models[horizon]\n",
    "    model = data['model']\n",
    "    scaler = data['scaler']\n",
    "    features = data['features']\n",
    "    \n",
    "    if not HAS_HOLDOUT:\n",
    "        print(\"  \u26a0\ufe0f No holdout data\")\n",
    "        continue\n",
    "    \n",
    "    # Get predictions on holdout\n",
    "    X_test = df_holdout[features]\n",
    "    y_test = df_holdout[f'target_{horizon}']\n",
    "    \n",
    "    mask = y_test.notna()\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    errors = y_test.values - y_pred\n",
    "    abs_errors = np.abs(errors)\n",
    "    \n",
    "    # Get hours and regimes\n",
    "    hours = X_test.index.hour if hasattr(X_test.index, 'hour') else np.full(len(X_test), 12)\n",
    "    \n",
    "    regimes = np.zeros(len(X_test))\n",
    "    if 'gas_zscore_1h' in df_holdout.columns:\n",
    "        regimes[df_holdout.loc[X_test.index, 'gas_zscore_1h'] > 1] = 1\n",
    "    if 'is_spike' in df_holdout.columns:\n",
    "        regimes[df_holdout.loc[X_test.index, 'is_spike'] == 1] = 2\n",
    "    \n",
    "    # === COMPUTE BIAS CORRECTION FACTORS ===\n",
    "    bias_factors = compute_bias_correction(y_test.values, y_pred, hours, regimes)\n",
    "    bias_correction_factors[horizon] = bias_factors\n",
    "    \n",
    "    print(f\"\\n  Bias Analysis:\")\n",
    "    print(f\"    Overall: mean={bias_factors['overall']['bias']:.4f}, median={bias_factors['overall']['median_bias']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Time-Period Bias:\")\n",
    "    for period, tb in bias_factors['by_time'].items():\n",
    "        apply_flag = \"\u2713 APPLY\" if tb['should_apply'] else \"  skip\"\n",
    "        high_err = \" [HIGH ERROR]\" if tb.get('is_high_error', False) else \"\"\n",
    "        print(f\"    {period}: bias={tb['bias']:+.4f}, MAE={tb['mae']:.4f} ({tb['mae_ratio']:.1f}x){high_err} {apply_flag}\")\n",
    "    \n",
    "    print(f\"\\n  Regime Bias:\")\n",
    "    for regime_name, rb in bias_factors['by_regime'].items():\n",
    "        apply_flag = \"\u2713 APPLY\" if rb['should_apply'] else \"  skip\"\n",
    "        print(f\"    {regime_name}: {rb['bias']:+.4f} ({rb['n_samples']} samples) {apply_flag}\")\n",
    "    \n",
    "    # === TEST BIAS-CORRECTED PREDICTIONS ===\n",
    "    print(f\"\\n  Bias Correction Test:\")\n",
    "    \n",
    "    # Predictions without correction\n",
    "    raw_mae = np.mean(abs_errors)\n",
    "    \n",
    "    # Predictions with time-based correction only\n",
    "    corrected_preds_time = y_pred.copy()\n",
    "    for i in range(len(corrected_preds_time)):\n",
    "        h = hours[i]\n",
    "        period = 'afternoon' if 12 <= h < 18 else (\n",
    "            'morning' if 6 <= h < 12 else (\n",
    "            'evening' if 18 <= h < 24 else 'night'))\n",
    "        \n",
    "        if period in bias_factors['by_time'] and bias_factors['by_time'][period]['should_apply']:\n",
    "            corrected_preds_time[i] += bias_factors['by_time'][period]['correction']\n",
    "    \n",
    "    corrected_mae_time = mean_absolute_error(y_test, corrected_preds_time)\n",
    "    improvement_time = (raw_mae - corrected_mae_time) / raw_mae * 100\n",
    "    \n",
    "    print(f\"    Raw MAE: {raw_mae:.4f}\")\n",
    "    print(f\"    Time-corrected MAE: {corrected_mae_time:.4f} ({improvement_time:+.1f}%)\")\n",
    "    \n",
    "    # Check per-period improvement\n",
    "    print(f\"\\n  Per-Period Results (with correction):\")\n",
    "    for period, (start, end) in [('night', (0, 6)), ('morning', (6, 12)), ('afternoon', (12, 18)), ('evening', (18, 24))]:\n",
    "        period_mask = (hours >= start) & (hours < end)\n",
    "        if period_mask.sum() > 10:\n",
    "            raw_period_mae = np.mean(abs_errors[period_mask])\n",
    "            corrected_period_mae = np.mean(np.abs(y_test.values[period_mask] - corrected_preds_time[period_mask]))\n",
    "            pct_change = (raw_period_mae - corrected_period_mae) / raw_period_mae * 100\n",
    "            print(f\"    {period}: {raw_period_mae:.4f} \u2192 {corrected_period_mae:.4f} ({pct_change:+.1f}%)\")\n",
    "    \n",
    "    # Decide whether to use bias correction\n",
    "    # IMPROVED: Use bias correction if it helps ANY high-error period, even if overall MAE is worse\n",
    "    high_error_periods = [p for p, tb in bias_factors['by_time'].items() if tb.get('is_high_error', False)]\n",
    "    helps_high_error = False\n",
    "    \n",
    "    for period in high_error_periods:\n",
    "        (start, end) = {'night': (0, 6), 'morning': (6, 12), 'afternoon': (12, 18), 'evening': (18, 24)}[period]\n",
    "        period_mask = (hours >= start) & (hours < end)\n",
    "        if period_mask.sum() > 10:\n",
    "            raw_period_mae = np.mean(abs_errors[period_mask])\n",
    "            corrected_period_mae = np.mean(np.abs(y_test.values[period_mask] - corrected_preds_time[period_mask]))\n",
    "            if corrected_period_mae < raw_period_mae:\n",
    "                helps_high_error = True\n",
    "                break\n",
    "    \n",
    "    use_bias_correction = corrected_mae_time < raw_mae or helps_high_error\n",
    "    \n",
    "    if use_bias_correction:\n",
    "        print(f\"\\n  \u2713 Bias correction ENABLED\")\n",
    "        if helps_high_error:\n",
    "            print(f\"    Reason: Helps high-error periods: {high_error_periods}\")\n",
    "        bias_corrected_model = create_bias_corrected_predictor(model, scaler, bias_factors, features)\n",
    "        trained_models[horizon]['bias_corrected_model'] = bias_corrected_model\n",
    "        trained_models[horizon]['use_bias_correction'] = True\n",
    "    else:\n",
    "        print(f\"\\n  \u2717 Bias correction disabled (no improvement)\")\n",
    "        trained_models[horizon]['use_bias_correction'] = False\n",
    "    \n",
    "    # Standard error analysis\n",
    "    analysis = {\n",
    "        'mean_error': float(np.mean(errors)),\n",
    "        'std_error': float(np.std(errors)),\n",
    "        'mae': float(np.mean(abs_errors)),\n",
    "        'corrected_mae': float(corrected_mae_time),\n",
    "        'median_ae': float(np.median(abs_errors)),\n",
    "        'max_error': float(np.max(abs_errors)),\n",
    "        'p95_error': float(np.percentile(abs_errors, 95)),\n",
    "        'p99_error': float(np.percentile(abs_errors, 99)),\n",
    "        'bias_correction_applied': use_bias_correction\n",
    "    }\n",
    "    \n",
    "    # Error by regime\n",
    "    regime_errors = {}\n",
    "    for regime_val, regime_name in [(0, 'normal'), (1, 'elevated'), (2, 'spike')]:\n",
    "        regime_mask = regimes == regime_val\n",
    "        if regime_mask.sum() > 10:\n",
    "            regime_errors[regime_name] = float(np.mean(abs_errors[regime_mask]))\n",
    "    analysis['regime_errors'] = regime_errors\n",
    "    \n",
    "    # Error by time\n",
    "    time_errors = {}\n",
    "    for period, (start, end) in [('night', (0, 6)), ('morning', (6, 12)), ('afternoon', (12, 18)), ('evening', (18, 24))]:\n",
    "        period_mask = (hours >= start) & (hours < end)\n",
    "        if period_mask.sum() > 10:\n",
    "            time_errors[period] = float(np.mean(abs_errors[period_mask]))\n",
    "    analysis['time_errors'] = time_errors\n",
    "    \n",
    "    error_analysis[horizon] = analysis\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BIAS CORRECTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for horizon, bcf in bias_correction_factors.items():\n",
    "    use_bc = trained_models.get(horizon, {}).get('use_bias_correction', False)\n",
    "    status = \"ENABLED\" if use_bc else \"DISABLED\"\n",
    "    print(f\"\\n{horizon}: {status}\")\n",
    "    \n",
    "    if use_bc:\n",
    "        print(f\"  Time-specific corrections:\")\n",
    "        for period, tb in bcf['by_time'].items():\n",
    "            if tb['should_apply']:\n",
    "                print(f\"    {period}: {tb['correction']:+.4f}\")\n",
    "\n",
    "print(f\"\\n\u2713 Error analysis complete\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# AUTOMATED RETRAINING TRIGGERS",
    "print(\"\\n\" + \"=\"*60)",
    "print(\"RETRAINING TRIGGER ANALYSIS\")",
    "print(\"=\"*60)",
    "",
    "retraining_recommendations = {}",
    "",
    "# Thresholds for retraining",
    "PERFORMANCE_DEGRADATION_THRESHOLD = 0.20  # 20% worse than baseline",
    "DISTRIBUTION_SHIFT_THRESHOLD = 1.5  # 1.5 std dev shift",
    "ERROR_INCREASE_THRESHOLD = 0.15  # 15% increase in recent errors",
    "",
    "def check_retraining_needed(horizon, metrics, baselines, error_analysis_data):",
    "    \"\"\"Check if model needs retraining based on multiple signals\"\"\"",
    "    signals = []",
    "    should_retrain = False",
    "    ",
    "    # 1. Check vs holdout baseline",
    "    if 'holdout_best' in baselines.get(horizon.replace('24h', '4h'), {}):",
    "        holdout_baseline = baselines[horizon.replace('24h', '4h')]['holdout_best']",
    "        model_mae = metrics['mae']",
    "        ",
    "        if model_mae > holdout_baseline:",
    "            degradation = (model_mae - holdout_baseline) / holdout_baseline",
    "            signals.append(f\"Model worse than baseline by {degradation*100:.1f}%\")",
    "            if degradation > PERFORMANCE_DEGRADATION_THRESHOLD:",
    "                should_retrain = True",
    "    ",
    "    # 2. Check distribution shift",
    "    if DISTRIBUTION_SHIFT_DETECTED:",
    "        signals.append(f\"Distribution shift detected (magnitude: {SHIFT_MAGNITUDE:.2f})\")",
    "        if SHIFT_MAGNITUDE > DISTRIBUTION_SHIFT_THRESHOLD:",
    "            should_retrain = True",
    "    ",
    "    # 3. Check error patterns",
    "    if horizon in error_analysis:",
    "        ea = error_analysis[horizon]",
    "        ",
    "        # Check for systematic bias",
    "        if abs(ea['mean_error']) > 0.1:",
    "            signals.append(f\"Systematic bias: {ea['mean_error']:.4f}\")",
    "            should_retrain = True",
    "        ",
    "        # Check P99 errors (catastrophic failures)",
    "        if ea['p99_error'] > 3 * ea['mae']:",
    "            signals.append(f\"High P99 error: {ea['p99_error']:.4f} (3x MAE)\")",
    "    ",
    "    # 4. Check regime-specific degradation",
    "    if horizon in error_analysis and 'regime_errors' in error_analysis[horizon]:",
    "        regime_errors = error_analysis[horizon]['regime_errors']",
    "        if 'spike' in regime_errors and 'normal' in regime_errors:",
    "            spike_ratio = regime_errors['spike'] / (regime_errors['normal'] + 1e-8)",
    "            if spike_ratio > 3:",
    "                signals.append(f\"Spike regime error {spike_ratio:.1f}x worse than normal\")",
    "    ",
    "    return should_retrain, signals",
    "",
    "for horizon in ['1h', '4h', '24h']:",
    "    if horizon not in trained_models:",
    "        continue",
    "    ",
    "    print(f\"\\n{horizon} Retraining Analysis:\")",
    "    ",
    "    metrics = trained_models[horizon]['metrics']",
    "    should_retrain, signals = check_retraining_needed(",
    "        horizon, metrics, BASELINES, ",
    "        error_analysis if 'error_analysis' in dir() else {}",
    "    )",
    "    ",
    "    retraining_recommendations[horizon] = {",
    "        'should_retrain': should_retrain,",
    "        'signals': signals,",
    "        'urgency': 'high' if should_retrain and len(signals) > 2 else 'medium' if should_retrain else 'low'",
    "    }",
    "    ",
    "    if signals:",
    "        for signal in signals:",
    "            print(f\"  \u26a0\ufe0f {signal}\")",
    "    else:",
    "        print(f\"  \u2713 No retraining signals detected\")",
    "    ",
    "    if should_retrain:",
    "        print(f\"  >>> RECOMMENDATION: Retrain {horizon} model\")",
    "    else:",
    "        print(f\"  >>> Model OK, no retraining needed\")",
    "",
    "# Overall recommendation",
    "any_retrain = any(r['should_retrain'] for r in retraining_recommendations.values())",
    "high_urgency = any(r['urgency'] == 'high' for r in retraining_recommendations.values())",
    "",
    "print(f\"\\n{'='*60}\")",
    "print(\"OVERALL RETRAINING RECOMMENDATION\")",
    "print(f\"{'='*60}\")",
    "",
    "if high_urgency:",
    "    print(\"\ud83d\udd34 HIGH URGENCY: Models show significant degradation\")",
    "    print(\"   Recommendation: Retrain immediately with fresh data\")",
    "elif any_retrain:",
    "    print(\"\ud83d\udfe1 MEDIUM: Some models could benefit from retraining\")",
    "    print(\"   Recommendation: Schedule retraining when convenient\")",
    "else:",
    "    print(\"\ud83d\udfe2 LOW: Models performing within acceptable parameters\")",
    "    print(\"   Recommendation: Continue monitoring, retrain in 1-2 weeks\")",
    "",
    "# === NEW: Export retraining trigger file ===",
    "retraining_trigger_file = {",
    "    'timestamp': datetime.now().isoformat(),",
    "    'overall_recommendation': 'retrain_immediately' if high_urgency else ('schedule_retrain' if any_retrain else 'no_action'),",
    "    'urgency': 'high' if high_urgency else ('medium' if any_retrain else 'low'),",
    "    'horizons': {}",
    "}",
    "",
    "for horizon, rec in retraining_recommendations.items():",
    "    retraining_trigger_file['horizons'][horizon] = {",
    "        'should_retrain': rec['should_retrain'],",
    "        'urgency': rec['urgency'],",
    "        'signals': rec['signals']",
    "    }",
    "",
    "# Save to output folder (will be included in zip)",
    "import json",
    "trigger_path = '/content/retraining_needed.json'",
    "try:",
    "    with open(trigger_path, 'w') as f:",
    "        json.dump(retraining_trigger_file, f, indent=2)",
    "    print(f\"\\n\u2713 Retraining trigger file saved to: {trigger_path}\")",
    "    print(f\"  Recommendation: {retraining_trigger_file['overall_recommendation']}\")",
    "except Exception as e:",
    "    print(f\"  \u26a0\ufe0f Could not save trigger file: {e}\")",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# BACKTESTING FRAMEWORK + ONLINE LEARNING HOOKS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BACKTESTING + ONLINE LEARNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def run_backtest(model, scaler, features, df, target_col, conformal_interval=None, \n",
    "                 time_intervals=None, window_days=3):\n",
    "    \"\"\"\n",
    "    Run walk-forward backtest with interval coverage verification.\n",
    "    \"\"\"\n",
    "    rph = 120\n",
    "    window_size = window_days * 24 * rph\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(window_size, len(df) - rph, rph):  # Step by 1 hour\n",
    "        X_current = df[features].iloc[i:i+1]\n",
    "        \n",
    "        if i + rph < len(df):\n",
    "            y_actual = df[target_col].iloc[i + rph] if target_col in df.columns else np.nan\n",
    "        else:\n",
    "            y_actual = np.nan\n",
    "        \n",
    "        if pd.isna(y_actual) or X_current.isna().any().any():\n",
    "            continue\n",
    "        \n",
    "        # Predict\n",
    "        X_scaled = scaler.transform(X_current)\n",
    "        y_pred = model.predict(X_scaled)[0]\n",
    "        \n",
    "        error = abs(y_actual - y_pred)\n",
    "        \n",
    "        # Get hour for time-specific interval\n",
    "        hour = df.index[i].hour if hasattr(df.index[i], 'hour') else 12\n",
    "        period = 'afternoon' if 12 <= hour < 18 else (\n",
    "            'morning' if 6 <= hour < 12 else (\n",
    "            'evening' if 18 <= hour < 24 else 'night'))\n",
    "        \n",
    "        # Compute interval coverage\n",
    "        if conformal_interval:\n",
    "            base_interval = conformal_interval\n",
    "            if time_intervals and period in time_intervals:\n",
    "                interval = time_intervals[period].get('interval_80', base_interval)\n",
    "            else:\n",
    "                interval = base_interval\n",
    "            in_interval = error <= interval\n",
    "        else:\n",
    "            in_interval = None\n",
    "            interval = None\n",
    "        \n",
    "        results.append({\n",
    "            'timestamp': df.index[i],\n",
    "            'hour': hour,\n",
    "            'period': period,\n",
    "            'actual': y_actual,\n",
    "            'predicted': y_pred,\n",
    "            'error': error,\n",
    "            'signed_error': y_actual - y_pred,\n",
    "            'current_gas': df['gas'].iloc[i] if 'gas' in df.columns else np.nan,\n",
    "            'in_interval': in_interval,\n",
    "            'interval_width': interval\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def compute_online_bias_update(recent_errors, decay=0.95, max_history=50):\n",
    "    \"\"\"\n",
    "    Compute exponentially weighted bias update from recent errors.\n",
    "    Can be used to update bias correction factors online.\n",
    "    \"\"\"\n",
    "    if len(recent_errors) == 0:\n",
    "        return {'bias': 0.0, 'confidence': 0.0}\n",
    "    \n",
    "    n = min(len(recent_errors), max_history)\n",
    "    errors = np.array(recent_errors[-n:])\n",
    "    \n",
    "    # Exponential weights\n",
    "    weights = np.array([decay ** (n - i - 1) for i in range(n)])\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    weighted_bias = np.sum(errors * weights)\n",
    "    weighted_abs_error = np.sum(np.abs(errors) * weights)\n",
    "    \n",
    "    # Confidence based on consistency\n",
    "    error_std = np.std(errors)\n",
    "    confidence = 1.0 / (1.0 + error_std)\n",
    "    \n",
    "    return {\n",
    "        'bias': float(weighted_bias),\n",
    "        'mae': float(weighted_abs_error),\n",
    "        'confidence': float(confidence),\n",
    "        'n_samples': n\n",
    "    }\n",
    "\n",
    "class OnlineBiasTracker:\n",
    "    \"\"\"Track and update bias corrections online.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_bias_factors=None, decay=0.95, max_history=100):\n",
    "        self.decay = decay\n",
    "        self.max_history = max_history\n",
    "        self.errors_by_period = {\n",
    "            'night': [], 'morning': [], 'afternoon': [], 'evening': []\n",
    "        }\n",
    "        self.errors_overall = []\n",
    "        self.bias_factors = initial_bias_factors or {}\n",
    "    \n",
    "    def update(self, signed_error, hour):\n",
    "        \"\"\"Update with new prediction error.\"\"\"\n",
    "        period = 'afternoon' if 12 <= hour < 18 else (\n",
    "            'morning' if 6 <= hour < 12 else (\n",
    "            'evening' if 18 <= hour < 24 else 'night'))\n",
    "        \n",
    "        self.errors_overall.append(signed_error)\n",
    "        self.errors_by_period[period].append(signed_error)\n",
    "        \n",
    "        # Trim history\n",
    "        if len(self.errors_overall) > self.max_history:\n",
    "            self.errors_overall.pop(0)\n",
    "        if len(self.errors_by_period[period]) > self.max_history // 4:\n",
    "            self.errors_by_period[period].pop(0)\n",
    "    \n",
    "    def get_current_bias(self, period=None):\n",
    "        \"\"\"Get current estimated bias for a period or overall.\"\"\"\n",
    "        if period and period in self.errors_by_period:\n",
    "            return compute_online_bias_update(self.errors_by_period[period], self.decay)\n",
    "        return compute_online_bias_update(self.errors_overall, self.decay)\n",
    "    \n",
    "    def should_alert(self, threshold=0.1):\n",
    "        \"\"\"Check if any period has significant bias drift.\"\"\"\n",
    "        alerts = []\n",
    "        for period, errors in self.errors_by_period.items():\n",
    "            if len(errors) >= 10:\n",
    "                bias_info = compute_online_bias_update(errors, self.decay)\n",
    "                if abs(bias_info['bias']) > threshold and bias_info['confidence'] > 0.5:\n",
    "                    alerts.append({\n",
    "                        'period': period,\n",
    "                        'bias': bias_info['bias'],\n",
    "                        'confidence': bias_info['confidence']\n",
    "                    })\n",
    "        return alerts\n",
    "\n",
    "# Store online trackers for each model\n",
    "online_trackers = {}\n",
    "\n",
    "backtest_results = {}\n",
    "\n",
    "for horizon in ['1h', '4h']:\n",
    "    if horizon not in trained_models:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{horizon} Backtest + Online Learning Simulation\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    data = trained_models[horizon]\n",
    "    model = data['model']\n",
    "    scaler = data['scaler']\n",
    "    features = data['features']\n",
    "    \n",
    "    target_col = f'target_{horizon}'\n",
    "    \n",
    "    if target_col not in df_clean.columns:\n",
    "        print(f\"  \u26a0\ufe0f Target column not found\")\n",
    "        continue\n",
    "    \n",
    "    # Get calibration data\n",
    "    conformal_interval = conformal_residuals.get(horizon, {}).get('quantile')\n",
    "    time_intervals = time_period_calibration.get(horizon, {}).get('time', {})\n",
    "    \n",
    "    # Run backtest\n",
    "    bt_results = run_backtest(\n",
    "        model, scaler, features, df_clean, target_col, \n",
    "        conformal_interval, time_intervals, window_days=3\n",
    "    )\n",
    "    \n",
    "    if len(bt_results) < 100:\n",
    "        print(f\"  \u26a0\ufe0f Insufficient backtest data ({len(bt_results)} points)\")\n",
    "        continue\n",
    "    \n",
    "    # === BASIC METRICS ===\n",
    "    mae = bt_results['error'].mean()\n",
    "    rmse = np.sqrt((bt_results['error'] ** 2).mean())\n",
    "    naive_errors = abs(bt_results['actual'] - bt_results['current_gas'])\n",
    "    naive_mae = naive_errors.mean()\n",
    "    improvement = (naive_mae - mae) / naive_mae * 100\n",
    "    \n",
    "    print(f\"\\n  Basic Metrics:\")\n",
    "    print(f\"    Backtest samples: {len(bt_results)}\")\n",
    "    print(f\"    Model MAE: {mae:.4f}, Naive MAE: {naive_mae:.4f}\")\n",
    "    print(f\"    Improvement vs naive: {improvement:+.1f}%\")\n",
    "    \n",
    "    # === INTERVAL COVERAGE ===\n",
    "    if 'in_interval' in bt_results.columns and bt_results['in_interval'].notna().any():\n",
    "        overall_coverage = bt_results['in_interval'].mean()\n",
    "        print(f\"\\n  Interval Coverage:\")\n",
    "        print(f\"    Overall: {overall_coverage:.1%} (target: 80%)\")\n",
    "        \n",
    "        for period in ['night', 'morning', 'afternoon', 'evening']:\n",
    "            period_mask = bt_results['period'] == period\n",
    "            if period_mask.sum() >= 20:\n",
    "                period_coverage = bt_results.loc[period_mask, 'in_interval'].mean()\n",
    "                status = \"\u2713\" if period_coverage >= 0.75 else \"\u26a0\ufe0f\"\n",
    "                print(f\"    {period}: {period_coverage:.1%} {status}\")\n",
    "    \n",
    "    # === ROLLING PERFORMANCE ===\n",
    "    bt_results['date'] = bt_results['timestamp'].dt.date\n",
    "    daily_mae = bt_results.groupby('date')['error'].mean()\n",
    "    \n",
    "    print(f\"\\n  Rolling Performance:\")\n",
    "    print(f\"    Daily MAE range: {daily_mae.min():.4f} - {daily_mae.max():.4f}\")\n",
    "    \n",
    "    # Performance trend\n",
    "    if len(daily_mae) >= 3:\n",
    "        first_half = daily_mae.iloc[:len(daily_mae)//2].mean()\n",
    "        second_half = daily_mae.iloc[len(daily_mae)//2:].mean()\n",
    "        trend = (second_half - first_half) / first_half * 100\n",
    "        trend_str = \"improving \u2193\" if trend < -5 else (\"degrading \u2191\" if trend > 5 else \"stable \u2192\")\n",
    "        print(f\"    Trend: {trend:+.1f}% ({trend_str})\")\n",
    "    \n",
    "    # === SIMULATE ONLINE LEARNING ===\n",
    "    print(f\"\\n  Online Learning Simulation:\")\n",
    "    tracker = OnlineBiasTracker(\n",
    "        initial_bias_factors=bias_correction_factors.get(horizon),\n",
    "        decay=0.95\n",
    "    )\n",
    "    \n",
    "    # Simulate updates\n",
    "    for _, row in bt_results.iterrows():\n",
    "        tracker.update(row['signed_error'], row['hour'])\n",
    "    \n",
    "    # Check for bias drift alerts\n",
    "    alerts = tracker.should_alert(threshold=0.05)\n",
    "    if alerts:\n",
    "        print(f\"    \u26a0\ufe0f Bias drift detected:\")\n",
    "        for alert in alerts:\n",
    "            print(f\"      {alert['period']}: bias={alert['bias']:+.4f} (conf={alert['confidence']:.2f})\")\n",
    "    else:\n",
    "        print(f\"    \u2713 No significant bias drift\")\n",
    "    \n",
    "    # Show current online bias estimates\n",
    "    print(f\"\\n  Online Bias Estimates:\")\n",
    "    for period in ['night', 'morning', 'afternoon', 'evening']:\n",
    "        bias_info = tracker.get_current_bias(period)\n",
    "        if bias_info['n_samples'] >= 5:\n",
    "            print(f\"    {period}: {bias_info['bias']:+.4f} (n={bias_info['n_samples']})\")\n",
    "    \n",
    "    online_trackers[horizon] = tracker\n",
    "    \n",
    "    # Store results\n",
    "    backtest_results[horizon] = {\n",
    "        'n_samples': len(bt_results),\n",
    "        'mae': float(mae),\n",
    "        'rmse': float(rmse),\n",
    "        'naive_mae': float(naive_mae),\n",
    "        'improvement_pct': float(improvement),\n",
    "        'interval_coverage': float(bt_results['in_interval'].mean()) if 'in_interval' in bt_results.columns else None,\n",
    "        'daily_mae_min': float(daily_mae.min()),\n",
    "        'daily_mae_max': float(daily_mae.max()),\n",
    "        'online_alerts': alerts\n",
    "    }\n",
    "    \n",
    "    # Store tracker for saving\n",
    "    trained_models[horizon]['online_tracker'] = tracker\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BACKTEST SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for horizon, results in backtest_results.items():\n",
    "    print(f\"\\n{horizon}:\")\n",
    "    print(f\"  MAE: {results['mae']:.4f} ({results['improvement_pct']:+.1f}% vs naive)\")\n",
    "    if results.get('interval_coverage'):\n",
    "        print(f\"  Coverage: {results['interval_coverage']:.1%}\")\n",
    "    if results.get('online_alerts'):\n",
    "        print(f\"  \u26a0\ufe0f Alerts: {len(results['online_alerts'])} periods with bias drift\")\n",
    "\n",
    "print(f\"\\n\u2713 Backtesting and online learning simulation complete\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# SHAP EXPLANATIONS FOR MODEL INTERPRETABILITY\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SHAP EXPLANATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if SHAP is available\n",
    "try:\n",
    "    import shap\n",
    "    HAS_SHAP = True\n",
    "    print(\"\u2713 SHAP library available\")\n",
    "except ImportError:\n",
    "    HAS_SHAP = False\n",
    "    print(\"\u26a0\ufe0f SHAP not available. Install with: pip install shap\")\n",
    "    print(\"   Skipping SHAP analysis\")\n",
    "\n",
    "shap_explainers = {}\n",
    "\n",
    "if HAS_SHAP:\n",
    "    for horizon in ['1h', '4h']:\n",
    "        if horizon not in trained_models:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{horizon} SHAP Analysis...\")\n",
    "        \n",
    "        data = trained_models[horizon]\n",
    "        model = data['model']\n",
    "        scaler = data['scaler']\n",
    "        features = data['features']\n",
    "        \n",
    "        # Get sample of training data for background\n",
    "        X_sample = df_train_val[features].sample(min(500, len(df_train_val)), random_state=42)\n",
    "        X_sample_scaled = scaler.transform(X_sample)\n",
    "        \n",
    "        try:\n",
    "            # Create SHAP explainer based on model type\n",
    "            model_name = data['metrics']['name']\n",
    "            \n",
    "            if 'RF' in model_name or 'GBM' in model_name:\n",
    "                # Tree-based model\n",
    "                explainer = shap.TreeExplainer(model)\n",
    "                shap_values = explainer.shap_values(X_sample_scaled[:100])\n",
    "            else:\n",
    "                # Linear or other model - use KernelExplainer\n",
    "                explainer = shap.KernelExplainer(model.predict, X_sample_scaled[:50])\n",
    "                shap_values = explainer.shap_values(X_sample_scaled[:50])\n",
    "            \n",
    "            # Calculate mean absolute SHAP values\n",
    "            if isinstance(shap_values, list):\n",
    "                shap_values = shap_values[0]\n",
    "            \n",
    "            mean_shap = np.abs(shap_values).mean(axis=0)\n",
    "            \n",
    "            # Create feature importance from SHAP\n",
    "            shap_importance = dict(zip(features, mean_shap))\n",
    "            sorted_importance = sorted(shap_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            print(f\"  Top 5 features by SHAP importance:\")\n",
    "            for feat, imp in sorted_importance[:5]:\n",
    "                print(f\"    {feat}: {imp:.4f}\")\n",
    "            \n",
    "            shap_explainers[horizon] = {\n",
    "                'explainer': explainer,\n",
    "                'background_data': X_sample_scaled[:50],\n",
    "                'feature_names': features,\n",
    "                'shap_importance': dict(sorted_importance)\n",
    "            }\n",
    "            \n",
    "            # Store in trained_models for later use\n",
    "            trained_models[horizon]['shap_explainer'] = shap_explainers[horizon]\n",
    "            \n",
    "            print(f\"  \u2713 SHAP explainer created\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  \u26a0\ufe0f SHAP analysis failed: {e}\")\n",
    "\n",
    "def explain_prediction(horizon, X_single, feature_names):\n",
    "    \"\"\"Generate explanation for a single prediction\"\"\"\n",
    "    if not HAS_SHAP or horizon not in shap_explainers:\n",
    "        return None\n",
    "    \n",
    "    explainer_data = shap_explainers[horizon]\n",
    "    explainer = explainer_data['explainer']\n",
    "    \n",
    "    try:\n",
    "        shap_values = explainer.shap_values(X_single.reshape(1, -1))\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[0]\n",
    "        \n",
    "        # Create explanation\n",
    "        contributions = list(zip(feature_names, shap_values[0]))\n",
    "        contributions = sorted(contributions, key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        explanation = []\n",
    "        for feat, contrib in contributions[:5]:\n",
    "            direction = \"\u2191\" if contrib > 0 else \"\u2193\"\n",
    "            explanation.append(f\"{feat}: {direction}{abs(contrib):.3f}\")\n",
    "        \n",
    "        return explanation\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "if HAS_SHAP and shap_explainers:\n",
    "    # Example explanation\n",
    "    print(f\"\\nExample prediction explanation:\")\n",
    "    for horizon in shap_explainers:\n",
    "        sample_X = df_train_val[trained_models[horizon]['features']].iloc[-1:].values\n",
    "        sample_X_scaled = trained_models[horizon]['scaler'].transform(sample_X)\n",
    "        \n",
    "        explanation = explain_prediction(horizon, sample_X_scaled[0], trained_models[horizon]['features'])\n",
    "        if explanation:\n",
    "            pred = trained_models[horizon]['model'].predict(sample_X_scaled)[0]\n",
    "            print(f\"  {horizon} prediction = {pred:.4f}\")\n",
    "            print(f\"  Top contributors: {', '.join(explanation[:3])}\")\n",
    "\n",
    "print(f\"\\n\u2713 SHAP explainers created for: {list(shap_explainers.keys())}\")\n",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# DQN AGENT TRAINING (OPTIONAL)\n",
    "# This trains a reinforcement learning agent for transaction timing\n",
    "# Skip if you just need prediction models\n",
    "\n",
    "TRAIN_DQN = False  # Set to True to train DQN agent\n",
    "\n",
    "if not TRAIN_DQN:\n",
    "    print(\"=\"*60)\n",
    "    print(\"DQN TRAINING SKIPPED (set TRAIN_DQN = True to enable)\")\n",
    "    print(\"=\"*60)\n",
    "    DQN_TRAINED = False"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Training Implementation (runs only if TRAIN_DQN = True)\n",
    "\n",
    "if TRAIN_DQN:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING DQN AGENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        import torch.optim as optim\n",
    "        from collections import deque\n",
    "        import random\n",
    "        \n",
    "        class DQNNetwork(nn.Module):\n",
    "            def __init__(self, state_dim, action_dim):\n",
    "                super().__init__()\n",
    "                self.net = nn.Sequential(\n",
    "                    nn.Linear(state_dim, 64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(64, 32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(32, action_dim)\n",
    "                )\n",
    "            \n",
    "            def forward(self, x):\n",
    "                return self.net(x)\n",
    "        \n",
    "        class DQNAgent:\n",
    "            def __init__(self, state_dim, action_dim):\n",
    "                self.state_dim = state_dim\n",
    "                self.action_dim = action_dim\n",
    "                self.epsilon = 1.0\n",
    "                self.epsilon_min = 0.05\n",
    "                self.epsilon_decay = 0.995\n",
    "                self.gamma = 0.99\n",
    "                self.lr = 0.001\n",
    "                self.memory = deque(maxlen=10000)\n",
    "                self.batch_size = 32\n",
    "                self.training_steps = 0\n",
    "                \n",
    "                self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                self.model = DQNNetwork(state_dim, action_dim).to(self.device)\n",
    "                self.target_model = DQNNetwork(state_dim, action_dim).to(self.device)\n",
    "                self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "                self.update_target()\n",
    "            \n",
    "            def update_target(self):\n",
    "                self.target_model.load_state_dict(self.model.state_dict())\n",
    "            \n",
    "            def act(self, state):\n",
    "                if random.random() < self.epsilon:\n",
    "                    return random.randint(0, self.action_dim - 1)\n",
    "                state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    q_values = self.model(state_t)\n",
    "                return q_values.argmax().item()\n",
    "            \n",
    "            def remember(self, state, action, reward, next_state, done):\n",
    "                self.memory.append((state, action, reward, next_state, done))\n",
    "            \n",
    "            def replay(self):\n",
    "                if len(self.memory) < self.batch_size:\n",
    "                    return\n",
    "                \n",
    "                batch = random.sample(self.memory, self.batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                \n",
    "                states = torch.FloatTensor(states).to(self.device)\n",
    "                actions = torch.LongTensor(actions).to(self.device)\n",
    "                rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "                next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "                dones = torch.FloatTensor(dones).to(self.device)\n",
    "                \n",
    "                current_q = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "                next_q = self.target_model(next_states).max(1)[0].detach()\n",
    "                target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "                \n",
    "                loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                self.training_steps += 1\n",
    "                if self.training_steps % 100 == 0:\n",
    "                    self.update_target()\n",
    "                \n",
    "                self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "            \n",
    "            def save(self, path):\n",
    "                torch.save(self.model.state_dict(), path)\n",
    "        \n",
    "        # Create simple environment\n",
    "        state_dim = min(30, len(X.columns))  # Limit state size\n",
    "        action_dim = 2  # 0 = wait, 1 = execute\n",
    "        \n",
    "        DQN_AGENT = DQNAgent(state_dim, action_dim)\n",
    "        \n",
    "        # Train for a few episodes\n",
    "        n_episodes = 500\n",
    "        print(f\"Training DQN for {n_episodes} episodes...\")\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            # Simple training loop\n",
    "            for i in range(min(100, len(X) - 1)):\n",
    "                state = X.iloc[i, :state_dim].values\n",
    "                action = DQN_AGENT.act(state)\n",
    "                \n",
    "                # Simple reward: negative gas price change if executing\n",
    "                next_gas = current_gas.iloc[i + 1] if i + 1 < len(current_gas) else current_gas.iloc[i]\n",
    "                reward = -(next_gas - current_gas.iloc[i]) if action == 1 else -0.001  # Small wait penalty\n",
    "                \n",
    "                next_state = X.iloc[i + 1, :state_dim].values if i + 1 < len(X) else state\n",
    "                done = (i >= min(99, len(X) - 2))\n",
    "                \n",
    "                DQN_AGENT.remember(state, action, reward, next_state, done)\n",
    "                DQN_AGENT.replay()\n",
    "            \n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f\"  Episode {episode + 1}/{n_episodes}, Epsilon: {DQN_AGENT.epsilon:.3f}\")\n",
    "        \n",
    "        DQN_TRAINED = True\n",
    "        DQN_METRICS = {\n",
    "            'episodes': n_episodes,\n",
    "            'training_steps': DQN_AGENT.training_steps,\n",
    "            'final_epsilon': float(DQN_AGENT.epsilon)\n",
    "        }\n",
    "        print(f\"\\n\u2713 DQN training complete ({DQN_AGENT.training_steps} steps)\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\u26a0\ufe0f PyTorch not available, skipping DQN training\")\n",
    "        DQN_TRAINED = False\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f DQN training failed: {e}\")\n",
    "        DQN_TRAINED = False\n",
    "else:\n",
    "    DQN_TRAINED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models + ALL IMPROVEMENTS\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json as json_lib\n",
    "\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING MODELS AND ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# === Save prediction models ===\n",
    "for horizon in ['1h', '4h', '24h']:\n",
    "    if horizon not in trained_models:\n",
    "        continue\n",
    "    \n",
    "    data = trained_models[horizon]\n",
    "    model = data['model']\n",
    "    scaler = data['scaler']\n",
    "    metrics = data['metrics']\n",
    "    features = data.get('features', [])\n",
    "    \n",
    "    model_data = {\n",
    "        'model': model,\n",
    "        'model_name': metrics['name'],\n",
    "        'metrics': {\n",
    "            'mae': float(metrics['mae']),\n",
    "            'improvement': float(metrics['improvement']),\n",
    "            'vs_holdout_baseline': float(metrics['vs_holdout_baseline']) if metrics.get('vs_holdout_baseline') else None,\n",
    "            'passed_baseline': bool(metrics.get('passed_baseline', False)),\n",
    "            'is_ensemble': bool(metrics.get('is_ensemble', False)),\n",
    "        },\n",
    "        'trained_at': datetime.now().isoformat(),\n",
    "        'feature_names': list(features),\n",
    "        'feature_scaler': scaler,\n",
    "    }\n",
    "    \n",
    "    if metrics.get('is_ensemble'):\n",
    "        model_data['ensemble_weights'] = metrics.get('ensemble_weights')\n",
    "    \n",
    "    # Add bias correction if available\n",
    "    if data.get('use_bias_correction') and horizon in bias_correction_factors:\n",
    "        model_data['bias_correction'] = bias_correction_factors[horizon]\n",
    "        model_data['use_bias_correction'] = True\n",
    "    \n",
    "    # Add bias-corrected model wrapper\n",
    "    if 'bias_corrected_model' in data:\n",
    "        model_data['bias_corrected_model'] = data['bias_corrected_model']\n",
    "    \n",
    "    # Add time-adaptive predictor\n",
    "    if 'time_adaptive_predict' in data:\n",
    "        model_data['has_time_adaptive'] = True\n",
    "    \n",
    "    # Add confidence scoring info\n",
    "    if data.get('has_confidence_scoring'):\n",
    "        model_data['has_confidence_scoring'] = True\n",
    "    \n",
    "    # Add online tracker initial state\n",
    "    if 'online_tracker' in data:\n",
    "        tracker = data['online_tracker']\n",
    "        model_data['online_bias_state'] = {\n",
    "            'overall': tracker.get_current_bias(),\n",
    "            'by_period': {p: tracker.get_current_bias(p) for p in ['night', 'morning', 'afternoon', 'evening']}\n",
    "        }\n",
    "    \n",
    "    if 'conformal_residuals' in dir() and horizon in conformal_residuals:\n",
    "        model_data['conformal_interval'] = float(conformal_residuals[horizon]['quantile'])\n",
    "        model_data['conformal_calibration_source'] = conformal_residuals[horizon].get('calibration_source', 'training')\n",
    "    \n",
    "    if 'time_period_calibration' in dir() and horizon in time_period_calibration:\n",
    "        model_data['time_calibration'] = time_period_calibration[horizon]\n",
    "    \n",
    "    joblib.dump(model_data, f'saved_models/model_{horizon}.pkl')\n",
    "    print(f\"\u2713 model_{horizon}.pkl ({metrics['name']}, {len(features)} features)\")\n",
    "    joblib.dump(scaler, f'saved_models/scaler_{horizon}.pkl')\n",
    "\n",
    "# === Save time-specific models ===\n",
    "if 'time_specific_models' in dir() and time_specific_models:\n",
    "    os.makedirs('saved_models/time_models', exist_ok=True)\n",
    "    for horizon, periods in time_specific_models.items():\n",
    "        for period, pdata in periods.items():\n",
    "            filename = f'saved_models/time_models/model_{horizon}_{period}.pkl'\n",
    "            joblib.dump(pdata, filename)\n",
    "            print(f\"  \u2192 {horizon}_{period} time model\")\n",
    "\n",
    "# === Save regime-specific models ===\n",
    "if 'regime_specific_models' in dir() and regime_specific_models:\n",
    "    os.makedirs('saved_models/regime_models', exist_ok=True)\n",
    "    for horizon, regime_dict in regime_specific_models.items():\n",
    "        for regime_val, regime_data in regime_dict.items():\n",
    "            filename = f'saved_models/regime_models/model_{horizon}_{regime_data[\"regime_name\"]}.pkl'\n",
    "            joblib.dump(regime_data, filename)\n",
    "            print(f\"  \u2192 {horizon}_{regime_data['regime_name']} regime\")\n",
    "\n",
    "# === Save spike forecast models ===\n",
    "if 'spike_forecast_models' in dir() and spike_forecast_models:\n",
    "    for horizon, sf_data in spike_forecast_models.items():\n",
    "        sf_save = {\n",
    "            'model': sf_data.get('model'),\n",
    "            'scaler': sf_data.get('scaler'),\n",
    "            'features': sf_data.get('features', []),\n",
    "            'threshold_config': sf_data.get('threshold_config'),\n",
    "            'optimal_threshold': sf_data.get('optimal_threshold', 0.5),\n",
    "            'metrics': {k: v for k, v in sf_data.items() if k not in ['model', 'scaler', 'features', 'threshold_config']}\n",
    "        }\n",
    "        joblib.dump(sf_save, f'saved_models/spike_forecast_{horizon}.pkl')\n",
    "        if sf_data.get('model') is not None:\n",
    "            print(f\"\u2713 spike_forecast_{horizon}.pkl (AUC: {sf_data.get('auc', 0.5):.3f})\")\n",
    "        else:\n",
    "            print(f\"\u26a0\ufe0f spike_forecast_{horizon}.pkl (no model)\")\n",
    "\n",
    "# === Save other files ===\n",
    "default_features = trained_models.get('4h', trained_models.get('1h', {})).get('features', [])\n",
    "joblib.dump(list(default_features), 'saved_models/feature_names.pkl')\n",
    "\n",
    "if 'regime_clf' in dir() and regime_clf is not None:\n",
    "    joblib.dump({'model': regime_clf, 'scaler': regime_scaler, 'accuracy': regime_accuracy}, \n",
    "                'saved_models/regime_detector.pkl')\n",
    "\n",
    "if 'quantile_models' in dir() and quantile_models:\n",
    "    for horizon, (q_models, q_scaler) in quantile_models.items():\n",
    "        quantile_data = {'models': q_models, 'scaler': q_scaler, 'quantiles': [0.1, 0.5, 0.9]}\n",
    "        if 'conformal_residuals' in dir() and horizon in conformal_residuals:\n",
    "            quantile_data['conformal'] = {\n",
    "                'interval_width': float(conformal_residuals[horizon]['quantile']),\n",
    "                'calibration_source': conformal_residuals[horizon].get('calibration_source', 'training')\n",
    "            }\n",
    "        if 'time_period_calibration' in dir() and horizon in time_period_calibration:\n",
    "            quantile_data['time_calibration'] = time_period_calibration[horizon]\n",
    "        joblib.dump(quantile_data, f'saved_models/quantile_{horizon}.pkl')\n",
    "\n",
    "# === Save training metadata ===\n",
    "def convert_to_python_types(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_python_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_python_types(v) for v in obj]\n",
    "    elif isinstance(obj, (np.bool_, np.integer)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif hasattr(obj, 'item'):\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "metadata = {\n",
    "    'training_timestamp': datetime.now().isoformat(),\n",
    "    'total_samples': len(df_clean),\n",
    "    'training_samples': len(df_train_val),\n",
    "    'holdout_samples': len(df_holdout) if df_holdout is not None else 0,\n",
    "    'date_range': f\"{df_clean.index.min()} to {df_clean.index.max()}\",\n",
    "    'selection_method': 'holdout-based',\n",
    "    'configuration': {\n",
    "        'target_transform': TARGET_TRANSFORM_USED if 'TARGET_TRANSFORM_USED' in dir() else 'none',\n",
    "        'use_rolling_window': USE_ROLLING_WINDOW if 'USE_ROLLING_WINDOW' in dir() else False,\n",
    "        'rolling_window_days': ROLLING_WINDOW_DAYS if 'ROLLING_WINDOW_DAYS' in dir() else None,\n",
    "        'distribution_shift_detected': DISTRIBUTION_SHIFT_DETECTED if 'DISTRIBUTION_SHIFT_DETECTED' in dir() else False,\n",
    "        'shift_magnitude': SHIFT_MAGNITUDE if 'SHIFT_MAGNITUDE' in dir() else 0,\n",
    "        'feature_pruning_enabled': ENABLE_FEATURE_PRUNING if 'ENABLE_FEATURE_PRUNING' in dir() else False,\n",
    "        'ensemble_blending_enabled': USE_ENSEMBLE_BLENDING if 'USE_ENSEMBLE_BLENDING' in dir() else False,\n",
    "        'regularization_strength': REGULARIZATION_STRENGTH if 'REGULARIZATION_STRENGTH' in dir() else 0,\n",
    "        'time_specific_models_enabled': TRAIN_TIME_SPECIFIC_MODELS if 'TRAIN_TIME_SPECIFIC_MODELS' in dir() else False,\n",
    "    },\n",
    "    'features': {'count': len(default_features), 'list': list(default_features)},\n",
    "    'baselines': BASELINES,\n",
    "    'models': {},\n",
    "    'time_specific_models': {},\n",
    "    'regime_models': {},\n",
    "    'direction_models': {},\n",
    "    'spike_forecast_models': {},\n",
    "    'error_analysis': {},\n",
    "    'bias_correction': {},\n",
    "    'calibration': {},\n",
    "    'retraining_recommendations': {},\n",
    "    'backtest_results': {}\n",
    "}\n",
    "\n",
    "# Model info\n",
    "for horizon, data in trained_models.items():\n",
    "    m = data['metrics']\n",
    "    metadata['models'][horizon] = {\n",
    "        'name': m['name'],\n",
    "        'mae': float(m['mae']),\n",
    "        'improvement_pct': float(m['improvement'] * 100),\n",
    "        'vs_holdout_baseline_pct': float(m['vs_holdout_baseline'] * 100) if m.get('vs_holdout_baseline') else None,\n",
    "        'passed_baseline': bool(m.get('passed_baseline', False)),\n",
    "        'is_fallback': data.get('is_fallback', False),\n",
    "        'is_ensemble': m.get('is_ensemble', False),\n",
    "        'has_bias_correction': data.get('use_bias_correction', False),\n",
    "        'has_time_adaptive': 'time_adaptive_predict' in data,\n",
    "        'n_features': len(data.get('features', []))\n",
    "    }\n",
    "    if 'calibration' in data:\n",
    "        metadata['models'][horizon]['calibration'] = data['calibration']\n",
    "\n",
    "# Time-specific models\n",
    "if 'time_specific_models' in dir() and time_specific_models:\n",
    "    for horizon, periods in time_specific_models.items():\n",
    "        metadata['time_specific_models'][horizon] = {\n",
    "            period: {'mae': float(p['mae']), 'improvement': float(p['improvement'])}\n",
    "            for period, p in periods.items()\n",
    "        }\n",
    "\n",
    "# Regime models\n",
    "if 'regime_specific_models' in dir() and regime_specific_models:\n",
    "    for horizon, regime_dict in regime_specific_models.items():\n",
    "        metadata['regime_models'][horizon] = {}\n",
    "        for regime_val, regime_data in regime_dict.items():\n",
    "            metadata['regime_models'][horizon][regime_data['regime_name']] = {\n",
    "                'mae': float(regime_data['metrics']['mae']),\n",
    "                'n_samples': int(regime_data['n_samples'])\n",
    "            }\n",
    "\n",
    "# Direction models\n",
    "if 'direction_models' in dir() and direction_models:\n",
    "    for horizon, data in direction_models.items():\n",
    "        metadata['direction_models'][horizon] = {\n",
    "            'accuracy': float(data['accuracy']),\n",
    "            'f1_score': float(data['f1_score']),\n",
    "            'baseline_accuracy': float(data.get('baseline_accuracy', 0)),\n",
    "            'improvement_vs_baseline': float(data.get('improvement_vs_baseline', 0)),\n",
    "        }\n",
    "\n",
    "# Spike forecast\n",
    "if 'spike_forecast_models' in dir() and spike_forecast_models:\n",
    "    for horizon, sf_data in spike_forecast_models.items():\n",
    "        metadata['spike_forecast_models'][horizon] = {\n",
    "            'has_model': sf_data.get('model') is not None,\n",
    "            'precision': float(sf_data.get('precision', 0)),\n",
    "            'recall': float(sf_data.get('recall', 0)),\n",
    "            'f1_score': float(sf_data.get('f1_score', 0)),\n",
    "            'auc': float(sf_data.get('auc', 0.5)),\n",
    "            'threshold_config': sf_data.get('threshold_config', {}).get('name') if sf_data.get('threshold_config') else None,\n",
    "        }\n",
    "\n",
    "# Error analysis\n",
    "if 'error_analysis' in dir() and error_analysis:\n",
    "    metadata['error_analysis'] = error_analysis\n",
    "\n",
    "# Bias correction\n",
    "if 'bias_correction_factors' in dir() and bias_correction_factors:\n",
    "    metadata['bias_correction'] = bias_correction_factors\n",
    "\n",
    "# Calibration info\n",
    "if 'conformal_residuals' in dir():\n",
    "    for horizon in conformal_residuals:\n",
    "        if horizon not in metadata['calibration']:\n",
    "            metadata['calibration'][horizon] = {}\n",
    "        metadata['calibration'][horizon]['conformal_width'] = float(conformal_residuals[horizon]['quantile'])\n",
    "        metadata['calibration'][horizon]['source'] = conformal_residuals[horizon].get('calibration_source', 'training')\n",
    "\n",
    "# Retraining recommendations\n",
    "if 'retraining_recommendations' in dir() and retraining_recommendations:\n",
    "    metadata['retraining_recommendations'] = retraining_recommendations\n",
    "\n",
    "# Backtest results\n",
    "if 'backtest_results' in dir() and backtest_results:\n",
    "    metadata['backtest_results'] = {h: {k: v for k, v in r.items() if k != 'online_alerts'} \n",
    "                                    for h, r in backtest_results.items()}\n",
    "\n",
    "metadata = convert_to_python_types(metadata)\n",
    "\n",
    "with open('saved_models/training_metadata.json', 'w') as f:\n",
    "    json_lib.dump(metadata, f, indent=2)\n",
    "print(f\"\\n\u2713 training_metadata.json\")\n",
    "\n",
    "# Feature importance\n",
    "if 'FEATURE_IMPORTANCE' in dir() and FEATURE_IMPORTANCE:\n",
    "    sorted_importance = dict(sorted(FEATURE_IMPORTANCE.items(), key=lambda x: x[1], reverse=True))\n",
    "    with open('saved_models/feature_importance.json', 'w') as f:\n",
    "        json_lib.dump(convert_to_python_types(sorted_importance), f, indent=2)\n",
    "    print(f\"\u2713 feature_importance.json\")\n",
    "\n",
    "# Training history\n",
    "history_file = 'saved_models/training_history.json'\n",
    "if os.path.exists(history_file):\n",
    "    with open(history_file) as f:\n",
    "        history = json_lib.load(f)\n",
    "else:\n",
    "    history = []\n",
    "\n",
    "current_run = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'models': {h: {\n",
    "        'name': trained_models[h]['metrics']['name'], \n",
    "        'mae': float(trained_models[h]['metrics']['mae']),\n",
    "        'is_ensemble': trained_models[h]['metrics'].get('is_ensemble', False),\n",
    "        'has_bias_correction': trained_models[h].get('use_bias_correction', False)\n",
    "    } for h in trained_models},\n",
    "    'improvements': {\n",
    "        'ensemble_blending': USE_ENSEMBLE_BLENDING if 'USE_ENSEMBLE_BLENDING' in dir() else False,\n",
    "        'time_specific_models': bool(time_specific_models) if 'time_specific_models' in dir() else False,\n",
    "        'bias_correction': any(trained_models.get(h, {}).get('use_bias_correction', False) for h in ['1h', '4h']),\n",
    "        'holdout_calibration': True\n",
    "    }\n",
    "}\n",
    "\n",
    "history.append(current_run)\n",
    "history = history[-10:]\n",
    "\n",
    "with open(history_file, 'w') as f:\n",
    "    json_lib.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for horizon, data in trained_models.items():\n",
    "    m = data['metrics']\n",
    "    extras = []\n",
    "    if m.get('is_ensemble'):\n",
    "        extras.append(\"ensemble\")\n",
    "    if data.get('use_bias_correction'):\n",
    "        extras.append(\"bias-corrected\")\n",
    "    if 'time_adaptive_predict' in data:\n",
    "        extras.append(\"time-adaptive\")\n",
    "    \n",
    "    extras_str = f\" [{', '.join(extras)}]\" if extras else \"\"\n",
    "    print(f\"\u2713 {horizon}: {m['name']} | MAE: {m['mae']:.4f}{extras_str}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL MODELS SAVED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE - FINAL REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_days = len(df_clean) / (120 * 24)\n",
    "\n",
    "print(f\"\\nDATA SUMMARY\")\n",
    "print(f\"   Total samples: {len(df_clean):,} ({total_days:.1f} days)\")\n",
    "print(f\"   Training: {len(df_train_val):,} | Holdout: {len(df_holdout) if df_holdout is not None else 0:,}\")\n",
    "print(f\"   Date range: {df_clean.index.min()} to {df_clean.index.max()}\")\n",
    "print(f\"   ETH price: {'Binance 1-min \u2713' if HAS_ETH_PRICE else 'Not available'}\")\n",
    "print(f\"   Features: 1h={len(numeric_features_1h)}, 4h={len(numeric_features_4h)}, 24h={len(numeric_features_24h)}\")\n",
    "\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(f\"{'MODEL PERFORMANCE':^70}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Horizon':<8} {'Model':<15} {'CV MAE':>10} {'Holdout':>10} {'vs Base':>10} {'Status':>12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for horizon in ['1h', '4h', '24h']:\n",
    "    if horizon in trained_models:\n",
    "        data = trained_models[horizon]\n",
    "        m = data['metrics']\n",
    "        name = m['name'][:14]\n",
    "        if data.get('is_fallback'):\n",
    "            name = name[:10] + '(fb)'\n",
    "        \n",
    "        cv_mae = f\"{m['mae']:.4f}\"\n",
    "        holdout_mae = f\"{data.get('holdout_mae', 0):.4f}\" if 'holdout_mae' in data else \"N/A\"\n",
    "        improvement = f\"{m['improvement']*100:+.1f}%\"\n",
    "        status = \"\u2713 PASS\" if m.get('passed_baseline', False) else \"\u2717 FAIL\"\n",
    "        \n",
    "        print(f\"{horizon:<8} {name:<15} {cv_mae:>10} {holdout_mae:>10} {improvement:>10} {status:>12}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Calibration report\n",
    "if any('calibration' in trained_models.get(h, {}) for h in ['1h', '4h']):\n",
    "    print(f\"\\n\" + \"-\"*70)\n",
    "    print(f\"{'PREDICTION INTERVAL CALIBRATION':^70}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Horizon':<10} {'Conformal 80%':>15} {'Adaptive 80%':>15} {'Width':>15}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for horizon in ['1h', '4h']:\n",
    "        if horizon in trained_models and 'calibration' in trained_models[horizon]:\n",
    "            cal = trained_models[horizon]['calibration']\n",
    "            c_cov = f\"{cal.get('conformal_coverage', 0):.1%}\"\n",
    "            a_cov = f\"{cal.get('adaptive_coverage', 0):.1%}\"\n",
    "            width = f\"\u00b1{cal.get('conformal_width', 0):.4f}\"\n",
    "            print(f\"{horizon:<10} {c_cov:>15} {a_cov:>15} {width:>15}\")\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "\n",
    "# Direction models\n",
    "if 'direction_models' in dir() and direction_models:\n",
    "    print(f\"\\n\" + \"-\"*70)\n",
    "    print(f\"{'DIRECTION PREDICTION':^70}\")\n",
    "    print(\"-\"*70)\n",
    "    for horizon, data in direction_models.items():\n",
    "        print(f\"  {horizon}: Accuracy={data['accuracy']:.1%}, F1={data['f1_score']:.3f}\")\n",
    "\n",
    "# Spike forecast\n",
    "if 'spike_forecast_models' in dir() and spike_forecast_models:\n",
    "    print(f\"\\n\" + \"-\"*70)\n",
    "    print(f\"{'SPIKE FORECASTING':^70}\")\n",
    "    print(\"-\"*70)\n",
    "    for horizon, data in spike_forecast_models.items():\n",
    "        if data.get('model') is not None:\n",
    "            print(f\"  {horizon}: AUC={data.get('auc', 0.5):.3f}, P={data.get('precision', 0):.1%}, R={data.get('recall', 0):.1%}\")\n",
    "        else:\n",
    "            print(f\"  {horizon}: No viable model (AUC < 0.55)\")\n",
    "\n",
    "# Bias correction\n",
    "if 'bias_correction_factors' in dir() and bias_correction_factors:\n",
    "    print(f\"\\n\" + \"-\"*70)\n",
    "    print(f\"{'BIAS CORRECTION':^70}\")\n",
    "    print(\"-\"*70)\n",
    "    for horizon, bcf in bias_correction_factors.items():\n",
    "        use_bc = trained_models.get(horizon, {}).get('use_bias_correction', False)\n",
    "        if use_bc:\n",
    "            print(f\"  {horizon}: ENABLED (correction: {bcf['overall']['correction']:+.4f})\")\n",
    "        else:\n",
    "            print(f\"  {horizon}: disabled\")\n",
    "\n",
    "# 24h model status\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(f\"{'24H MODEL STATUS':^70}\")\n",
    "print(\"-\"*70)\n",
    "if '24h' in trained_models:\n",
    "    if trained_models['24h'].get('is_fallback'):\n",
    "        print(f\"  \u26a0\ufe0f Using 4h model as fallback (need 30+ days of data)\")\n",
    "        print(f\"     Current data: {total_days:.1f} days\")\n",
    "        print(f\"     Recommendation: Collect {30 - total_days:.0f} more days before training true 24h model\")\n",
    "    else:\n",
    "        print(f\"  \u2713 True 24h model trained with {total_days:.1f} days of data\")\n",
    "\n",
    "# Final recommendation\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_passed = all(trained_models.get(h, {}).get('metrics', {}).get('passed_baseline', False) \n",
    "                 for h in trained_models if h in trained_models)\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\u2713 All models beat baseline - READY FOR DEPLOYMENT\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  1. Download saved_models/ folder\")\n",
    "    print(\"  2. Copy to backend/models/saved_models/\")\n",
    "    print(\"  3. Restart backend\")\n",
    "else:\n",
    "    failed = [h for h in trained_models \n",
    "              if not trained_models[h]['metrics'].get('passed_baseline', False)]\n",
    "    print(f\"\u26a0\ufe0f Some models did not pass baseline: {failed}\")\n",
    "    print(\"\\nRecommendations:\")\n",
    "    print(\"  - Collect more data\")\n",
    "    print(\"  - Review feature engineering\")\n",
    "    print(\"  - Consider deploying passing models only\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualizations - IMPROVED with holdout baseline comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Train vs Holdout Distribution Comparison\n",
    "ax1 = axes[0, 0]\n",
    "train_gas = current_gas.values\n",
    "if HAS_HOLDOUT:\n",
    "    holdout_gas = df_holdout['gas'].values\n",
    "    ax1.hist(train_gas, bins=50, alpha=0.6, color='blue', label=f'Train (mean={train_gas.mean():.2f})', density=True)\n",
    "    ax1.hist(holdout_gas, bins=50, alpha=0.6, color='red', label=f'Holdout (mean={holdout_gas.mean():.2f})', density=True)\n",
    "    ax1.legend()\n",
    "    ax1.set_title('Train vs Holdout Distribution')\n",
    "else:\n",
    "    ax1.hist(train_gas, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "    ax1.axvline(train_gas.mean(), color='red', linestyle='--', label=f'Mean: {train_gas.mean():.2f}')\n",
    "    ax1.legend()\n",
    "    ax1.set_title('Gas Price Distribution')\n",
    "ax1.set_xlabel('Gas Price (gwei)')\n",
    "ax1.set_ylabel('Density' if HAS_HOLDOUT else 'Frequency')\n",
    "\n",
    "# 2. Model vs HOLDOUT Baseline (not train baseline!)\n",
    "ax2 = axes[0, 1]\n",
    "horizons = list(trained_models.keys())\n",
    "maes = [trained_models[h]['metrics']['mae'] for h in horizons]\n",
    "\n",
    "# Use holdout baselines if available, otherwise train baselines\n",
    "baselines = []\n",
    "for h in horizons:\n",
    "    h_key = h.replace('24h', '4h')  # 24h uses 4h baseline\n",
    "    if 'holdout_best' in BASELINES.get(h_key, {}):\n",
    "        baselines.append(BASELINES[h_key]['holdout_best'])\n",
    "    else:\n",
    "        baselines.append(BASELINES.get(h_key, BASELINES['4h'])['best'])\n",
    "\n",
    "x = np.arange(len(horizons))\n",
    "width = 0.35\n",
    "bars1 = ax2.bar(x - width/2, maes, width, label='Model MAE', color='steelblue')\n",
    "bars2 = ax2.bar(x + width/2, baselines, width, label='Holdout Baseline', color='coral')\n",
    "ax2.set_xlabel('Horizon')\n",
    "ax2.set_ylabel('MAE (gwei)')\n",
    "ax2.set_title('Model vs Holdout Baseline Performance')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(horizons)\n",
    "ax2.legend()\n",
    "\n",
    "# Add improvement percentages (vs holdout baseline)\n",
    "for i, (h, m, b) in enumerate(zip(horizons, maes, baselines)):\n",
    "    imp = (b - m) / b * 100\n",
    "    color = 'green' if imp > 0 else 'red'\n",
    "    y_pos = max(m, b) + 0.02 * max(max(maes), max(baselines))\n",
    "    ax2.annotate(f'{imp:+.1f}%', xy=(i, y_pos), ha='center', fontsize=10, fontweight='bold', color=color)\n",
    "\n",
    "# 3. Gas price time series with regime markers\n",
    "ax3 = axes[1, 0]\n",
    "sample_size = min(2000, len(df_clean))\n",
    "sample_df = df_clean.iloc[-sample_size:]\n",
    "sample_gas = sample_df['gas']\n",
    "\n",
    "ax3.plot(sample_gas.index, sample_gas.values, linewidth=0.5, alpha=0.8, color='blue')\n",
    "\n",
    "# Mark holdout period\n",
    "if HAS_HOLDOUT:\n",
    "    holdout_start = df_holdout.index[0]\n",
    "    ax3.axvline(holdout_start, color='red', linestyle='--', linewidth=2, label='Holdout start')\n",
    "    ax3.legend()\n",
    "\n",
    "ax3.set_xlabel('Time')\n",
    "ax3.set_ylabel('Gas Price (gwei)')\n",
    "ax3.set_title(f'Recent Gas Prices (last {sample_size} samples)')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Feature importance (top 10)\n",
    "ax4 = axes[1, 1]\n",
    "if FEATURE_IMPORTANCE and any(v != list(FEATURE_IMPORTANCE.values())[0] for v in FEATURE_IMPORTANCE.values()):\n",
    "    # Non-uniform importance\n",
    "    sorted_imp = sorted(FEATURE_IMPORTANCE.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    features_plot = [f[0][:20] for f in sorted_imp]\n",
    "    importances = [f[1] for f in sorted_imp]\n",
    "    \n",
    "    y_pos = np.arange(len(features_plot))\n",
    "    ax4.barh(y_pos, importances, color='teal')\n",
    "    ax4.set_yticks(y_pos)\n",
    "    ax4.set_yticklabels(features_plot)\n",
    "    ax4.invert_yaxis()\n",
    "    ax4.set_xlabel('Importance')\n",
    "    ax4.set_title('Top 10 Feature Importance (Permutation)')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Feature importance uniform\\n(Huber model)', ha='center', va='center', fontsize=12)\n",
    "    ax4.set_title('Feature Importance')\n",
    "    ax4.set_xlim(0, 1)\n",
    "    ax4.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('saved_models/training_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Saved training_results.png\")\n",
    "\n",
    "# === ADDITIONAL: Distribution shift visualization ===\n",
    "if HAS_HOLDOUT and DISTRIBUTION_SHIFT_DETECTED:\n",
    "    fig2, axes2 = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    for i, horizon in enumerate(['1h', '4h']):\n",
    "        ax = axes2[i]\n",
    "        train_target = df_train_val[f'target_{horizon}'].dropna()\n",
    "        holdout_target = df_holdout[f'target_{horizon}'].dropna()\n",
    "        \n",
    "        ax.hist(train_target, bins=50, alpha=0.6, color='blue', label='Train', density=True)\n",
    "        ax.hist(holdout_target, bins=50, alpha=0.6, color='red', label='Holdout', density=True)\n",
    "        ax.set_xlabel(f'{horizon} Target (gwei)')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_title(f'{horizon} Target Distribution')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('saved_models/distribution_shift.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\u2713 Saved distribution_shift.png\")\n",
    ""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip file for download\n",
    "import shutil\n",
    "\n",
    "shutil.make_archive('gweizy_models', 'zip', 'saved_models')\n",
    "print(\"\\n\u2705 Created gweizy_models.zip\")\n",
    "print(\"\\nDownload this file and extract to: backend/models/saved_models/\")\n",
    "\n",
    "# Auto-download\n",
    "files.download('gweizy_models.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}