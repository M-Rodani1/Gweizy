{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gweizy Model Training Notebook\n",
    "\n",
    "Train all gas prediction models for Gweizy.\n",
    "\n",
    "## Instructions:\n",
    "1. Upload your `gas_data.db` file (from `backend/gas_data.db`)\n",
    "2. Run all cells\n",
    "3. Download the trained models zip file\n",
    "4. Extract to `backend/models/saved_models/` and push to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q scikit-learn pandas numpy joblib lightgbm xgboost matplotlib seaborn optuna"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your gas_data.db file\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"Upload your gas_data.db file from backend/gas_data.db\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "if 'gas_data.db' in uploaded:\n",
    "    print(f\"\\n\u2705 Uploaded gas_data.db ({len(uploaded['gas_data.db']) / 1024 / 1024:.1f} MB)\")\n",
    "else:\n",
    "    print(\"\u274c Please upload gas_data.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load data from database\n",
    "conn = sqlite3.connect('gas_data.db')\n",
    "df = pd.read_sql(\"\"\"\n",
    "    SELECT timestamp, current_gas as gas, base_fee, priority_fee, \n",
    "           block_number, gas_used, gas_limit, utilization\n",
    "    FROM gas_prices ORDER BY timestamp ASC\n",
    "\"\"\", conn)\n",
    "conn.close()\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df = df.set_index('timestamp').sort_index()\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "\n",
    "# === IMPROVED: Resample to 30-second intervals (was 1-min, losing too much data) ===\n",
    "print(\"\\nResampling to 30-second intervals (preserves more data)...\")\n",
    "df = df.resample('30s').mean().dropna(subset=['gas'])\n",
    "print(f\"After resample: {len(df):,} records\")\n",
    "\n",
    "# Find segments (gap > 30 min = new segment)\n",
    "df['time_diff'] = df.index.to_series().diff()\n",
    "df['segment'] = (df['time_diff'] > pd.Timedelta(minutes=30)).cumsum()\n",
    "\n",
    "segment_sizes = df.groupby('segment').size()\n",
    "print(f\"\\nSegments found: {len(segment_sizes)}\")\n",
    "print(f\"Segment sizes: {segment_sizes.sort_values(ascending=False).head(10).tolist()}\")\n",
    "\n",
    "# === IMPROVED: Lower threshold from 120 to 30 minutes (keeps more segments) ===\n",
    "MIN_SEGMENT_SIZE = 60  # 30 minutes at 30-sec intervals = 60 records\n",
    "good_segments = segment_sizes[segment_sizes >= MIN_SEGMENT_SIZE].index.tolist()\n",
    "df = df[df['segment'].isin(good_segments)]\n",
    "print(f\"\\nKeeping {len(good_segments)} segments with >= 30 minutes of data\")\n",
    "print(f\"Total usable records: {len(df):,}\")\n",
    "\n",
    "# === DATA SUFFICIENCY CHECK ===\n",
    "MIN_REQUIRED_SAMPLES = 10000\n",
    "if len(df) < MIN_REQUIRED_SAMPLES:\n",
    "    print(f\"\\n\u26a0\ufe0f  WARNING: Only {len(df):,} samples. Recommend at least {MIN_REQUIRED_SAMPLES:,}\")\n",
    "    print(\"   Models may underperform. Consider collecting more data.\")\n",
    "else:\n",
    "    print(f\"\\n\u2713 Data sufficiency check passed: {len(df):,} samples\")\n",
    "\n",
    "RECORDS_PER_HOUR = 120  # 30-sec intervals = 120 records per hour"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Fetch ETH Price Data (External Feature)\n",
    "import requests\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FETCHING EXTERNAL DATA: ETH PRICE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def fetch_eth_price_history(start_date, end_date):\n",
    "    \"\"\"Fetch ETH price history from CoinGecko API (free, no key needed)\"\"\"\n",
    "    try:\n",
    "        start_ts = int(start_date.timestamp())\n",
    "        end_ts = int(end_date.timestamp())\n",
    "        \n",
    "        url = f\"https://api.coingecko.com/api/v3/coins/ethereum/market_chart/range\"\n",
    "        params = {\n",
    "            'vs_currency': 'usd',\n",
    "            'from': start_ts,\n",
    "            'to': end_ts\n",
    "        }\n",
    "        \n",
    "        print(f\"Fetching ETH prices from {start_date} to {end_date}...\")\n",
    "        response = requests.get(url, params=params, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            prices = data.get('prices', [])\n",
    "            \n",
    "            eth_df = pd.DataFrame(prices, columns=['timestamp', 'eth_price'])\n",
    "            eth_df['timestamp'] = pd.to_datetime(eth_df['timestamp'], unit='ms')\n",
    "            eth_df = eth_df.set_index('timestamp')\n",
    "            \n",
    "            print(f\"  Fetched {len(eth_df)} ETH price points\")\n",
    "            return eth_df\n",
    "        else:\n",
    "            print(f\"  API returned status {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Failed to fetch ETH prices: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fetch ETH data for our date range\n",
    "eth_data = fetch_eth_price_history(df.index.min(), df.index.max())\n",
    "\n",
    "has_eth_data = False\n",
    "if eth_data is not None and len(eth_data) > 0:\n",
    "    # Resample to 30-second intervals to match our gas data\n",
    "    eth_data = eth_data.resample('30s').ffill()\n",
    "    \n",
    "    # Merge with gas data\n",
    "    df = df.join(eth_data, how='left')\n",
    "    df['eth_price'] = df['eth_price'].ffill().bfill()\n",
    "    \n",
    "    eth_coverage = df['eth_price'].notna().mean()\n",
    "    print(f\"  ETH price coverage: {eth_coverage:.1%}\")\n",
    "    \n",
    "    if eth_coverage > 0.5:\n",
    "        has_eth_data = True\n",
    "        print(\"  \u2713 ETH price data integrated\")\n",
    "    else:\n",
    "        print(\"  \u26a0\ufe0f Low ETH coverage, features may be limited\")\n",
    "else:\n",
    "    print(\"  \u26a0\ufe0f No ETH price data available - training without ETH features\")\n",
    "    df['eth_price'] = np.nan\n",
    "\n",
    "HAS_ETH_PRICE = has_eth_data"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - SIMPLIFIED for better generalization\n",
    "# Key insight: Too many features (65) with limited data causes overfitting\n",
    "# Reduced to ~20 essential features based on importance analysis\n",
    "\n",
    "print(\"Engineering SIMPLIFIED feature set (~20 features)...\")\n",
    "\n",
    "def engineer_features_for_segment(seg_df, has_eth=False):\n",
    "    \"\"\"Engineer features for a single continuous segment - SIMPLIFIED\"\"\"\n",
    "    df = seg_df.copy()\n",
    "    rph = 120  # records per hour (30-sec intervals)\n",
    "    \n",
    "    # === Log transform gas ===\n",
    "    df['gas_log'] = np.log1p(df['gas'])\n",
    "    \n",
    "    # === Time features (cyclical only - most predictive) ===\n",
    "    df['hour'] = df.index.hour\n",
    "    df['minute'] = df.index.minute\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['is_peak_hours'] = ((df['hour'] >= 14) & (df['hour'] <= 22)).astype(int)\n",
    "    \n",
    "    # === ETH PRICE FEATURES (if available) - only top 3 ===\n",
    "    if has_eth and 'eth_price' in df.columns and df['eth_price'].notna().any():\n",
    "        df['eth_log'] = np.log1p(df['eth_price'])\n",
    "        eth_mean = df['eth_price'].rolling(4*rph).mean()\n",
    "        eth_std = df['eth_price'].rolling(4*rph).std()\n",
    "        df['eth_zscore_4h'] = np.where(eth_std > 0.01, (df['eth_price'] - eth_mean) / eth_std, 0)\n",
    "        df['gas_eth_corr_1h'] = df['gas'].rolling(rph).corr(df['eth_price']).fillna(0)\n",
    "    \n",
    "    # === NETWORK UTILIZATION (if available) - only top 2 ===\n",
    "    if 'utilization' in df.columns:\n",
    "        df['util_mean_1h'] = df['utilization'].rolling(rph, min_periods=rph//2).mean()\n",
    "        df['util_mean_2h'] = df['utilization'].rolling(2*rph, min_periods=rph).mean()\n",
    "    \n",
    "    # === GAS LAG FEATURES - most important ones only ===\n",
    "    # These were top features in importance analysis\n",
    "    for lag_mins in [5, 15, 30, 60]:\n",
    "        lag_periods = lag_mins * 2  # 30-sec intervals\n",
    "        df[f'gas_lag_{lag_mins}min'] = df['gas'].shift(lag_periods)\n",
    "    \n",
    "    df['gas_lag_1h'] = df['gas'].shift(rph)\n",
    "    df['gas_lag_4h'] = df['gas'].shift(4*rph)\n",
    "    \n",
    "    # === ROLLING STATS - essential only ===\n",
    "    # 1h window stats (most important)\n",
    "    df['gas_mean_1h'] = df['gas'].rolling(rph, min_periods=rph//2).mean()\n",
    "    df['gas_std_1h'] = df['gas'].rolling(rph, min_periods=rph//2).std()\n",
    "    df['gas_median_1h'] = df['gas'].rolling(rph, min_periods=rph//2).median()\n",
    "    df['gas_cv_1h'] = np.where(df['gas_mean_1h'] > 0.01, df['gas_std_1h'] / df['gas_mean_1h'], 0)\n",
    "    \n",
    "    # 2h and 4h (for trend detection)\n",
    "    df['gas_mean_2h'] = df['gas'].rolling(2*rph, min_periods=rph).mean()\n",
    "    df['gas_mean_4h'] = df['gas'].rolling(4*rph, min_periods=2*rph).mean()\n",
    "    \n",
    "    # === MOMENTUM - top features ===\n",
    "    df['momentum_1h'] = df['gas'] - df['gas'].shift(rph)\n",
    "    df['momentum_pct_2h'] = np.where(df['gas'].shift(2*rph) > 0.001, \n",
    "        (df['gas'] - df['gas'].shift(2*rph)) / df['gas'].shift(2*rph), 0)\n",
    "    \n",
    "    # === TREND - the #1 most important feature ===\n",
    "    df['trend_1h_4h'] = np.where(df['gas_mean_4h'] > 0.01, df['gas_mean_1h'] / df['gas_mean_4h'], 1.0)\n",
    "    ema_short = df['gas'].ewm(span=rph).mean()\n",
    "    ema_long = df['gas'].ewm(span=2*rph).mean()\n",
    "    df['ema_trend_short'] = np.where(ema_long > 0.01, ema_short / ema_long, 1.0)\n",
    "    \n",
    "    # === Z-SCORE ===\n",
    "    df['gas_zscore_1h'] = np.where(df['gas_std_1h'] > 0.001, (df['gas'] - df['gas_mean_1h']) / df['gas_std_1h'], 0)\n",
    "    \n",
    "    # === VOLATILITY REGIME (for spike detection) ===\n",
    "    rolling_std = df['gas'].rolling(4*rph).std()\n",
    "    overall_std = df['gas'].std()\n",
    "    df['volatility_regime'] = pd.cut(\n",
    "        np.where(overall_std > 0.001, rolling_std / overall_std, 1.0),\n",
    "        bins=[0, 0.5, 1.5, float('inf')],\n",
    "        labels=['low', 'normal', 'high']\n",
    "    )\n",
    "    \n",
    "    # === Spike indicators ===\n",
    "    df['is_spike'] = (df['gas'] > df['gas_mean_1h'] + 2 * df['gas_std_1h']).astype(int)\n",
    "    df['is_high_gas'] = (df['gas'] > df['gas'].rolling(4*rph).quantile(0.9)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process each segment\n",
    "print(\"\\nProcessing segments...\")\n",
    "segments = df['segment'].unique()\n",
    "processed_segments = []\n",
    "\n",
    "for seg_id in segments:\n",
    "    seg_df = df[df['segment'] == seg_id].copy()\n",
    "    processed = engineer_features_for_segment(seg_df, has_eth=has_eth_data)\n",
    "    processed_segments.append(processed)\n",
    "\n",
    "df_features = pd.concat(processed_segments, axis=0)\n",
    "print(f\"After feature engineering: {len(df_features):,} records\")\n",
    "\n",
    "# Create targets\n",
    "print(\"\\nCreating prediction targets...\")\n",
    "\n",
    "def create_targets_for_segment(seg_df):\n",
    "    \"\"\"Create target variables - future gas prices\"\"\"\n",
    "    df = seg_df.copy()\n",
    "    rph = 120  # 30-sec intervals\n",
    "    \n",
    "    # Future prices (targets)\n",
    "    df['target_1h'] = df['gas'].shift(-rph)\n",
    "    df['target_4h'] = df['gas'].shift(-4*rph)\n",
    "    df['target_24h'] = df['gas'].shift(-24*rph)\n",
    "    \n",
    "    # Price changes (for differencing approach)\n",
    "    df['target_diff_1h'] = df['target_1h'] - df['gas']\n",
    "    df['target_diff_4h'] = df['target_4h'] - df['gas']\n",
    "    \n",
    "    # Direction classification\n",
    "    threshold = 0.01  # 1% change threshold\n",
    "    for horizon in ['1h', '4h']:\n",
    "        pct_change = np.where(df['gas'] > 0.001, (df[f'target_{horizon}'] - df['gas']) / df['gas'], 0)\n",
    "        df[f'direction_class_{horizon}'] = pd.cut(\n",
    "            pct_change,\n",
    "            bins=[-float('inf'), -threshold, threshold, float('inf')],\n",
    "            labels=['down', 'stable', 'up']\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to each segment\n",
    "processed_with_targets = []\n",
    "for seg_id in df_features['segment'].unique():\n",
    "    seg_df = df_features[df_features['segment'] == seg_id].copy()\n",
    "    processed = create_targets_for_segment(seg_df)\n",
    "    processed_with_targets.append(processed)\n",
    "\n",
    "df_features = pd.concat(processed_with_targets, axis=0)\n",
    "\n",
    "# === CRITICAL: Clean inf/nan values ===\n",
    "print(\"\\nCleaning inf/nan values...\")\n",
    "\n",
    "# Replace inf with nan, then fill\n",
    "for col in df_features.select_dtypes(include=[np.number]).columns:\n",
    "    # Replace inf with nan\n",
    "    df_features[col] = df_features[col].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Clip extreme values (beyond 99.9th percentile)\n",
    "    if df_features[col].notna().sum() > 0:\n",
    "        q_low = df_features[col].quantile(0.001)\n",
    "        q_high = df_features[col].quantile(0.999)\n",
    "        df_features[col] = df_features[col].clip(q_low, q_high)\n",
    "\n",
    "# Fill remaining NaN with forward fill then median\n",
    "df_features = df_features.ffill().bfill()\n",
    "\n",
    "# Final check\n",
    "inf_count = np.isinf(df_features.select_dtypes(include=[np.number])).sum().sum()\n",
    "nan_count = df_features.select_dtypes(include=[np.number]).isna().sum().sum()\n",
    "print(f\"  After cleaning: {inf_count} inf, {nan_count} nan values\")\n",
    "\n",
    "print(f\"Records with targets: {len(df_features):,}\")\n",
    "\n",
    "# Count features\n",
    "feature_cols_temp = [c for c in df_features.columns if c not in [\n",
    "    'gas', 'gas_log', 'base_fee', 'priority_fee', 'block_number',\n",
    "    'gas_used', 'gas_limit', 'utilization', 'eth_price', 'segment', 'time_diff',\n",
    "    'target_1h', 'target_4h', 'target_24h', 'target_diff_1h', 'target_diff_4h',\n",
    "    'direction_class_1h', 'direction_class_4h', 'volatility_regime'\n",
    "]]\n",
    "print(f\"\\n\u2713 Feature count: {len(feature_cols_temp)} (target: ~20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data - SIMPLIFIED (no complex feature selection needed)\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Columns to exclude from features\n",
    "exclude_cols = ['gas', 'gas_log', 'base_fee', 'priority_fee', 'block_number', \n",
    "                'gas_used', 'gas_limit', 'utilization', 'eth_price', 'segment', 'time_diff',\n",
    "                'target_1h', 'target_4h', 'target_24h',\n",
    "                'target_diff_1h', 'target_diff_4h',\n",
    "                'direction_class_1h', 'direction_class_4h',\n",
    "                'volatility_regime']\n",
    "\n",
    "feature_cols = [c for c in df_features.columns if c not in exclude_cols]\n",
    "\n",
    "# Only keep numeric columns (exclude categorical like volatility_regime)\n",
    "numeric_features = df_features[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols = numeric_features\n",
    "print(f\"Feature columns: {len(feature_cols)} (numeric only)\")\n",
    "\n",
    "# Drop rows with NaN\n",
    "df_clean = df_features.dropna()\n",
    "print(f\"Clean samples: {len(df_clean):,}\")\n",
    "\n",
    "# Final safety check - replace any remaining inf\n",
    "for col in df_clean.select_dtypes(include=[np.number]).columns:\n",
    "    df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "    if df_clean[col].isna().any():\n",
    "        df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "\n",
    "# Verify no inf/nan (only check float columns)\n",
    "float_cols = df_clean.select_dtypes(include=[np.float64, np.float32, float]).columns\n",
    "for col in float_cols:\n",
    "    if np.isinf(df_clean[col]).any():\n",
    "        print(f\"  Fixing inf in {col}\")\n",
    "        df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "        df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "    if np.isnan(df_clean[col]).any():\n",
    "        print(f\"  Fixing nan in {col}\")\n",
    "        df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "\n",
    "# Final check\n",
    "has_inf = any(np.isinf(df_clean[col]).any() for col in float_cols)\n",
    "has_nan = any(np.isnan(df_clean[col]).any() for col in float_cols)\n",
    "assert not has_inf, \"Data still contains inf!\"\n",
    "assert not has_nan, \"Data still contains nan!\"\n",
    "print(\"\u2713 Data validated: no inf/nan values\")\n",
    "\n",
    "# === SIMPLIFIED: No complex feature selection - we already reduced features ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE LIST (pre-selected for optimal performance)\")\n",
    "print(\"=\"*60)\n",
    "for i, col in enumerate(feature_cols):\n",
    "    print(f\"  {i+1:2}. {col}\")\n",
    "\n",
    "# Prepare data\n",
    "X = df_clean[feature_cols]\n",
    "\n",
    "y_1h = df_clean['target_1h']\n",
    "y_4h = df_clean['target_4h']\n",
    "y_24h = df_clean['target_24h']\n",
    "\n",
    "# Difference targets\n",
    "y_diff_1h = df_clean['target_diff_1h']\n",
    "y_diff_4h = df_clean['target_diff_4h']\n",
    "\n",
    "# Direction targets\n",
    "y_dir_1h = df_clean['direction_class_1h']\n",
    "y_dir_4h = df_clean['direction_class_4h']\n",
    "\n",
    "# Volatility regime\n",
    "volatility_regime = df_clean['volatility_regime']\n",
    "\n",
    "# Current gas for baseline\n",
    "current_gas = df_clean['gas']\n",
    "\n",
    "# === BASELINE MODELS ===\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BASELINE COMPARISONS\")\n",
    "print(\"{'='*60}\")\n",
    "\n",
    "# Naive baseline: predict current value\n",
    "naive_mae_1h = np.mean(np.abs(y_1h.values - current_gas.values))\n",
    "naive_mae_4h = np.mean(np.abs(y_4h.values - current_gas.values))\n",
    "\n",
    "# Mean baseline\n",
    "mean_pred = np.full_like(y_1h.values, y_1h.mean())\n",
    "mean_mae_1h = np.mean(np.abs(y_1h.values - mean_pred))\n",
    "mean_mae_4h = np.mean(np.abs(y_4h.values - mean_pred))\n",
    "\n",
    "print(f\"\\nBaseline MAEs:\")\n",
    "print(f\"  Naive (current price):     MAE_1h={naive_mae_1h:.6f}, MAE_4h={naive_mae_4h:.6f}\")\n",
    "print(f\"  Mean (historical average): MAE_1h={mean_mae_1h:.6f}, MAE_4h={mean_mae_4h:.6f}\")\n",
    "\n",
    "best_baseline_1h = min(naive_mae_1h, mean_mae_1h)\n",
    "best_baseline_4h = min(naive_mae_4h, mean_mae_4h)\n",
    "\n",
    "print(f\"\\n  Best baseline 1h: {best_baseline_1h:.6f}\")\n",
    "print(f\"  Best baseline 4h: {best_baseline_4h:.6f}\")\n",
    "\n",
    "BASELINES = {\n",
    "    '1h': {'naive_mae': naive_mae_1h, 'mean_mae': mean_mae_1h, 'best': best_baseline_1h},\n",
    "    '4h': {'naive_mae': naive_mae_4h, 'mean_mae': mean_mae_4h, 'best': best_baseline_4h}\n",
    "}\n",
    "\n",
    "# Store feature importance (equal weights since pre-selected)\n",
    "FEATURE_IMPORTANCE = {col: 1.0/len(feature_cols) for col in feature_cols}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING DATA SUMMARY\")\n",
    "print(\"{'='*60}\")\n",
    "print(f\"Samples: {len(X):,}\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Target 1h range: {y_1h.min():.4f} - {y_1h.max():.4f} gwei\")\n",
    "print(f\"Target 4h range: {y_4h.min():.4f} - {y_4h.max():.4f} gwei\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training - SIMPLIFIED with Walk-Forward Validation + Baseline Gate\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, HuberRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === BASELINE GATE: Only save models that beat baseline ===\n",
    "MINIMUM_IMPROVEMENT = 0.05  # Must be at least 5% better than baseline\n",
    "\n",
    "def check_baseline_gate(model_mae, baseline_mae, model_name):\n",
    "    \"\"\"Check if model beats baseline by minimum threshold\"\"\"\n",
    "    improvement = (baseline_mae - model_mae) / baseline_mae\n",
    "    passed = improvement >= MINIMUM_IMPROVEMENT\n",
    "    \n",
    "    if passed:\n",
    "        print(f\"  \u2713 PASSED baseline gate: {improvement*100:.1f}% improvement\")\n",
    "    else:\n",
    "        print(f\"  \u2717 FAILED baseline gate: {improvement*100:.1f}% (need {MINIMUM_IMPROVEMENT*100:.0f}%+)\")\n",
    "    \n",
    "    return passed, improvement\n",
    "\n",
    "def evaluate_model(y_true, y_pred, baseline_mae):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    improvement = (baseline_mae - mae) / baseline_mae\n",
    "    vs_baseline = f\"{improvement*100:+.1f}%\"\n",
    "    \n",
    "    # Directional accuracy\n",
    "    if len(y_true) > 1:\n",
    "        actual_dir = np.sign(np.diff(y_true))\n",
    "        pred_dir = np.sign(np.diff(y_pred))\n",
    "        dir_acc = np.mean(actual_dir == pred_dir)\n",
    "    else:\n",
    "        dir_acc = 0\n",
    "    \n",
    "    return {\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'improvement': improvement,\n",
    "        'vs_baseline': vs_baseline,\n",
    "        'directional_accuracy': dir_acc\n",
    "    }\n",
    "\n",
    "# === WALK-FORWARD VALIDATION ===\n",
    "def walk_forward_validate(model_class, model_params, X, y, baseline_mae, n_splits=5):\n",
    "    \"\"\"\n",
    "    Walk-forward validation for time series.\n",
    "    More robust than simple train/test split.\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Scale\n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Train\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Evaluate\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        fold_results.append(mae)\n",
    "    \n",
    "    avg_mae = np.mean(fold_results)\n",
    "    std_mae = np.std(fold_results)\n",
    "    \n",
    "    return {\n",
    "        'avg_mae': avg_mae,\n",
    "        'std_mae': std_mae,\n",
    "        'fold_maes': fold_results,\n",
    "        'improvement': (baseline_mae - avg_mae) / baseline_mae\n",
    "    }\n",
    "\n",
    "# === SIMPLIFIED MODEL TRAINING ===\n",
    "def train_model_simple(X, y, baseline_mae, horizon_name):\n",
    "    \"\"\"\n",
    "    Train model with simplified approach:\n",
    "    1. Try a few simple models\n",
    "    2. Use walk-forward validation\n",
    "    3. Only keep if beats baseline\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {horizon_name} model\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Samples: {len(X):,}, Features: {X.shape[1]}\")\n",
    "    print(f\"Baseline MAE: {baseline_mae:.6f}\")\n",
    "    \n",
    "    # Models to try (simple to complex)\n",
    "    models_to_try = [\n",
    "        ('Ridge', Ridge, {'alpha': 1.0, 'random_state': 42}),\n",
    "        ('Huber', HuberRegressor, {'epsilon': 1.35, 'alpha': 0.1, 'max_iter': 1000}),\n",
    "        ('RF_small', RandomForestRegressor, {'n_estimators': 50, 'max_depth': 6, 'random_state': 42, 'n_jobs': -1}),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model_class, params in models_to_try:\n",
    "        print(f\"\\n[{name}] Walk-forward validation...\")\n",
    "        try:\n",
    "            wf_result = walk_forward_validate(model_class, params, X, y, baseline_mae, n_splits=5)\n",
    "            print(f\"  Avg MAE: {wf_result['avg_mae']:.6f} \u00b1 {wf_result['std_mae']:.6f}\")\n",
    "            print(f\"  vs Baseline: {wf_result['improvement']*100:+.1f}%\")\n",
    "            \n",
    "            results.append({\n",
    "                'name': name,\n",
    "                'model_class': model_class,\n",
    "                'params': params,\n",
    "                'avg_mae': wf_result['avg_mae'],\n",
    "                'std_mae': wf_result['std_mae'],\n",
    "                'improvement': wf_result['improvement']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed: {e}\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"\\n\u26a0\ufe0f  All models failed!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Select best model\n",
    "    best = min(results, key=lambda x: x['avg_mae'])\n",
    "    print(f\"\\n>>> Best model: {best['name']} (MAE: {best['avg_mae']:.6f})\")\n",
    "    \n",
    "    # Check baseline gate\n",
    "    passed, improvement = check_baseline_gate(best['avg_mae'], baseline_mae, best['name'])\n",
    "    \n",
    "    if not passed:\n",
    "        print(f\"\\n\u26a0\ufe0f  WARNING: Best model doesn't beat baseline!\")\n",
    "        print(f\"   Will save anyway but model may not be useful.\")\n",
    "    \n",
    "    # Train final model on all data\n",
    "    print(f\"\\nTraining final {best['name']} model on all data...\")\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    final_model = best['model_class'](**best['params'])\n",
    "    final_model.fit(X_scaled, y)\n",
    "    \n",
    "    return final_model, scaler, {\n",
    "        'name': best['name'],\n",
    "        'mae': best['avg_mae'],\n",
    "        'improvement': best['improvement'],\n",
    "        'passed_baseline': passed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models with the simplified approach\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING ALL PREDICTION MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "# === 1H MODEL ===\n",
    "model_1h, scaler_1h, metrics_1h = train_model_simple(\n",
    "    X, y_1h, BASELINES['1h']['best'], '1h'\n",
    ")\n",
    "if model_1h:\n",
    "    trained_models['1h'] = {\n",
    "        'model': model_1h,\n",
    "        'scaler': scaler_1h,\n",
    "        'metrics': metrics_1h\n",
    "    }\n",
    "\n",
    "# === 4H MODEL ===\n",
    "model_4h, scaler_4h, metrics_4h = train_model_simple(\n",
    "    X, y_4h, BASELINES['4h']['best'], '4h'\n",
    ")\n",
    "if model_4h:\n",
    "    trained_models['4h'] = {\n",
    "        'model': model_4h,\n",
    "        'scaler': scaler_4h,\n",
    "        'metrics': metrics_4h\n",
    "    }\n",
    "\n",
    "# === 24H MODEL - HONEST HANDLING ===\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"24h Model\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Check if we have enough data for 24h predictions\n",
    "records_per_hour = 120  # 30-sec intervals\n",
    "required_24h_records = 24 * records_per_hour  # Records needed for 24h lookahead\n",
    "valid_24h_samples = y_24h.notna().sum()\n",
    "\n",
    "print(f\"Valid 24h samples: {valid_24h_samples:,}\")\n",
    "print(f\"Required for reliable 24h model: ~{required_24h_records * 10:,} samples\")\n",
    "\n",
    "if valid_24h_samples < 5000:\n",
    "    print(f\"\\n\u26a0\ufe0f  Insufficient data for 24h model ({valid_24h_samples:,} < 5,000)\")\n",
    "    print(\"   Using 4h model as fallback for 24h predictions\")\n",
    "    print(\"   (This is honest - 24h predictions will be less accurate)\")\n",
    "    \n",
    "    if model_4h:\n",
    "        trained_models['24h'] = {\n",
    "            'model': model_4h,  # Reuse 4h model\n",
    "            'scaler': scaler_4h,\n",
    "            'metrics': {\n",
    "                'name': metrics_4h['name'] + ' (4h fallback)',\n",
    "                'mae': metrics_4h['mae'],\n",
    "                'improvement': metrics_4h['improvement'],\n",
    "                'passed_baseline': metrics_4h['passed_baseline'],\n",
    "                'is_fallback': True\n",
    "            }\n",
    "        }\n",
    "else:\n",
    "    # Train actual 24h model\n",
    "    model_24h, scaler_24h, metrics_24h = train_model_simple(\n",
    "        X[y_24h.notna()], y_24h.dropna(), BASELINES['4h']['best'], '24h'  # Use 4h baseline as reference\n",
    "    )\n",
    "    if model_24h:\n",
    "        trained_models['24h'] = {\n",
    "            'model': model_24h,\n",
    "            'scaler': scaler_24h,\n",
    "            'metrics': metrics_24h\n",
    "        }\n",
    "\n",
    "# === SUMMARY ===\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for horizon, data in trained_models.items():\n",
    "    m = data['metrics']\n",
    "    status = \"\u2713 PASSED\" if m['passed_baseline'] else \"\u26a0 BELOW BASELINE\"\n",
    "    fallback = \" (fallback)\" if m.get('is_fallback') else \"\"\n",
    "    print(f\"{horizon}: {m['name']}{fallback} | MAE: {m['mae']:.6f} | {m['improvement']*100:+.1f}% | {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# PREDICTION INTERVALS using Quantile Regression - SIMPLIFIED\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING QUANTILE MODELS (Prediction Intervals)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "quantile_models = {}\n",
    "\n",
    "for horizon, y_target in [('1h', y_1h), ('4h', y_4h)]:\n",
    "    print(f\"\\n{horizon} quantile models...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    mask = y_target.notna()\n",
    "    X_q = X[mask]\n",
    "    y_q = y_target[mask]\n",
    "    \n",
    "    if len(X_q) < 1000:\n",
    "        print(f\"  \u26a0\ufe0f Insufficient data for {horizon} quantiles, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Train/test split\n",
    "    split_idx = int(len(X_q) * 0.8)\n",
    "    X_train, X_test = X_q.iloc[:split_idx], X_q.iloc[split_idx:]\n",
    "    y_train, y_test = y_q.iloc[:split_idx], y_q.iloc[split_idx:]\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    q_models = {}\n",
    "    for q in [0.1, 0.5, 0.9]:\n",
    "        model = GradientBoostingRegressor(\n",
    "            loss='quantile', alpha=q,\n",
    "            n_estimators=50, max_depth=4,\n",
    "            learning_rate=0.1, random_state=42\n",
    "        )\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        q_models[q] = model\n",
    "        print(f\"  Trained {q:.0%} quantile model\")\n",
    "    \n",
    "    quantile_models[horizon] = (q_models, scaler)\n",
    "\n",
    "# Copy 4h to 24h if available\n",
    "if '4h' in quantile_models:\n",
    "    quantile_models['24h'] = quantile_models['4h']\n",
    "    print(\"\\n24h: Using 4h quantile models (fallback)\")\n",
    "\n",
    "print(f\"\\n\u2713 Quantile models trained for: {list(quantile_models.keys())}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Direction Prediction (Classification: Down/Stable/Up) - SIMPLIFIED\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING DIRECTION MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "direction_models = {}\n",
    "\n",
    "for horizon, y_dir in [('1h', y_dir_1h), ('4h', y_dir_4h)]:\n",
    "    print(f\"\\n{horizon} direction model...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    mask = y_dir.notna()\n",
    "    X_d = X[mask]\n",
    "    y_d = y_dir[mask]\n",
    "    \n",
    "    if len(X_d) < 1000:\n",
    "        print(f\"  \u26a0\ufe0f Insufficient data for {horizon} direction, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Train/test split\n",
    "    split_idx = int(len(X_d) * 0.8)\n",
    "    X_train, X_test = X_d.iloc[:split_idx], X_d.iloc[split_idx:]\n",
    "    y_train, y_test = y_d.iloc[:split_idx], y_d.iloc[split_idx:]\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train classifier\n",
    "    clf = GradientBoostingClassifier(\n",
    "        n_estimators=50, max_depth=4,\n",
    "        learning_rate=0.1, random_state=42\n",
    "    )\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    direction_models[horizon] = {\n",
    "        'model': clf,\n",
    "        'scaler': scaler,\n",
    "        'accuracy': float(acc),\n",
    "        'f1_score': float(f1)\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {acc:.1%}, F1: {f1:.3f}\")\n",
    "\n",
    "print(f\"\\n\u2713 Direction models trained for: {list(direction_models.keys())}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# REGIME DETECTION - SIMPLIFIED\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING REGIME DETECTION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create regime labels from volatility\n",
    "if 'volatility_regime' in df_clean.columns:\n",
    "    regime_labels = df_clean['volatility_regime'].map({'low': 0, 'normal': 1, 'high': 2})\n",
    "    mask = regime_labels.notna()\n",
    "    \n",
    "    X_r = X[mask]\n",
    "    y_r = regime_labels[mask]\n",
    "    \n",
    "    # Train/test split\n",
    "    split_idx = int(len(X_r) * 0.8)\n",
    "    X_train, X_test = X_r.iloc[:split_idx], X_r.iloc[split_idx:]\n",
    "    y_train, y_test = y_r.iloc[:split_idx], y_r.iloc[split_idx:]\n",
    "    \n",
    "    regime_scaler = RobustScaler()\n",
    "    X_train_scaled = regime_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = regime_scaler.transform(X_test)\n",
    "    \n",
    "    # Train classifier\n",
    "    regime_clf = RandomForestClassifier(\n",
    "        n_estimators=50, max_depth=6,\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    regime_clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = regime_clf.predict(X_test_scaled)\n",
    "    regime_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Regime classes: Normal (0), Elevated (1), Spike (2)\")\n",
    "    print(f\"Accuracy: {regime_accuracy:.1%}\")\n",
    "    \n",
    "    if regime_accuracy > 0.95:\n",
    "        print(\"\u26a0\ufe0f  Warning: Very high accuracy may indicate overfitting\")\n",
    "else:\n",
    "    regime_clf = None\n",
    "    regime_scaler = None\n",
    "    regime_accuracy = 0\n",
    "    print(\"\u26a0\ufe0f  No volatility regime data available, skipping regime detection\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Spike Detectors - SIMPLIFIED\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SPIKE DETECTORS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "spike_models = {}\n",
    "\n",
    "for horizon, y_target in [('1h', y_1h), ('4h', y_4h)]:\n",
    "    print(f\"\\n{horizon} spike detector...\")\n",
    "    \n",
    "    # Create spike labels (>2 std from mean is a spike)\n",
    "    mask = y_target.notna()\n",
    "    X_s = X[mask]\n",
    "    y_s = y_target[mask]\n",
    "    current = current_gas[mask]\n",
    "    \n",
    "    # Define spike threshold\n",
    "    price_change = y_s - current\n",
    "    threshold = price_change.std() * 2\n",
    "    spike_labels = (price_change > threshold).astype(int)\n",
    "    \n",
    "    spike_rate = spike_labels.mean()\n",
    "    print(f\"  Spike rate: {spike_rate:.1%}\")\n",
    "    \n",
    "    if spike_rate < 0.01 or spike_rate > 0.5:\n",
    "        print(f\"  \u26a0\ufe0f Unusual spike rate, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Train/test split\n",
    "    split_idx = int(len(X_s) * 0.8)\n",
    "    X_train, X_test = X_s.iloc[:split_idx], X_s.iloc[split_idx:]\n",
    "    y_train, y_test = spike_labels.iloc[:split_idx], spike_labels.iloc[split_idx:]\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train with class weights for imbalanced data\n",
    "    clf = GradientBoostingClassifier(\n",
    "        n_estimators=50, max_depth=4,\n",
    "        learning_rate=0.1, random_state=42\n",
    "    )\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    spike_models[horizon] = (clf, scaler)\n",
    "    print(f\"  Accuracy: {acc:.1%}\")\n",
    "\n",
    "# Copy 4h to 24h if available\n",
    "if '4h' in spike_models:\n",
    "    spike_models['24h'] = spike_models['4h']\n",
    "    print(\"\\n24h: Using 4h spike detector (fallback)\")\n",
    "\n",
    "print(f\"\\n\u2713 Spike detectors trained for: {list(spike_models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# DQN AGENT TRAINING (OPTIONAL)\n",
    "# This trains a reinforcement learning agent for transaction timing\n",
    "# Skip if you just need prediction models\n",
    "\n",
    "TRAIN_DQN = False  # Set to True to train DQN agent\n",
    "\n",
    "if not TRAIN_DQN:\n",
    "    print(\"=\"*60)\n",
    "    print(\"DQN TRAINING SKIPPED (set TRAIN_DQN = True to enable)\")\n",
    "    print(\"=\"*60)\n",
    "    DQN_TRAINED = False"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Training Implementation (runs only if TRAIN_DQN = True)\n",
    "\n",
    "if TRAIN_DQN:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING DQN AGENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        import torch.optim as optim\n",
    "        from collections import deque\n",
    "        import random\n",
    "        \n",
    "        class DQNNetwork(nn.Module):\n",
    "            def __init__(self, state_dim, action_dim):\n",
    "                super().__init__()\n",
    "                self.net = nn.Sequential(\n",
    "                    nn.Linear(state_dim, 64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(64, 32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(32, action_dim)\n",
    "                )\n",
    "            \n",
    "            def forward(self, x):\n",
    "                return self.net(x)\n",
    "        \n",
    "        class DQNAgent:\n",
    "            def __init__(self, state_dim, action_dim):\n",
    "                self.state_dim = state_dim\n",
    "                self.action_dim = action_dim\n",
    "                self.epsilon = 1.0\n",
    "                self.epsilon_min = 0.05\n",
    "                self.epsilon_decay = 0.995\n",
    "                self.gamma = 0.99\n",
    "                self.lr = 0.001\n",
    "                self.memory = deque(maxlen=10000)\n",
    "                self.batch_size = 32\n",
    "                self.training_steps = 0\n",
    "                \n",
    "                self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                self.model = DQNNetwork(state_dim, action_dim).to(self.device)\n",
    "                self.target_model = DQNNetwork(state_dim, action_dim).to(self.device)\n",
    "                self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "                self.update_target()\n",
    "            \n",
    "            def update_target(self):\n",
    "                self.target_model.load_state_dict(self.model.state_dict())\n",
    "            \n",
    "            def act(self, state):\n",
    "                if random.random() < self.epsilon:\n",
    "                    return random.randint(0, self.action_dim - 1)\n",
    "                state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    q_values = self.model(state_t)\n",
    "                return q_values.argmax().item()\n",
    "            \n",
    "            def remember(self, state, action, reward, next_state, done):\n",
    "                self.memory.append((state, action, reward, next_state, done))\n",
    "            \n",
    "            def replay(self):\n",
    "                if len(self.memory) < self.batch_size:\n",
    "                    return\n",
    "                \n",
    "                batch = random.sample(self.memory, self.batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                \n",
    "                states = torch.FloatTensor(states).to(self.device)\n",
    "                actions = torch.LongTensor(actions).to(self.device)\n",
    "                rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "                next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "                dones = torch.FloatTensor(dones).to(self.device)\n",
    "                \n",
    "                current_q = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "                next_q = self.target_model(next_states).max(1)[0].detach()\n",
    "                target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "                \n",
    "                loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                self.training_steps += 1\n",
    "                if self.training_steps % 100 == 0:\n",
    "                    self.update_target()\n",
    "                \n",
    "                self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "            \n",
    "            def save(self, path):\n",
    "                torch.save(self.model.state_dict(), path)\n",
    "        \n",
    "        # Create simple environment\n",
    "        state_dim = min(30, len(X.columns))  # Limit state size\n",
    "        action_dim = 2  # 0 = wait, 1 = execute\n",
    "        \n",
    "        DQN_AGENT = DQNAgent(state_dim, action_dim)\n",
    "        \n",
    "        # Train for a few episodes\n",
    "        n_episodes = 500\n",
    "        print(f\"Training DQN for {n_episodes} episodes...\")\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            # Simple training loop\n",
    "            for i in range(min(100, len(X) - 1)):\n",
    "                state = X.iloc[i, :state_dim].values\n",
    "                action = DQN_AGENT.act(state)\n",
    "                \n",
    "                # Simple reward: negative gas price change if executing\n",
    "                next_gas = current_gas.iloc[i + 1] if i + 1 < len(current_gas) else current_gas.iloc[i]\n",
    "                reward = -(next_gas - current_gas.iloc[i]) if action == 1 else -0.001  # Small wait penalty\n",
    "                \n",
    "                next_state = X.iloc[i + 1, :state_dim].values if i + 1 < len(X) else state\n",
    "                done = (i >= min(99, len(X) - 2))\n",
    "                \n",
    "                DQN_AGENT.remember(state, action, reward, next_state, done)\n",
    "                DQN_AGENT.replay()\n",
    "            \n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f\"  Episode {episode + 1}/{n_episodes}, Epsilon: {DQN_AGENT.epsilon:.3f}\")\n",
    "        \n",
    "        DQN_TRAINED = True\n",
    "        DQN_METRICS = {\n",
    "            'episodes': n_episodes,\n",
    "            'training_steps': DQN_AGENT.training_steps,\n",
    "            'final_epsilon': float(DQN_AGENT.epsilon)\n",
    "        }\n",
    "        print(f\"\\n\u2713 DQN training complete ({DQN_AGENT.training_steps} steps)\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\u26a0\ufe0f PyTorch not available, skipping DQN training\")\n",
    "        DQN_TRAINED = False\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f DQN training failed: {e}\")\n",
    "        DQN_TRAINED = False\n",
    "else:\n",
    "    DQN_TRAINED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models - SIMPLIFIED\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json as json_lib\n",
    "\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# === Save prediction models ===\n",
    "for horizon in ['1h', '4h', '24h']:\n",
    "    if horizon not in trained_models:\n",
    "        print(f\"\u26a0\ufe0f  No {horizon} model to save\")\n",
    "        continue\n",
    "    \n",
    "    data = trained_models[horizon]\n",
    "    model = data['model']\n",
    "    scaler = data['scaler']\n",
    "    metrics = data['metrics']\n",
    "    \n",
    "    model_data = {\n",
    "        'model': model,\n",
    "        'model_name': metrics['name'],\n",
    "        'metrics': {\n",
    "            'mae': float(metrics['mae']),\n",
    "            'improvement': float(metrics['improvement']),\n",
    "            'passed_baseline': metrics['passed_baseline'],\n",
    "            'is_fallback': metrics.get('is_fallback', False)\n",
    "        },\n",
    "        'trained_at': datetime.now().isoformat(),\n",
    "        'feature_names': list(X.columns),\n",
    "        'feature_scaler': scaler,\n",
    "        'scaler_type': 'RobustScaler'\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_data, f'saved_models/model_{horizon}.pkl')\n",
    "    status = \"\u2713\" if metrics['passed_baseline'] else \"\u26a0\"\n",
    "    print(f\"{status} Saved model_{horizon}.pkl ({metrics['name']}, MAE={metrics['mae']:.6f})\")\n",
    "    \n",
    "    joblib.dump(scaler, f'saved_models/scaler_{horizon}.pkl')\n",
    "\n",
    "# === Save feature names ===\n",
    "joblib.dump(list(X.columns), 'saved_models/feature_names.pkl')\n",
    "print(f\"\\nSaved feature_names.pkl ({len(X.columns)} features)\")\n",
    "\n",
    "# === Save spike detectors (if trained) ===\n",
    "if 'spike_models' in dir() and spike_models:\n",
    "    for horizon, (clf, scaler) in spike_models.items():\n",
    "        spike_data = {\n",
    "            'model': clf,\n",
    "            'scaler': scaler,\n",
    "            'trained_at': datetime.now().isoformat()\n",
    "        }\n",
    "        joblib.dump(spike_data, f'saved_models/spike_detector_{horizon}.pkl')\n",
    "        print(f\"Saved spike_detector_{horizon}.pkl\")\n",
    "\n",
    "# === Save regime detector (if trained) ===\n",
    "if 'regime_clf' in dir() and regime_clf is not None:\n",
    "    regime_data = {\n",
    "        'model': regime_clf,\n",
    "        'scaler': regime_scaler,\n",
    "        'regimes': {0: 'Normal', 1: 'Elevated', 2: 'Spike'},\n",
    "        'accuracy': regime_accuracy,\n",
    "        'trained_at': datetime.now().isoformat()\n",
    "    }\n",
    "    joblib.dump(regime_data, 'saved_models/regime_detector.pkl')\n",
    "    print(f\"Saved regime_detector.pkl (Accuracy: {regime_accuracy:.1%})\")\n",
    "\n",
    "# === Save quantile models (if trained) ===\n",
    "if 'quantile_models' in dir() and quantile_models:\n",
    "    for horizon, (q_models, q_scaler) in quantile_models.items():\n",
    "        quantile_data = {\n",
    "            'models': q_models,\n",
    "            'scaler': q_scaler,\n",
    "            'quantiles': [0.1, 0.5, 0.9],\n",
    "            'trained_at': datetime.now().isoformat()\n",
    "        }\n",
    "        joblib.dump(quantile_data, f'saved_models/quantile_{horizon}.pkl')\n",
    "        print(f\"Saved quantile_{horizon}.pkl\")\n",
    "\n",
    "# === Save training metadata ===\n",
    "metadata = {\n",
    "    'training_timestamp': datetime.now().isoformat(),\n",
    "    'total_samples': len(df_clean),\n",
    "    'date_range': f\"{df_clean.index.min()} to {df_clean.index.max()}\",\n",
    "    'resampling': '30-second intervals',\n",
    "    'features': {\n",
    "        'total': len(X.columns),\n",
    "        'list': list(X.columns)\n",
    "    },\n",
    "    'baselines': BASELINES,\n",
    "    'models': {}\n",
    "}\n",
    "\n",
    "for horizon, data in trained_models.items():\n",
    "    m = data['metrics']\n",
    "    metadata['models'][horizon] = {\n",
    "        'name': m['name'],\n",
    "        'mae': float(m['mae']),\n",
    "        'improvement_pct': float(m['improvement'] * 100),\n",
    "        'passed_baseline': m['passed_baseline'],\n",
    "        'is_fallback': m.get('is_fallback', False)\n",
    "    }\n",
    "\n",
    "# Add direction model info if available (only metrics, not model objects)\n",
    "if 'direction_models' in dir() and direction_models:\n",
    "    metadata['direction_models'] = {\n",
    "        horizon: {'accuracy': data['accuracy'], 'f1_score': data['f1_score']}\n",
    "        for horizon, data in direction_models.items()\n",
    "    }\n",
    "\n",
    "# Convert numpy types to Python types for JSON serialization\n",
    "def convert_to_python_types(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_python_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_python_types(v) for v in obj]\n",
    "    elif isinstance(obj, (np.bool_, np.integer)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "metadata = convert_to_python_types(metadata)\n",
    "\n",
    "with open('saved_models/training_metadata.json', 'w') as f:\n",
    "    json_lib.dump(metadata, f, indent=2)\n",
    "print(f\"\\nSaved training_metadata.json\")\n",
    "\n",
    "# === Save feature importance ===\n",
    "with open('saved_models/feature_importance.json', 'w') as f:\n",
    "    json_lib.dump(FEATURE_IMPORTANCE, f, indent=2)\n",
    "print(f\"Saved feature_importance.json\")\n",
    "\n",
    "# === Save DQN Agent (if trained) ===\n",
    "if 'DQN_TRAINED' in dir() and DQN_TRAINED:\n",
    "    os.makedirs('saved_models/rl_agents', exist_ok=True)\n",
    "    DQN_AGENT.save('saved_models/rl_agents/dqn_agent.pt')\n",
    "    print(f\"\\nSaved dqn_agent.pt\")\n",
    "    \n",
    "    dqn_meta = {\n",
    "        'state_dim': DQN_AGENT.state_dim,\n",
    "        'action_dim': DQN_AGENT.action_dim,\n",
    "        'training_steps': DQN_AGENT.training_steps,\n",
    "        'epsilon': float(DQN_AGENT.epsilon),\n",
    "        'metrics': DQN_METRICS if 'DQN_METRICS' in dir() else {},\n",
    "        'trained_at': datetime.now().isoformat()\n",
    "    }\n",
    "    with open('saved_models/rl_agents/dqn_metadata.json', 'w') as f:\n",
    "        json_lib.dump(dqn_meta, f, indent=2)\n",
    "    print(\"Saved dqn_metadata.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL MODELS SAVED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# === FINAL SUMMARY ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples used: {len(df_clean):,}\")\n",
    "print(f\"Features: {len(X.columns)}\")\n",
    "print(f\"\\nModel Performance:\")\n",
    "for horizon, data in trained_models.items():\n",
    "    m = data['metrics']\n",
    "    status = \"\u2713 PASSED\" if m['passed_baseline'] else \"\u26a0 BELOW BASELINE\"\n",
    "    print(f\"  {horizon}: MAE={m['mae']:.6f} ({m['improvement']*100:+.1f}% vs baseline) {status}\")\n",
    "\n",
    "any_failed = any(not d['metrics']['passed_baseline'] for d in trained_models.values())\n",
    "if any_failed:\n",
    "    print(\"\\n\u26a0\ufe0f  WARNING: Some models did not beat baseline!\")\n",
    "    print(\"   Consider collecting more data before deploying.\")\n",
    "else:\n",
    "    print(\"\\n\u2713 All models beat baseline - ready for deployment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE - FINAL REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nDATA SUMMARY\")\n",
    "print(f\"   Total samples: {len(df_clean):,}\")\n",
    "print(f\"   Resampling: 30-second intervals\")\n",
    "print(f\"   Features: {len(X.columns)}\")\n",
    "print(f\"   Date range: {df_clean.index.min()} to {df_clean.index.max()}\")\n",
    "print(f\"   ETH price data: {'Yes' if HAS_ETH_PRICE else 'No'}\")\n",
    "\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(f\"{'PRICE PREDICTION MODELS':^70}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Horizon':<8} {'Model':<20} {'MAE':>10} {'vs Baseline':>15} {'Status':>12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for horizon in ['1h', '4h', '24h']:\n",
    "    if horizon in trained_models:\n",
    "        m = trained_models[horizon]['metrics']\n",
    "        name = m['name']\n",
    "        if m.get('is_fallback'):\n",
    "            name += ' (fallback)'\n",
    "        status = \"\u2713 PASSED\" if m['passed_baseline'] else \"\u26a0 FAILED\"\n",
    "        print(f\"{horizon:<8} {name:<20} {m['mae']:>10.6f} {m['improvement']*100:>+14.1f}% {status:>12}\")\n",
    "    else:\n",
    "        print(f\"{horizon:<8} {'Not trained':<20}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(f\"{'AUXILIARY MODELS':^70}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Direction models\n",
    "if 'direction_models' in dir() and direction_models:\n",
    "    print(\"\\nDirection Prediction:\")\n",
    "    for horizon, data in direction_models.items():\n",
    "        print(f\"   {horizon}: Accuracy={data['accuracy']:.1%}, F1={data['f1_score']:.3f}\")\n",
    "\n",
    "# Regime detector\n",
    "if 'regime_clf' in dir() and regime_clf is not None:\n",
    "    print(f\"\\nRegime Detection: Accuracy={regime_accuracy:.1%}\")\n",
    "\n",
    "# Spike detectors\n",
    "if 'spike_models' in dir() and spike_models:\n",
    "    print(f\"\\nSpike Detectors: {list(spike_models.keys())}\")\n",
    "\n",
    "# Quantile models\n",
    "if 'quantile_models' in dir() and quantile_models:\n",
    "    print(f\"\\nQuantile Models: {list(quantile_models.keys())}\")\n",
    "\n",
    "# DQN\n",
    "if 'DQN_TRAINED' in dir() and DQN_TRAINED:\n",
    "    print(f\"\\nDQN Agent: Trained ({DQN_AGENT.training_steps} steps)\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Final recommendation\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_passed = all(trained_models[h]['metrics']['passed_baseline'] \n",
    "                 for h in trained_models if h in trained_models)\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\u2713 All models beat baseline - READY FOR DEPLOYMENT\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  1. Download saved_models/ folder\")\n",
    "    print(\"  2. Copy to backend/models/saved_models/\")\n",
    "    print(\"  3. Restart backend\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Some models did not beat baseline\")\n",
    "    print(\"\\nRecommendations:\")\n",
    "    print(\"  - Collect more data before deploying\")\n",
    "    print(\"  - Current models may perform worse than naive prediction\")\n",
    "    print(\"  - Consider using only the models that passed\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Training data distribution\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(current_gas.values, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax1.set_xlabel('Gas Price (gwei)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Gas Price Distribution')\n",
    "ax1.axvline(current_gas.mean(), color='red', linestyle='--', label=f'Mean: {current_gas.mean():.2f}')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Model performance comparison\n",
    "ax2 = axes[0, 1]\n",
    "horizons = list(trained_models.keys())\n",
    "maes = [trained_models[h]['metrics']['mae'] for h in horizons]\n",
    "baselines = [BASELINES.get(h.replace('24h', '4h'), BASELINES['4h'])['best'] for h in horizons]\n",
    "\n",
    "x = np.arange(len(horizons))\n",
    "width = 0.35\n",
    "bars1 = ax2.bar(x - width/2, maes, width, label='Model MAE', color='steelblue')\n",
    "bars2 = ax2.bar(x + width/2, baselines, width, label='Baseline MAE', color='coral')\n",
    "ax2.set_xlabel('Horizon')\n",
    "ax2.set_ylabel('MAE (gwei)')\n",
    "ax2.set_title('Model vs Baseline Performance')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(horizons)\n",
    "ax2.legend()\n",
    "\n",
    "# Add improvement percentages\n",
    "for i, (h, m, b) in enumerate(zip(horizons, maes, baselines)):\n",
    "    imp = (b - m) / b * 100\n",
    "    color = 'green' if imp > 0 else 'red'\n",
    "    ax2.annotate(f'{imp:+.1f}%', xy=(i, max(m, b) + 0.02), ha='center', fontsize=9, color=color)\n",
    "\n",
    "# 3. Gas price time series (sample)\n",
    "ax3 = axes[1, 0]\n",
    "sample_size = min(2000, len(current_gas))\n",
    "sample_gas = current_gas.iloc[-sample_size:]\n",
    "ax3.plot(sample_gas.index, sample_gas.values, linewidth=0.5, alpha=0.8)\n",
    "ax3.set_xlabel('Time')\n",
    "ax3.set_ylabel('Gas Price (gwei)')\n",
    "ax3.set_title(f'Recent Gas Prices (last {sample_size} samples)')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Feature importance (top 10)\n",
    "ax4 = axes[1, 1]\n",
    "if FEATURE_IMPORTANCE:\n",
    "    sorted_imp = sorted(FEATURE_IMPORTANCE.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    features_plot = [f[0][:20] for f in sorted_imp]  # Truncate names\n",
    "    importances = [f[1] for f in sorted_imp]\n",
    "    \n",
    "    y_pos = np.arange(len(features_plot))\n",
    "    ax4.barh(y_pos, importances, color='teal')\n",
    "    ax4.set_yticks(y_pos)\n",
    "    ax4.set_yticklabels(features_plot)\n",
    "    ax4.invert_yaxis()\n",
    "    ax4.set_xlabel('Importance')\n",
    "    ax4.set_title('Top 10 Feature Importance')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'No feature importance data', ha='center', va='center')\n",
    "    ax4.set_title('Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('saved_models/training_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Saved training_results.png\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip file for download\n",
    "import shutil\n",
    "\n",
    "shutil.make_archive('gweizy_models', 'zip', 'saved_models')\n",
    "print(\"\\n\u2705 Created gweizy_models.zip\")\n",
    "print(\"\\nDownload this file and extract to: backend/models/saved_models/\")\n",
    "\n",
    "# Auto-download\n",
    "files.download('gweizy_models.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}