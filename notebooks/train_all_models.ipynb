{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gweizy Model Training Notebook\n",
    "\n",
    "Train all gas prediction models for Gweizy.\n",
    "\n",
    "## Instructions:\n",
    "1. Upload your `gas_data.db` file (from `backend/gas_data.db`)\n",
    "2. Run all cells\n",
    "3. Download the trained models zip file\n",
    "4. Extract to `backend/models/saved_models/` and push to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q scikit-learn pandas numpy joblib lightgbm xgboost matplotlib seaborn optuna"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your gas_data.db file\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"Upload your gas_data.db file from backend/gas_data.db\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "if 'gas_data.db' in uploaded:\n",
    "    print(f\"\\n\u2705 Uploaded gas_data.db ({len(uploaded['gas_data.db']) / 1024 / 1024:.1f} MB)\")\n",
    "else:\n",
    "    print(\"\u274c Please upload gas_data.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load data from database\n",
    "conn = sqlite3.connect('gas_data.db')\n",
    "df = pd.read_sql(\"\"\"\n",
    "    SELECT timestamp, current_gas as gas, base_fee, priority_fee, \n",
    "           block_number, gas_used, gas_limit, utilization\n",
    "    FROM gas_prices ORDER BY timestamp ASC\n",
    "\"\"\", conn)\n",
    "conn.close()\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df = df.set_index('timestamp').sort_index()\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "\n",
    "# === IMPROVED: Resample to 30-second intervals (was 1-min, losing too much data) ===\n",
    "print(\"\\nResampling to 30-second intervals (preserves more data)...\")\n",
    "df = df.resample('30s').mean().dropna(subset=['gas'])\n",
    "print(f\"After resample: {len(df):,} records\")\n",
    "\n",
    "# Find segments (gap > 30 min = new segment)\n",
    "df['time_diff'] = df.index.to_series().diff()\n",
    "df['segment'] = (df['time_diff'] > pd.Timedelta(minutes=30)).cumsum()\n",
    "\n",
    "segment_sizes = df.groupby('segment').size()\n",
    "print(f\"\\nSegments found: {len(segment_sizes)}\")\n",
    "print(f\"Segment sizes: {segment_sizes.sort_values(ascending=False).head(10).tolist()}\")\n",
    "\n",
    "# === IMPROVED: Lower threshold from 120 to 30 minutes (keeps more segments) ===\n",
    "MIN_SEGMENT_SIZE = 60  # 30 minutes at 30-sec intervals = 60 records\n",
    "good_segments = segment_sizes[segment_sizes >= MIN_SEGMENT_SIZE].index.tolist()\n",
    "df = df[df['segment'].isin(good_segments)]\n",
    "print(f\"\\nKeeping {len(good_segments)} segments with >= 30 minutes of data\")\n",
    "print(f\"Total usable records: {len(df):,}\")\n",
    "\n",
    "# === DATA SUFFICIENCY CHECK ===\n",
    "MIN_REQUIRED_SAMPLES = 10000\n",
    "if len(df) < MIN_REQUIRED_SAMPLES:\n",
    "    print(f\"\\n\u26a0\ufe0f  WARNING: Only {len(df):,} samples. Recommend at least {MIN_REQUIRED_SAMPLES:,}\")\n",
    "    print(\"   Models may underperform. Consider collecting more data.\")\n",
    "else:\n",
    "    print(f\"\\n\u2713 Data sufficiency check passed: {len(df):,} samples\")\n",
    "\n",
    "RECORDS_PER_HOUR = 120  # 30-sec intervals = 120 records per hour"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Fetch ETH Price Data - IMPROVED with Binance (1-minute data)\nimport requests\n\nprint(\"=\"*60)\nprint(\"FETCHING EXTERNAL DATA\")\nprint(\"=\"*60)\n\ndef fetch_eth_price_binance(start_date, end_date):\n    \"\"\"Fetch ETH price from Binance API (1-minute candles, much better than CoinGecko hourly)\"\"\"\n    try:\n        start_ts = int(start_date.timestamp() * 1000)\n        end_ts = int(end_date.timestamp() * 1000)\n        \n        all_prices = []\n        current_ts = start_ts\n        \n        print(f\"Fetching ETH prices from Binance (1-min candles)...\")\n        \n        while current_ts < end_ts:\n            url = \"https://api.binance.com/api/v3/klines\"\n            params = {\n                'symbol': 'ETHUSDT',\n                'interval': '1m',\n                'startTime': current_ts,\n                'endTime': min(current_ts + 1000 * 60 * 1000, end_ts),  # Max 1000 candles\n                'limit': 1000\n            }\n            \n            response = requests.get(url, params=params, timeout=30)\n            \n            if response.status_code == 200:\n                data = response.json()\n                if not data:\n                    break\n                    \n                for candle in data:\n                    all_prices.append({\n                        'timestamp': pd.to_datetime(candle[0], unit='ms'),\n                        'eth_price': float(candle[4]),  # Close price\n                        'eth_volume': float(candle[5]),  # Volume\n                        'eth_high': float(candle[2]),\n                        'eth_low': float(candle[3])\n                    })\n                \n                current_ts = data[-1][0] + 60000  # Next minute\n                \n                if len(all_prices) % 5000 == 0:\n                    print(f\"  Fetched {len(all_prices):,} candles...\")\n            else:\n                print(f\"  Binance API error: {response.status_code}\")\n                break\n        \n        if all_prices:\n            eth_df = pd.DataFrame(all_prices)\n            eth_df = eth_df.set_index('timestamp')\n            print(f\"  Total: {len(eth_df):,} 1-minute ETH candles\")\n            return eth_df\n        return None\n        \n    except Exception as e:\n        print(f\"  Failed to fetch from Binance: {e}\")\n        return None\n\ndef fetch_eth_price_coingecko(start_date, end_date):\n    \"\"\"Fallback: CoinGecko API (hourly data)\"\"\"\n    try:\n        start_ts = int(start_date.timestamp())\n        end_ts = int(end_date.timestamp())\n        \n        url = \"https://api.coingecko.com/api/v3/coins/ethereum/market_chart/range\"\n        params = {'vs_currency': 'usd', 'from': start_ts, 'to': end_ts}\n        \n        print(f\"Fallback: Fetching from CoinGecko (hourly)...\")\n        response = requests.get(url, params=params, timeout=30)\n        \n        if response.status_code == 200:\n            data = response.json()\n            prices = data.get('prices', [])\n            \n            eth_df = pd.DataFrame(prices, columns=['timestamp', 'eth_price'])\n            eth_df['timestamp'] = pd.to_datetime(eth_df['timestamp'], unit='ms')\n            eth_df = eth_df.set_index('timestamp')\n            eth_df['eth_volume'] = np.nan\n            eth_df['eth_high'] = eth_df['eth_price']\n            eth_df['eth_low'] = eth_df['eth_price']\n            \n            print(f\"  Fetched {len(eth_df)} hourly ETH prices\")\n            return eth_df\n        return None\n    except Exception as e:\n        print(f\"  CoinGecko failed: {e}\")\n        return None\n\n# Try Binance first, fallback to CoinGecko\neth_data = fetch_eth_price_binance(df.index.min(), df.index.max())\nif eth_data is None or len(eth_data) < 100:\n    eth_data = fetch_eth_price_coingecko(df.index.min(), df.index.max())\n\nhas_eth_data = False\nif eth_data is not None and len(eth_data) > 0:\n    # Resample to 30-second intervals\n    eth_data = eth_data.resample('30s').ffill()\n    \n    # Merge with gas data\n    df = df.join(eth_data, how='left')\n    df['eth_price'] = df['eth_price'].ffill().bfill()\n    \n    # Fill other ETH columns\n    for col in ['eth_volume', 'eth_high', 'eth_low']:\n        if col in df.columns:\n            df[col] = df[col].ffill().bfill()\n    \n    eth_coverage = df['eth_price'].notna().mean()\n    print(f\"  ETH price coverage: {eth_coverage:.1%}\")\n    \n    if eth_coverage > 0.5:\n        has_eth_data = True\n        print(\"  \u2713 ETH price data integrated (1-min resolution)\")\nelse:\n    print(\"  \u26a0\ufe0f No ETH price data available\")\n    df['eth_price'] = np.nan\n    df['eth_volume'] = np.nan\n    df['eth_high'] = np.nan\n    df['eth_low'] = np.nan\n\nHAS_ETH_PRICE = has_eth_data"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - SIMPLIFIED v3 + SPIKE-ADJUSTED TARGETS\n# Focus: 15-20 high-value features to prevent overfitting\n# NEW: Option for log-transformed or winsorized targets\n\nprint(\"Engineering SIMPLIFIED feature set (15-20 features)...\")\n\n# === CONFIGURATION ===\nTARGET_TRANSFORM = \"log\"  # Options: \"none\", \"log\", \"winsorize\"\nWINSORIZE_PERCENTILE = 0.95  # For winsorize: cap at this percentile\n\ndef engineer_features_for_segment(seg_df, has_eth=False, horizon='all'):\n    \"\"\"Engineer focused feature set - quality over quantity\"\"\"\n    df = seg_df.copy()\n    rph = 120  # records per hour (30-sec intervals)\n    \n    # === TIME FEATURES (3 features) ===\n    df['hour'] = df.index.hour\n    hour_of_day = df.index.hour + df.index.minute / 60\n    df['hour_sin'] = np.sin(2 * np.pi * hour_of_day / 24)\n    df['hour_cos'] = np.cos(2 * np.pi * hour_of_day / 24)\n    \n    # === ETH FEATURES (2 features) ===\n    if has_eth and 'eth_price' in df.columns and df['eth_price'].notna().any():\n        df['eth_log'] = np.log1p(df['eth_price'])\n        eth_mean = df['eth_price'].rolling(4*rph, min_periods=rph).mean()\n        eth_std = df['eth_price'].rolling(4*rph, min_periods=rph).std()\n        df['eth_zscore_4h'] = np.where(eth_std > 0.01, (df['eth_price'] - eth_mean) / eth_std, 0)\n        df['gas_eth_corr_1h'] = df['gas'].rolling(rph, min_periods=rph//2).corr(df['eth_price']).fillna(0)\n    \n    # === NETWORK UTILIZATION (2 features) ===\n    if 'utilization' in df.columns:\n        df['util_mean_1h'] = df['utilization'].rolling(rph, min_periods=rph//2).mean()\n        df['util_mean_2h'] = df['utilization'].rolling(2*rph, min_periods=rph).mean()\n    \n    # === GAS LAG FEATURES (5 features) ===\n    df['gas_lag_5min'] = df['gas'].shift(10)\n    df['gas_lag_15min'] = df['gas'].shift(30)\n    df['gas_lag_30min'] = df['gas'].shift(60)\n    df['gas_lag_1h'] = df['gas'].shift(rph)\n    df['gas_lag_4h'] = df['gas'].shift(4*rph)\n    \n    # === ROLLING STATS (6 features) ===\n    df['gas_mean_1h'] = df['gas'].rolling(rph, min_periods=rph//2).mean()\n    df['gas_std_1h'] = df['gas'].rolling(rph, min_periods=rph//2).std()\n    df['gas_cv_1h'] = np.where(df['gas_mean_1h'] > 0.01, \n                                df['gas_std_1h'] / df['gas_mean_1h'], 0)\n    df['gas_mean_2h'] = df['gas'].rolling(2*rph, min_periods=rph).mean()\n    df['gas_mean_4h'] = df['gas'].rolling(4*rph, min_periods=rph).mean()\n    \n    # === MOMENTUM (3 features) ===\n    df['momentum_1h'] = df['gas'] - df['gas'].shift(rph)\n    shift_2h = df['gas'].shift(2*rph)\n    df['momentum_pct_2h'] = np.where(shift_2h > 0.01, (df['gas'] - shift_2h) / shift_2h, 0)\n    df['trend_1h_4h'] = np.where(df['gas_mean_4h'] > 0.01, df['gas_mean_1h'] / df['gas_mean_4h'], 1.0)\n    \n    # === Z-SCORE AND REGIME (3 features) ===\n    df['gas_zscore_1h'] = np.where(df['gas_std_1h'] > 0.001, \n        (df['gas'] - df['gas_mean_1h']) / df['gas_std_1h'], 0)\n    df['is_spike'] = (df['gas'] > df['gas_mean_1h'] + 2 * df['gas_std_1h']).astype(int)\n    df['is_high_gas'] = (df['gas'] > df['gas'].rolling(4*rph, min_periods=rph).quantile(0.9)).astype(int)\n    \n    return df\n\n# Process each segment\nprint(\"\\n",
    "Processing segments...\")\nsegments = df['segment'].unique()\nprocessed_segments = []\n\nfor seg_id in segments:\n    seg_df = df[df['segment'] == seg_id].copy()\n    processed = engineer_features_for_segment(seg_df, has_eth=has_eth_data, horizon='all')\n    processed_segments.append(processed)\n\ndf_features = pd.concat(processed_segments, axis=0)\nprint(f\"After feature engineering: {len(df_features):,} records\")\n\n# Create targets\nprint(\"\\n",
    "Creating prediction targets...\")\n\ndef create_targets_for_segment(seg_df, transform=\"none\", winsorize_pct=0.95):\n    \"\"\"Create target variables with optional transformation\"\"\"\n    df = seg_df.copy()\n    rph = 120\n    \n    # Raw future prices\n    raw_1h = df['gas'].shift(-rph)\n    raw_4h = df['gas'].shift(-4*rph)\n    raw_24h = df['gas'].shift(-24*rph)\n    \n    # Apply transformation\n    if transform == \"log\":\n        # Log transform - better for multiplicative changes\n        df['target_1h'] = np.log1p(raw_1h)\n        df['target_4h'] = np.log1p(raw_4h)\n        df['target_24h'] = np.log1p(raw_24h)\n        # Also store raw for evaluation\n        df['target_1h_raw'] = raw_1h\n        df['target_4h_raw'] = raw_4h\n        df['target_24h_raw'] = raw_24h\n    elif transform == \"winsorize\":\n        # Winsorize - cap extreme values\n        cap_1h = raw_1h.quantile(winsorize_pct)\n        cap_4h = raw_4h.quantile(winsorize_pct)\n        cap_24h = raw_24h.quantile(winsorize_pct) if raw_24h.notna().sum() > 100 else cap_4h\n        df['target_1h'] = raw_1h.clip(upper=cap_1h)\n        df['target_4h'] = raw_4h.clip(upper=cap_4h)\n        df['target_24h'] = raw_24h.clip(upper=cap_24h)\n        df['target_1h_raw'] = raw_1h\n        df['target_4h_raw'] = raw_4h\n        df['target_24h_raw'] = raw_24h\n        print(f\"  Winsorized caps: 1h={cap_1h:.2f}, 4h={cap_4h:.2f}\")\n    else:\n        # No transform\n        df['target_1h'] = raw_1h\n        df['target_4h'] = raw_4h\n        df['target_24h'] = raw_24h\n    \n    # Direction classification (always on raw)\n    threshold = 0.02\n    for horizon in ['1h', '4h']:\n        raw_target = raw_1h if horizon == '1h' else raw_4h\n        pct_change = np.where(df['gas'] > 0.001, \n            (raw_target - df['gas']) / df['gas'], 0)\n        df[f'direction_class_{horizon}'] = pd.cut(\n            pct_change,\n            bins=[-float('inf'), -threshold, threshold, float('inf')],\n            labels=['down', 'stable', 'up']\n        )\n    \n    return df\n\nprint(f\"Target transform: {TARGET_TRANSFORM}\")\nprocessed_with_targets = []\nfor seg_id in df_features['segment'].unique():\n    seg_df = df_features[df_features['segment'] == seg_id].copy()\n    processed = create_targets_for_segment(seg_df, transform=TARGET_TRANSFORM, winsorize_pct=WINSORIZE_PERCENTILE)\n    processed_with_targets.append(processed)\n\ndf_features = pd.concat(processed_with_targets, axis=0)\n\n# Store transform info for later use\nTARGET_TRANSFORM_USED = TARGET_TRANSFORM\n\n# === CLEAN INF/NAN VALUES ===\nprint(\"\\n",
    "Cleaning inf/nan values...\")\nnumeric_cols = df_features.select_dtypes(include=[np.number]).columns\n\nfor col in numeric_cols:\n    df_features[col] = df_features[col].replace([np.inf, -np.inf], np.nan)\n    if df_features[col].notna().sum() > 0:\n        q_low = df_features[col].quantile(0.001)\n        q_high = df_features[col].quantile(0.999)\n        df_features[col] = df_features[col].clip(q_low, q_high)\n\ndf_features = df_features.ffill().bfill()\n\nfor col in numeric_cols:\n    if df_features[col].isna().any():\n        median_val = df_features[col].median()\n        if pd.isna(median_val):\n            median_val = 0\n        df_features[col] = df_features[col].fillna(median_val)\n\ninf_count = np.isinf(df_features.select_dtypes(include=[np.number])).sum().sum()\nnan_count = df_features.select_dtypes(include=[np.number]).isna().sum().sum()\nprint(f\"  After cleaning: {inf_count} inf, {nan_count} nan values\")\n\n# === DEFINE FOCUSED FEATURE SET ===\nCORE_FEATURES = [\n    'hour', 'hour_sin', 'hour_cos',\n    'eth_log', 'eth_zscore_4h', 'gas_eth_corr_1h',\n    'util_mean_1h', 'util_mean_2h',\n    'gas_lag_5min', 'gas_lag_15min', 'gas_lag_30min', 'gas_lag_1h', 'gas_lag_4h',\n    'gas_mean_1h', 'gas_std_1h', 'gas_cv_1h', 'gas_mean_2h', 'gas_mean_4h',\n    'momentum_1h', 'momentum_pct_2h', 'trend_1h_4h',\n    'gas_zscore_1h', 'is_spike', 'is_high_gas'\n]\n\navailable_features = [f for f in CORE_FEATURES if f in df_features.columns]\nfeatures_1h = available_features\nfeatures_4h = available_features  \nfeatures_24h = available_features\n\nprint(f\"\\n",
    "\u2713 Focused feature set: {len(available_features)} features\")\nprint(f\"  Features: {', '.join(available_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data - with AUTO-ADAPT to distribution shift\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy import stats\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "USE_ROLLING_WINDOW = False  # Set True to use only recent data (AUTO-ENABLED if shift detected)\n",
    "ROLLING_WINDOW_DAYS = 7     # Days of data to use if rolling window enabled\n",
    "HOLDOUT_HOURS = 48          # Hours to reserve for holdout\n",
    "AUTO_ADAPT_ON_SHIFT = True  # Automatically adapt when distribution shift detected\n",
    "\n",
    "# Only keep numeric columns\n",
    "numeric_features_1h = df_features[features_1h].select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_features_4h = df_features[features_4h].select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_features_24h = df_features[features_24h].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: 1h={len(numeric_features_1h)}, 4h={len(numeric_features_4h)}, 24h={len(numeric_features_24h)}\")\n",
    "\n",
    "# Drop rows only where TARGET columns are NaN\n",
    "target_cols = ['target_1h', 'target_4h']\n",
    "df_clean = df_features.dropna(subset=target_cols)\n",
    "print(f\"Clean samples (with valid 1h/4h targets): {len(df_clean):,}\")\n",
    "\n",
    "valid_24h = df_features['target_24h'].notna().sum()\n",
    "print(f\"Samples with valid 24h target: {valid_24h:,}\")\n",
    "\n",
    "# === OUT-OF-TIME HOLDOUT (do this FIRST to detect shift) ===\n",
    "rph = 120  # records per hour\n",
    "holdout_size = HOLDOUT_HOURS * rph\n",
    "\n",
    "if len(df_clean) > holdout_size + 5000:\n",
    "    df_train_val_initial = df_clean.iloc[:-holdout_size]\n",
    "    df_holdout = df_clean.iloc[-holdout_size:]\n",
    "    print(f\"\\n\u2713 Out-of-time holdout: {len(df_holdout):,} samples (last {HOLDOUT_HOURS}h)\")\n",
    "    HAS_HOLDOUT = True\n",
    "else:\n",
    "    df_train_val_initial = df_clean\n",
    "    df_holdout = None\n",
    "    print(f\"\\n\u26a0\ufe0f Not enough data for holdout, using all for training\")\n",
    "    HAS_HOLDOUT = False\n",
    "\n",
    "# === DISTRIBUTION SHIFT DETECTION ===\n",
    "def detect_distribution_shift(train_data, holdout_data, name=\"\"):\n",
    "    \"\"\"Detect distribution shift between train and holdout\"\"\"\n",
    "    results = {'name': name, 'warnings': [], 'passed': True, 'shift_magnitude': 0}\n",
    "    \n",
    "    train_mean, train_std = train_data.mean(), train_data.std()\n",
    "    holdout_mean = holdout_data.mean()\n",
    "    mean_shift = abs(holdout_mean - train_mean) / (train_std + 1e-8)\n",
    "    results['mean_shift_std'] = mean_shift\n",
    "    \n",
    "    if mean_shift > 1.0:\n",
    "        results['warnings'].append(f\"Large mean shift: {mean_shift:.2f} std devs\")\n",
    "        results['passed'] = False\n",
    "        results['shift_magnitude'] = max(results['shift_magnitude'], mean_shift)\n",
    "    elif mean_shift > 0.5:\n",
    "        results['warnings'].append(f\"Moderate mean shift: {mean_shift:.2f} std devs\")\n",
    "    \n",
    "    var_ratio = holdout_data.var() / (train_data.var() + 1e-8)\n",
    "    results['var_ratio'] = var_ratio\n",
    "    \n",
    "    if var_ratio > 4 or var_ratio < 0.25:\n",
    "        results['warnings'].append(f\"Large variance change: {var_ratio:.2f}x\")\n",
    "        results['passed'] = False\n",
    "        results['shift_magnitude'] = max(results['shift_magnitude'], abs(np.log(var_ratio)))\n",
    "    \n",
    "    ks_stat, ks_pval = stats.ks_2samp(train_data.values[:5000], holdout_data.values[:5000])\n",
    "    results['ks_statistic'] = ks_stat\n",
    "    results['ks_pvalue'] = ks_pval\n",
    "    \n",
    "    if ks_pval < 0.001 and ks_stat > 0.3:\n",
    "        results['warnings'].append(f\"KS test: distributions differ significantly\")\n",
    "        results['passed'] = False\n",
    "        results['shift_magnitude'] = max(results['shift_magnitude'], ks_stat * 3)\n",
    "    \n",
    "    train_spikes = (train_data > train_data.quantile(0.95)).mean()\n",
    "    holdout_spikes = (holdout_data > train_data.quantile(0.95)).mean()\n",
    "    spike_ratio = holdout_spikes / (train_spikes + 1e-8)\n",
    "    results['spike_ratio'] = spike_ratio\n",
    "    \n",
    "    if spike_ratio > 3:\n",
    "        results['warnings'].append(f\"Spike frequency {spike_ratio:.1f}x higher in holdout\")\n",
    "        results['passed'] = False\n",
    "        results['shift_magnitude'] = max(results['shift_magnitude'], spike_ratio / 2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "DISTRIBUTION_SHIFT_DETECTED = False\n",
    "SHIFT_MAGNITUDE = 0\n",
    "\n",
    "if HAS_HOLDOUT:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DISTRIBUTION SHIFT DETECTION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for horizon in ['1h', '4h']:\n",
    "        target_col = f'target_{horizon}'\n",
    "        train_targets = df_train_val_initial[target_col].dropna()\n",
    "        holdout_targets = df_holdout[target_col].dropna()\n",
    "        \n",
    "        shift_result = detect_distribution_shift(train_targets, holdout_targets, f\"{horizon} target\")\n",
    "        \n",
    "        status = \"\u2713 OK\" if shift_result['passed'] else \"\u26a0\ufe0f SHIFT DETECTED\"\n",
    "        print(f\"\\n{horizon}: {status}\")\n",
    "        print(f\"  Train:   mean={train_targets.mean():.4f}, std={train_targets.std():.4f}\")\n",
    "        print(f\"  Holdout: mean={holdout_targets.mean():.4f}, std={holdout_targets.std():.4f}\")\n",
    "        print(f\"  Mean shift: {shift_result['mean_shift_std']:.2f} std, Var ratio: {shift_result['var_ratio']:.2f}x\")\n",
    "        \n",
    "        if shift_result['warnings']:\n",
    "            for w in shift_result['warnings']:\n",
    "                print(f\"  \u26a0\ufe0f {w}\")\n",
    "            DISTRIBUTION_SHIFT_DETECTED = True\n",
    "            SHIFT_MAGNITUDE = max(SHIFT_MAGNITUDE, shift_result['shift_magnitude'])\n",
    "\n",
    "# === AUTO-ADAPT TO DISTRIBUTION SHIFT ===\n",
    "if DISTRIBUTION_SHIFT_DETECTED and AUTO_ADAPT_ON_SHIFT:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"AUTO-ADAPTING TO DISTRIBUTION SHIFT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Calculate adaptive window based on shift magnitude\n",
    "    if SHIFT_MAGNITUDE > 2:\n",
    "        adaptive_days = 3  # Severe shift - use very recent data\n",
    "    elif SHIFT_MAGNITUDE > 1:\n",
    "        adaptive_days = 5  # Moderate shift\n",
    "    else:\n",
    "        adaptive_days = 7  # Mild shift\n",
    "    \n",
    "    window_samples = adaptive_days * 24 * rph\n",
    "    \n",
    "    if len(df_train_val_initial) > window_samples:\n",
    "        df_train_val = df_train_val_initial.iloc[-window_samples:]\n",
    "        print(f\"\u2713 Auto-enabled rolling window: {adaptive_days} days ({len(df_train_val):,} samples)\")\n",
    "        print(f\"  Shift magnitude: {SHIFT_MAGNITUDE:.2f} \u2192 window: {adaptive_days} days\")\n",
    "        USE_ROLLING_WINDOW = True\n",
    "        ROLLING_WINDOW_DAYS = adaptive_days\n",
    "    else:\n",
    "        df_train_val = df_train_val_initial\n",
    "        print(f\"\u26a0\ufe0f Not enough data for adaptive window, using all training data\")\n",
    "elif USE_ROLLING_WINDOW:\n",
    "    # Manual rolling window\n",
    "    window_samples = ROLLING_WINDOW_DAYS * 24 * rph\n",
    "    if len(df_train_val_initial) > window_samples:\n",
    "        df_train_val = df_train_val_initial.iloc[-window_samples:]\n",
    "        print(f\"\\n\u2713 Rolling window: Using last {ROLLING_WINDOW_DAYS} days ({len(df_train_val):,} samples)\")\n",
    "    else:\n",
    "        df_train_val = df_train_val_initial\n",
    "else:\n",
    "    df_train_val = df_train_val_initial\n",
    "\n",
    "print(f\"\\nFinal training set: {len(df_train_val):,} samples\")\n",
    "\n",
    "# Final safety check\n",
    "for col in df_train_val.select_dtypes(include=[np.float64, np.float32, float]).columns:\n",
    "    df_train_val[col] = df_train_val[col].replace([np.inf, -np.inf], np.nan)\n",
    "    if df_train_val[col].isna().any():\n",
    "        df_train_val[col] = df_train_val[col].fillna(df_train_val[col].median())\n",
    "\n",
    "float_cols = df_train_val.select_dtypes(include=[np.float64, np.float32, float]).columns\n",
    "has_inf = any(np.isinf(df_train_val[col]).any() for col in float_cols)\n",
    "has_nan = any(np.isnan(df_train_val[col]).any() for col in float_cols)\n",
    "assert not has_inf, \"Data still contains inf!\"\n",
    "assert not has_nan, \"Data still contains nan!\"\n",
    "print(\"\u2713 Data validated: no inf/nan values\")\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_1h = df_train_val[numeric_features_1h]\n",
    "X_4h = df_train_val[numeric_features_4h]\n",
    "X_24h = df_train_val[numeric_features_24h]\n",
    "\n",
    "y_1h = df_train_val['target_1h']\n",
    "y_4h = df_train_val['target_4h']\n",
    "y_24h = df_train_val['target_24h']\n",
    "\n",
    "y_dir_1h = df_train_val['direction_class_1h']\n",
    "y_dir_4h = df_train_val['direction_class_4h']\n",
    "\n",
    "current_gas = df_train_val['gas']\n",
    "\n",
    "# === BASELINE MODELS (on both train AND holdout) ===\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BASELINE COMPARISONS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "naive_mae_1h = np.mean(np.abs(y_1h.values - current_gas.values))\n",
    "naive_mae_4h = np.mean(np.abs(y_4h.values - current_gas.values))\n",
    "\n",
    "mean_pred = np.full_like(y_1h.values, y_1h.mean())\n",
    "mean_mae_1h = np.mean(np.abs(y_1h.values - mean_pred))\n",
    "mean_mae_4h = np.mean(np.abs(y_4h.values - mean_pred))\n",
    "\n",
    "print(f\"\\nTRAINING SET Baseline MAEs:\")\n",
    "print(f\"  Naive (current price):     MAE_1h={naive_mae_1h:.6f}, MAE_4h={naive_mae_4h:.6f}\")\n",
    "print(f\"  Mean (historical average): MAE_1h={mean_mae_1h:.6f}, MAE_4h={mean_mae_4h:.6f}\")\n",
    "\n",
    "best_baseline_1h = min(naive_mae_1h, mean_mae_1h)\n",
    "best_baseline_4h = min(naive_mae_4h, mean_mae_4h)\n",
    "\n",
    "BASELINES = {\n",
    "    '1h': {'naive_mae': naive_mae_1h, 'mean_mae': mean_mae_1h, 'best': best_baseline_1h},\n",
    "    '4h': {'naive_mae': naive_mae_4h, 'mean_mae': mean_mae_4h, 'best': best_baseline_4h}\n",
    "}\n",
    "\n",
    "# === HOLDOUT BASELINES ===\n",
    "if HAS_HOLDOUT:\n",
    "    print(f\"\\nHOLDOUT SET Baseline MAEs:\")\n",
    "    holdout_gas = df_holdout['gas']\n",
    "    \n",
    "    for horizon in ['1h', '4h']:\n",
    "        holdout_target = df_holdout[f'target_{horizon}'].dropna()\n",
    "        holdout_current = holdout_gas.loc[holdout_target.index]\n",
    "        \n",
    "        holdout_naive_mae = np.mean(np.abs(holdout_target.values - holdout_current.values))\n",
    "        train_mean = df_train_val[f'target_{horizon}'].mean()\n",
    "        holdout_mean_mae = np.mean(np.abs(holdout_target.values - train_mean))\n",
    "        \n",
    "        holdout_best = min(holdout_naive_mae, holdout_mean_mae)\n",
    "        BASELINES[horizon]['holdout_naive_mae'] = holdout_naive_mae\n",
    "        BASELINES[horizon]['holdout_mean_mae'] = holdout_mean_mae\n",
    "        BASELINES[horizon]['holdout_best'] = holdout_best\n",
    "        \n",
    "        print(f\"  {horizon}: Naive={holdout_naive_mae:.6f}, Mean={holdout_mean_mae:.6f}, Best={holdout_best:.6f}\")\n",
    "    \n",
    "    for horizon in ['1h', '4h']:\n",
    "        train_best = BASELINES[horizon]['best']\n",
    "        holdout_best = BASELINES[horizon]['holdout_best']\n",
    "        ratio = holdout_best / (train_best + 1e-8)\n",
    "        if ratio > 2:\n",
    "            print(f\"\\n  \u26a0\ufe0f {horizon}: Holdout baseline {ratio:.1f}x worse than train - regime change!\")\n",
    "\n",
    "FEATURE_IMPORTANCE = {}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING DATA SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training samples: {len(df_train_val):,}\")\n",
    "print(f\"Holdout samples: {len(df_holdout) if df_holdout is not None else 0:,}\")\n",
    "print(f\"Features: {len(numeric_features_1h)}\")\n",
    "print(f\"Distribution shift: {DISTRIBUTION_SHIFT_DETECTED} (magnitude: {SHIFT_MAGNITUDE:.2f})\")\n",
    "print(f\"Auto-adapt enabled: {AUTO_ADAPT_ON_SHIFT}\")\n",
    "if USE_ROLLING_WINDOW:\n",
    "    print(f\"Rolling window: {ROLLING_WINDOW_DAYS} days\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training - WITH PERMUTATION IMPORTANCE\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, HuberRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "TRAIN_REGIME_MODELS = True\n",
    "MINIMUM_IMPROVEMENT = 0.05\n",
    "HOLDOUT_DEGRADATION_LIMIT = 0.30\n",
    "COMPUTE_PERMUTATION_IMPORTANCE = True  # Compute for all models\n",
    "\n",
    "def check_baseline_gate(model_mae, baseline_mae, model_name):\n",
    "    \"\"\"Check if model beats baseline by minimum threshold\"\"\"\n",
    "    improvement = (baseline_mae - model_mae) / baseline_mae\n",
    "    passed = improvement >= MINIMUM_IMPROVEMENT\n",
    "    if passed:\n",
    "        print(f\"  \u2713 PASSED baseline gate: {improvement*100:.1f}% improvement\")\n",
    "    else:\n",
    "        print(f\"  \u2717 FAILED baseline gate: {improvement*100:.1f}% (need {MINIMUM_IMPROVEMENT*100:.0f}%+)\")\n",
    "    return passed, improvement\n",
    "\n",
    "def check_holdout_gate(cv_mae, holdout_mae, model_name, holdout_baseline=None):\n",
    "    \"\"\"Check if holdout performance is acceptable\"\"\"\n",
    "    if cv_mae <= 0:\n",
    "        return False, 0\n",
    "    degradation = (holdout_mae - cv_mae) / cv_mae\n",
    "    \n",
    "    if holdout_baseline is not None:\n",
    "        holdout_improvement = (holdout_baseline - holdout_mae) / holdout_baseline\n",
    "        if holdout_improvement >= 0:\n",
    "            print(f\"  \u2713 Beats holdout baseline by {holdout_improvement*100:.1f}%\")\n",
    "            return True, degradation\n",
    "    \n",
    "    passed = degradation < HOLDOUT_DEGRADATION_LIMIT\n",
    "    if passed:\n",
    "        print(f\"  \u2713 PASSED holdout gate: {degradation*100:+.1f}% degradation\")\n",
    "    else:\n",
    "        print(f\"  \u2717 FAILED holdout gate: {degradation*100:+.1f}% degradation (limit: {HOLDOUT_DEGRADATION_LIMIT*100}%)\")\n",
    "    return passed, degradation\n",
    "\n",
    "def walk_forward_validate(model_class, model_params, X, y, baseline_mae, n_splits=5, purge_gap=120):\n",
    "    \"\"\"Walk-forward validation with purge gap\"\"\"\n",
    "    n = len(X)\n",
    "    fold_size = n // (n_splits + 1)\n",
    "    fold_results = []\n",
    "    \n",
    "    for fold in range(n_splits):\n",
    "        train_end = fold_size * (fold + 1)\n",
    "        test_start = train_end + purge_gap\n",
    "        test_end = test_start + fold_size\n",
    "        \n",
    "        if test_end > n:\n",
    "            break\n",
    "            \n",
    "        X_train = X.iloc[:train_end]\n",
    "        X_test = X.iloc[test_start:test_end]\n",
    "        y_train = y.iloc[:train_end]\n",
    "        y_test = y.iloc[test_start:test_end]\n",
    "        \n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        fold_results.append(mae)\n",
    "    \n",
    "    if not fold_results:\n",
    "        return None\n",
    "        \n",
    "    return {\n",
    "        'avg_mae': np.mean(fold_results),\n",
    "        'std_mae': np.std(fold_results),\n",
    "        'improvement': (baseline_mae - np.mean(fold_results)) / baseline_mae\n",
    "    }\n",
    "\n",
    "def get_models_to_try():\n",
    "    \"\"\"Get list of simple, robust models\"\"\"\n",
    "    return [\n",
    "        ('Ridge', Ridge, {'alpha': 1.0, 'random_state': 42}),\n",
    "        ('Huber', HuberRegressor, {'epsilon': 1.35, 'alpha': 0.1, 'max_iter': 1000}),\n",
    "        ('RF', RandomForestRegressor, {\n",
    "            'n_estimators': 30, 'max_depth': 4, 'min_samples_leaf': 20,\n",
    "            'random_state': 42, 'n_jobs': -1\n",
    "        }),\n",
    "        ('GBM', GradientBoostingRegressor, {\n",
    "            'n_estimators': 30, 'max_depth': 3, 'learning_rate': 0.1,\n",
    "            'min_samples_leaf': 20, 'random_state': 42\n",
    "        }),\n",
    "    ]\n",
    "\n",
    "def compute_permutation_importance(model, X, y, scaler, feature_names, n_repeats=5):\n",
    "    \"\"\"Compute permutation importance for any model type\"\"\"\n",
    "    X_scaled = scaler.transform(X)\n",
    "    \n",
    "    # Use sklearn's permutation_importance\n",
    "    result = permutation_importance(\n",
    "        model, X_scaled, y,\n",
    "        n_repeats=n_repeats,\n",
    "        random_state=42,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Convert to dictionary (higher = more important)\n",
    "    importance_dict = {}\n",
    "    for i, feat in enumerate(feature_names):\n",
    "        # Negative because we use neg_mae, so more negative = worse = more important\n",
    "        importance_dict[feat] = -result.importances_mean[i]\n",
    "    \n",
    "    # Normalize to sum to 1\n",
    "    total = sum(importance_dict.values())\n",
    "    if total > 0:\n",
    "        importance_dict = {k: v/total for k, v in importance_dict.items()}\n",
    "    \n",
    "    return importance_dict\n",
    "\n",
    "def train_model_with_holdout(X_train, y_train, X_holdout, y_holdout, baseline_mae, \n",
    "                             horizon_name, feature_names, holdout_baseline=None):\n",
    "    \"\"\"Train model and select based on HOLDOUT performance\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {horizon_name} model\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Train: {len(X_train):,}, Holdout: {len(X_holdout):,}, Features: {X_train.shape[1]}\")\n",
    "    print(f\"Train baseline: {baseline_mae:.6f}\", end=\"\")\n",
    "    if holdout_baseline:\n",
    "        print(f\", Holdout baseline: {holdout_baseline:.6f}\")\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "    models_to_try = get_models_to_try()\n",
    "    results = []\n",
    "    \n",
    "    for name, model_class, params in models_to_try:\n",
    "        print(f\"\\n[{name}]\")\n",
    "        try:\n",
    "            wf_result = walk_forward_validate(model_class, params, X_train, y_train, baseline_mae, n_splits=4, purge_gap=120)\n",
    "            if not wf_result:\n",
    "                continue\n",
    "                \n",
    "            cv_mae = wf_result['avg_mae']\n",
    "            print(f\"  CV MAE: {cv_mae:.6f} \u00b1 {wf_result['std_mae']:.6f}\")\n",
    "            \n",
    "            scaler = RobustScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_holdout_scaled = scaler.transform(X_holdout)\n",
    "            \n",
    "            model = model_class(**params)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            y_holdout_pred = model.predict(X_holdout_scaled)\n",
    "            holdout_mae = mean_absolute_error(y_holdout, y_holdout_pred)\n",
    "            holdout_improvement = (baseline_mae - holdout_mae) / baseline_mae\n",
    "            \n",
    "            # Calculate vs holdout baseline\n",
    "            if holdout_baseline:\n",
    "                vs_holdout = (holdout_baseline - holdout_mae) / holdout_baseline\n",
    "                print(f\"  HOLDOUT MAE: {holdout_mae:.6f} ({vs_holdout*100:+.1f}% vs holdout baseline)\")\n",
    "            else:\n",
    "                print(f\"  HOLDOUT MAE: {holdout_mae:.6f} ({holdout_improvement*100:+.1f}% vs train baseline)\")\n",
    "            \n",
    "            use_baseline = holdout_baseline if holdout_baseline else baseline_mae\n",
    "            passed_baseline, _ = check_baseline_gate(holdout_mae, use_baseline, name)\n",
    "            passed_holdout, degradation = check_holdout_gate(cv_mae, holdout_mae, name, holdout_baseline)\n",
    "            \n",
    "            if passed_baseline or (passed_holdout and holdout_improvement > 0):\n",
    "                results.append({\n",
    "                    'name': name, 'model_class': model_class, 'params': params,\n",
    "                    'cv_mae': cv_mae, 'holdout_mae': holdout_mae,\n",
    "                    'holdout_improvement': holdout_improvement,\n",
    "                    'vs_holdout_baseline': (holdout_baseline - holdout_mae) / holdout_baseline if holdout_baseline else None,\n",
    "                    'model': model, 'scaler': scaler\n",
    "                })\n",
    "                print(f\"  \u2192 Accepted\")\n",
    "            else:\n",
    "                print(f\"  \u2192 Rejected\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Failed: {e}\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"\\n\u26a0\ufe0f All models failed! Using Huber fallback...\")\n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        model = HuberRegressor(epsilon=1.35, alpha=0.1, max_iter=1000)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_holdout_pred = model.predict(scaler.transform(X_holdout))\n",
    "        holdout_mae = mean_absolute_error(y_holdout, y_holdout_pred)\n",
    "        \n",
    "        importance = {}\n",
    "        if COMPUTE_PERMUTATION_IMPORTANCE:\n",
    "            print(\"  Computing permutation importance...\")\n",
    "            importance = compute_permutation_importance(model, X_holdout, y_holdout, scaler, feature_names)\n",
    "        \n",
    "        return model, scaler, {\n",
    "            'name': 'Huber (fallback)',\n",
    "            'mae': holdout_mae,\n",
    "            'improvement': (baseline_mae - holdout_mae) / baseline_mae,\n",
    "            'vs_holdout_baseline': (holdout_baseline - holdout_mae) / holdout_baseline if holdout_baseline else None,\n",
    "            'passed_baseline': False,\n",
    "            'is_fallback': True\n",
    "        }, importance\n",
    "    \n",
    "    best = min(results, key=lambda x: x['holdout_mae'])\n",
    "    print(f\"\\n>>> Best: {best['name']} (Holdout MAE: {best['holdout_mae']:.6f})\")\n",
    "    \n",
    "    # Compute permutation importance for the best model\n",
    "    importance = {}\n",
    "    if COMPUTE_PERMUTATION_IMPORTANCE:\n",
    "        print(\"  Computing permutation importance...\")\n",
    "        importance = compute_permutation_importance(best['model'], X_holdout, y_holdout, best['scaler'], feature_names)\n",
    "        top_3 = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        print(f\"  Top features: {', '.join([f'{f[0]}({f[1]:.2f})' for f in top_3])}\")\n",
    "    elif hasattr(best['model'], 'feature_importances_'):\n",
    "        importance = dict(zip(feature_names, best['model'].feature_importances_))\n",
    "    elif hasattr(best['model'], 'coef_'):\n",
    "        importance = dict(zip(feature_names, np.abs(best['model'].coef_)))\n",
    "    \n",
    "    return best['model'], best['scaler'], {\n",
    "        'name': best['name'],\n",
    "        'mae': best['holdout_mae'],\n",
    "        'cv_mae': best['cv_mae'],\n",
    "        'improvement': best['holdout_improvement'],\n",
    "        'vs_holdout_baseline': best['vs_holdout_baseline'],\n",
    "        'passed_baseline': True,\n",
    "        'is_fallback': False\n",
    "    }, importance\n",
    "\n",
    "def train_regime_models(X_train, y_train, X_holdout, y_holdout, regime_train, regime_holdout,\n",
    "                        baseline_mae, horizon_name, feature_names, holdout_baseline=None):\n",
    "    \"\"\"Train separate models for each regime\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training REGIME-SPECIFIC {horizon_name} models\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    regime_models = {}\n",
    "    \n",
    "    for regime_val, regime_name in [(0, 'normal'), (1, 'elevated'), (2, 'spike')]:\n",
    "        train_mask = regime_train == regime_val\n",
    "        holdout_mask = regime_holdout == regime_val\n",
    "        \n",
    "        n_train = train_mask.sum()\n",
    "        n_holdout = holdout_mask.sum()\n",
    "        \n",
    "        print(f\"\\n[{regime_name.upper()}] Train: {n_train}, Holdout: {n_holdout}\")\n",
    "        \n",
    "        if n_train < 500 or n_holdout < 100:\n",
    "            print(f\"  Insufficient data, skipping\")\n",
    "            continue\n",
    "        \n",
    "        X_r_train = X_train[train_mask]\n",
    "        y_r_train = y_train[train_mask]\n",
    "        X_r_holdout = X_holdout[holdout_mask]\n",
    "        y_r_holdout = y_holdout[holdout_mask]\n",
    "        \n",
    "        model, scaler, metrics, importance = train_model_with_holdout(\n",
    "            X_r_train, y_r_train, X_r_holdout, y_r_holdout,\n",
    "            baseline_mae, f\"{horizon_name}_{regime_name}\", feature_names, holdout_baseline\n",
    "        )\n",
    "        \n",
    "        if model:\n",
    "            regime_models[regime_val] = {\n",
    "                'model': model, 'scaler': scaler, 'metrics': metrics,\n",
    "                'regime_name': regime_name, 'n_samples': n_train\n",
    "            }\n",
    "    \n",
    "    return regime_models\n",
    "\n",
    "def print_distribution_diagnostics(y_train, y_holdout, name=\"\"):\n",
    "    \"\"\"Print diagnostics\"\"\"\n",
    "    print(f\"\\n[Distribution - {name}]\")\n",
    "    print(f\"  Train:   mean={y_train.mean():.4f}, std={y_train.std():.4f}\")\n",
    "    print(f\"  Holdout: mean={y_holdout.mean():.4f}, std={y_holdout.std():.4f}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models with ENSEMBLE REGIME SWITCHING\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING ALL PREDICTION MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trained_models = {}\n",
    "regime_specific_models = {}\n",
    "all_feature_importance = {}\n",
    "\n",
    "if not HAS_HOLDOUT or df_holdout is None or len(df_holdout) < 1000:\n",
    "    print(\"\\n\u26a0\ufe0f WARNING: Limited holdout data\")\n",
    "\n",
    "print(f\"\\nTraining set: {len(df_train_val):,} samples\")\n",
    "print(f\"Holdout set:  {len(df_holdout) if df_holdout is not None else 0:,} samples\")\n",
    "if DISTRIBUTION_SHIFT_DETECTED:\n",
    "    print(f\"\u26a0\ufe0f Distribution shift detected (magnitude: {SHIFT_MAGNITUDE:.2f})\")\n",
    "    if USE_ROLLING_WINDOW:\n",
    "        print(f\"   Auto-adapted to {ROLLING_WINDOW_DAYS}-day rolling window\")\n",
    "\n",
    "# === CREATE REGIME LABELS ===\n",
    "print(\"\\nCreating regime labels...\")\n",
    "regime_train = pd.Series(0, index=df_train_val.index)\n",
    "if 'gas_zscore_1h' in df_train_val.columns:\n",
    "    regime_train[df_train_val['gas_zscore_1h'] > 1] = 1\n",
    "if 'is_spike' in df_train_val.columns:\n",
    "    regime_train[df_train_val['is_spike'] == 1] = 2\n",
    "\n",
    "regime_holdout = None\n",
    "if HAS_HOLDOUT:\n",
    "    regime_holdout = pd.Series(0, index=df_holdout.index)\n",
    "    if 'gas_zscore_1h' in df_holdout.columns:\n",
    "        regime_holdout[df_holdout['gas_zscore_1h'] > 1] = 1\n",
    "    if 'is_spike' in df_holdout.columns:\n",
    "        regime_holdout[df_holdout['is_spike'] == 1] = 2\n",
    "\n",
    "print(f\"Regime distribution (train): {dict(regime_train.value_counts().sort_index())}\")\n",
    "if regime_holdout is not None:\n",
    "    print(f\"Regime distribution (holdout): {dict(regime_holdout.value_counts().sort_index())}\")\n",
    "\n",
    "# === ENSEMBLE PREDICTION FUNCTION ===\n",
    "def create_ensemble_predictor(global_model, global_scaler, regime_models, features):\n",
    "    \"\"\"Create a predictor that uses regime-specific models when available\"\"\"\n",
    "    def predict(X, current_regime=None):\n",
    "        X_scaled = global_scaler.transform(X)\n",
    "        global_pred = global_model.predict(X_scaled)\n",
    "        \n",
    "        if current_regime is not None and regime_models and current_regime in regime_models:\n",
    "            regime_data = regime_models[current_regime]\n",
    "            X_regime_scaled = regime_data['scaler'].transform(X)\n",
    "            regime_pred = regime_data['model'].predict(X_regime_scaled)\n",
    "            # Weighted average: 70% regime, 30% global\n",
    "            return 0.7 * regime_pred + 0.3 * global_pred\n",
    "        \n",
    "        return global_pred\n",
    "    \n",
    "    return predict\n",
    "\n",
    "# === 1H MODEL ===\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"1-HOUR MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_holdout_1h = df_holdout[numeric_features_1h] if HAS_HOLDOUT else X_1h.iloc[-1000:]\n",
    "y_holdout_1h = df_holdout['target_1h'] if HAS_HOLDOUT else y_1h.iloc[-1000:]\n",
    "mask_1h = y_holdout_1h.notna()\n",
    "X_holdout_1h = X_holdout_1h[mask_1h]\n",
    "y_holdout_1h = y_holdout_1h[mask_1h]\n",
    "\n",
    "print_distribution_diagnostics(y_1h, y_holdout_1h, \"1h targets\")\n",
    "\n",
    "holdout_baseline_1h = BASELINES['1h'].get('holdout_best', None)\n",
    "\n",
    "model_1h, scaler_1h, metrics_1h, importance_1h = train_model_with_holdout(\n",
    "    X_1h, y_1h, X_holdout_1h, y_holdout_1h,\n",
    "    BASELINES['1h']['best'], '1h', numeric_features_1h, holdout_baseline_1h\n",
    ")\n",
    "if model_1h:\n",
    "    trained_models['1h'] = {\n",
    "        'model': model_1h, 'scaler': scaler_1h, \n",
    "        'metrics': metrics_1h, 'features': numeric_features_1h\n",
    "    }\n",
    "    if importance_1h:\n",
    "        all_feature_importance['1h'] = importance_1h\n",
    "\n",
    "# Train regime-specific models\n",
    "if TRAIN_REGIME_MODELS and regime_holdout is not None:\n",
    "    regime_holdout_1h = regime_holdout[mask_1h]\n",
    "    regime_models_1h = train_regime_models(\n",
    "        X_1h, y_1h, X_holdout_1h, y_holdout_1h,\n",
    "        regime_train, regime_holdout_1h,\n",
    "        BASELINES['1h']['best'], '1h', numeric_features_1h, holdout_baseline_1h\n",
    "    )\n",
    "    if regime_models_1h:\n",
    "        regime_specific_models['1h'] = regime_models_1h\n",
    "        # Create ensemble predictor\n",
    "        trained_models['1h']['ensemble_predict'] = create_ensemble_predictor(\n",
    "            model_1h, scaler_1h, regime_models_1h, numeric_features_1h\n",
    "        )\n",
    "\n",
    "# === 4H MODEL ===\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"4-HOUR MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_holdout_4h = df_holdout[numeric_features_4h] if HAS_HOLDOUT else X_4h.iloc[-1000:]\n",
    "y_holdout_4h = df_holdout['target_4h'] if HAS_HOLDOUT else y_4h.iloc[-1000:]\n",
    "mask_4h = y_holdout_4h.notna()\n",
    "X_holdout_4h = X_holdout_4h[mask_4h]\n",
    "y_holdout_4h = y_holdout_4h[mask_4h]\n",
    "\n",
    "print_distribution_diagnostics(y_4h, y_holdout_4h, \"4h targets\")\n",
    "\n",
    "holdout_baseline_4h = BASELINES['4h'].get('holdout_best', None)\n",
    "\n",
    "model_4h, scaler_4h, metrics_4h, importance_4h = train_model_with_holdout(\n",
    "    X_4h, y_4h, X_holdout_4h, y_holdout_4h,\n",
    "    BASELINES['4h']['best'], '4h', numeric_features_4h, holdout_baseline_4h\n",
    ")\n",
    "if model_4h:\n",
    "    trained_models['4h'] = {\n",
    "        'model': model_4h, 'scaler': scaler_4h,\n",
    "        'metrics': metrics_4h, 'features': numeric_features_4h\n",
    "    }\n",
    "    if importance_4h:\n",
    "        all_feature_importance['4h'] = importance_4h\n",
    "\n",
    "if TRAIN_REGIME_MODELS and regime_holdout is not None:\n",
    "    regime_holdout_4h = regime_holdout[mask_4h]\n",
    "    regime_models_4h = train_regime_models(\n",
    "        X_4h, y_4h, X_holdout_4h, y_holdout_4h,\n",
    "        regime_train, regime_holdout_4h,\n",
    "        BASELINES['4h']['best'], '4h', numeric_features_4h, holdout_baseline_4h\n",
    "    )\n",
    "    if regime_models_4h:\n",
    "        regime_specific_models['4h'] = regime_models_4h\n",
    "        trained_models['4h']['ensemble_predict'] = create_ensemble_predictor(\n",
    "            model_4h, scaler_4h, regime_models_4h, numeric_features_4h\n",
    "        )\n",
    "\n",
    "# === 24H MODEL ===\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"24-HOUR MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rph = 120\n",
    "total_hours = len(df_clean) / rph\n",
    "total_days = total_hours / 24\n",
    "print(f\"Total data: {total_days:.1f} days\")\n",
    "\n",
    "if total_days >= 30:\n",
    "    mask_24h_train = y_24h.notna()\n",
    "    X_24h_valid = X_24h[mask_24h_train]\n",
    "    y_24h_valid = y_24h[mask_24h_train]\n",
    "    \n",
    "    if HAS_HOLDOUT:\n",
    "        y_holdout_24h = df_holdout['target_24h']\n",
    "        mask_24h_holdout = y_holdout_24h.notna()\n",
    "        X_holdout_24h = df_holdout[numeric_features_24h][mask_24h_holdout]\n",
    "        y_holdout_24h = y_holdout_24h[mask_24h_holdout]\n",
    "    else:\n",
    "        X_holdout_24h = X_24h_valid.iloc[-500:]\n",
    "        y_holdout_24h = y_24h_valid.iloc[-500:]\n",
    "    \n",
    "    if len(y_holdout_24h) > 100:\n",
    "        model_24h, scaler_24h, metrics_24h, _ = train_model_with_holdout(\n",
    "            X_24h_valid, y_24h_valid, X_holdout_24h, y_holdout_24h,\n",
    "            BASELINES['4h']['best'], '24h', numeric_features_24h\n",
    "        )\n",
    "        if model_24h:\n",
    "            trained_models['24h'] = {\n",
    "                'model': model_24h, 'scaler': scaler_24h,\n",
    "                'metrics': metrics_24h, 'features': numeric_features_24h,\n",
    "                'is_fallback': False\n",
    "            }\n",
    "    else:\n",
    "        print(f\"\u26a0\ufe0f Using 4h model as 24h fallback\")\n",
    "        if model_4h:\n",
    "            trained_models['24h'] = {\n",
    "                'model': model_4h, 'scaler': scaler_4h,\n",
    "                'metrics': {'name': metrics_4h['name'] + ' (4h fallback)', 'mae': metrics_4h['mae'],\n",
    "                           'improvement': metrics_4h['improvement'], \n",
    "                           'vs_holdout_baseline': metrics_4h.get('vs_holdout_baseline'),\n",
    "                           'passed_baseline': metrics_4h.get('passed_baseline', False)},\n",
    "                'features': numeric_features_4h,\n",
    "                'is_fallback': True\n",
    "            }\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f Using 4h model as 24h fallback ({total_days:.1f} days < 30)\")\n",
    "    if model_4h:\n",
    "        trained_models['24h'] = {\n",
    "            'model': model_4h, 'scaler': scaler_4h,\n",
    "            'metrics': {'name': metrics_4h['name'] + ' (4h fallback)', 'mae': metrics_4h['mae'],\n",
    "                       'improvement': metrics_4h['improvement'],\n",
    "                       'vs_holdout_baseline': metrics_4h.get('vs_holdout_baseline'),\n",
    "                       'passed_baseline': metrics_4h.get('passed_baseline', False)},\n",
    "            'features': numeric_features_4h,\n",
    "            'is_fallback': True\n",
    "        }\n",
    "\n",
    "# === SUMMARY ===\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for horizon, data in trained_models.items():\n",
    "    m = data['metrics']\n",
    "    status = \"\u2713\" if m.get('passed_baseline', False) else \"\u26a0\"\n",
    "    fallback = \" (fallback)\" if data.get('is_fallback') else \"\"\n",
    "    \n",
    "    # Show vs holdout baseline if available\n",
    "    if m.get('vs_holdout_baseline') is not None:\n",
    "        vs_baseline = f\"{m['vs_holdout_baseline']*100:+.1f}% vs holdout baseline\"\n",
    "    else:\n",
    "        vs_baseline = f\"{m['improvement']*100:+.1f}% vs train baseline\"\n",
    "    \n",
    "    has_ensemble = \" [+ensemble]\" if 'ensemble_predict' in data else \"\"\n",
    "    print(f\"{status} {horizon}: {m['name']}{fallback} | MAE: {m['mae']:.4f} | {vs_baseline}{has_ensemble}\")\n",
    "\n",
    "if regime_specific_models:\n",
    "    print(f\"\\nRegime-specific models:\")\n",
    "    for horizon, regime_dict in regime_specific_models.items():\n",
    "        for regime_val, regime_data in regime_dict.items():\n",
    "            print(f\"  {horizon}_{regime_data['regime_name']}: MAE={regime_data['metrics']['mae']:.4f}\")\n",
    "\n",
    "FEATURE_IMPORTANCE = all_feature_importance.get('4h', all_feature_importance.get('1h', {}))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# PREDICTION INTERVALS - WITH UNCERTAINTY SCALING\n",
    "from sklearn.ensemble import GradientBoostingRegressor, IsolationForest\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING PREDICTION INTERVALS (Conformal + Uncertainty Scaling)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "quantile_models = {}\n",
    "conformal_residuals = {}\n",
    "uncertainty_scalers = {}\n",
    "\n",
    "def train_conformal_intervals(X, y, model, scaler, horizon, alpha=0.2):\n",
    "    \"\"\"Conformal prediction for guaranteed coverage. alpha=0.2 means 80% interval\"\"\"\n",
    "    cal_size = int(len(X) * 0.2)\n",
    "    X_train, X_cal = X.iloc[:-cal_size], X.iloc[-cal_size:]\n",
    "    y_train, y_cal = y.iloc[:-cal_size], y.iloc[-cal_size:]\n",
    "    \n",
    "    X_cal_scaled = scaler.transform(X_cal)\n",
    "    y_pred_cal = model.predict(X_cal_scaled)\n",
    "    \n",
    "    residuals = np.abs(y_cal.values - y_pred_cal)\n",
    "    q = np.quantile(residuals, 1 - alpha)\n",
    "    \n",
    "    return {\n",
    "        'quantile': q,\n",
    "        'residuals': residuals,\n",
    "        'coverage_target': 1 - alpha\n",
    "    }\n",
    "\n",
    "def train_uncertainty_scaler(X_train, scaler, residuals, features):\n",
    "    \"\"\"\n",
    "    Train a model to predict when uncertainty should be higher.\n",
    "    Uses:\n",
    "    1. Isolation Forest to detect out-of-distribution samples\n",
    "    2. Volatility features to detect high-uncertainty periods\n",
    "    \"\"\"\n",
    "    X_scaled = scaler.transform(X_train)\n",
    "    \n",
    "    # Train Isolation Forest to detect OOD samples\n",
    "    iso_forest = IsolationForest(\n",
    "        n_estimators=50, contamination=0.1,\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    iso_forest.fit(X_scaled)\n",
    "    \n",
    "    # Calculate feature statistics for OOD detection\n",
    "    feature_means = X_scaled.mean(axis=0)\n",
    "    feature_stds = X_scaled.std(axis=0) + 1e-8\n",
    "    \n",
    "    # Calculate baseline interval width\n",
    "    base_interval = np.quantile(residuals, 0.8)\n",
    "    \n",
    "    return {\n",
    "        'iso_forest': iso_forest,\n",
    "        'feature_means': feature_means,\n",
    "        'feature_stds': feature_stds,\n",
    "        'base_interval': base_interval,\n",
    "        'features': features\n",
    "    }\n",
    "\n",
    "def calculate_uncertainty_multiplier(X_sample, uncertainty_scaler, current_volatility=None):\n",
    "    \"\"\"\n",
    "    Calculate how much to scale the prediction interval.\n",
    "    Returns multiplier >= 1.0\n",
    "    \"\"\"\n",
    "    multiplier = 1.0\n",
    "    \n",
    "    # 1. Out-of-distribution detection (Isolation Forest)\n",
    "    iso_score = uncertainty_scaler['iso_forest'].decision_function(X_sample.reshape(1, -1))[0]\n",
    "    # iso_score < 0 means anomaly (OOD)\n",
    "    if iso_score < -0.1:\n",
    "        ood_multiplier = 1 + abs(iso_score)  # Scale by how anomalous\n",
    "        multiplier *= min(ood_multiplier, 2.0)  # Cap at 2x\n",
    "    \n",
    "    # 2. Distance from training distribution\n",
    "    z_scores = np.abs((X_sample - uncertainty_scaler['feature_means']) / uncertainty_scaler['feature_stds'])\n",
    "    max_z = np.max(z_scores)\n",
    "    if max_z > 3:\n",
    "        dist_multiplier = 1 + (max_z - 3) * 0.2  # 20% increase per std beyond 3\n",
    "        multiplier *= min(dist_multiplier, 2.0)\n",
    "    \n",
    "    # 3. Volatility regime (if provided)\n",
    "    if current_volatility is not None:\n",
    "        if current_volatility == 2:  # Spike regime\n",
    "            multiplier *= 1.5\n",
    "        elif current_volatility == 1:  # Elevated regime\n",
    "            multiplier *= 1.2\n",
    "    \n",
    "    return min(multiplier, 3.0)  # Cap total multiplier at 3x\n",
    "\n",
    "for horizon in ['1h', '4h']:\n",
    "    if horizon not in trained_models:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n{horizon} prediction intervals...\")\n",
    "    \n",
    "    data = trained_models[horizon]\n",
    "    features = data['features']\n",
    "    \n",
    "    X_h = df_train_val[features]\n",
    "    y_h = df_train_val[f'target_{horizon}']\n",
    "    \n",
    "    mask = y_h.notna()\n",
    "    X_h = X_h[mask]\n",
    "    y_h = y_h[mask]\n",
    "    \n",
    "    if len(X_h) < 1000:\n",
    "        print(f\"  \u26a0\ufe0f Insufficient data for {horizon} intervals, skipping\")\n",
    "        continue\n",
    "    \n",
    "    split_idx = int(len(X_h) * 0.8)\n",
    "    X_train, X_test = X_h.iloc[:split_idx], X_h.iloc[split_idx:]\n",
    "    y_train, y_test = y_h.iloc[:split_idx], y_h.iloc[split_idx:]\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # === Quantile Regression ===\n",
    "    q_models = {}\n",
    "    for q in [0.1, 0.5, 0.9]:\n",
    "        model = GradientBoostingRegressor(\n",
    "            loss='quantile', alpha=q,\n",
    "            n_estimators=50, max_depth=4,  # Reduced complexity\n",
    "            learning_rate=0.1, random_state=42\n",
    "        )\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        q_models[q] = model\n",
    "    \n",
    "    quantile_models[horizon] = (q_models, scaler)\n",
    "    print(f\"  \u2713 Quantile models trained (10th, 50th, 90th percentiles)\")\n",
    "    \n",
    "    # === Conformal Prediction ===\n",
    "    conformal = train_conformal_intervals(X_h, y_h, data['model'], data['scaler'], horizon, alpha=0.2)\n",
    "    conformal_residuals[horizon] = conformal\n",
    "    print(f\"  \u2713 Conformal interval: \u00b1{conformal['quantile']:.4f} gwei (80% coverage)\")\n",
    "    \n",
    "    # === Uncertainty Scaler ===\n",
    "    unc_scaler = train_uncertainty_scaler(X_train, scaler, conformal['residuals'], features)\n",
    "    uncertainty_scalers[horizon] = unc_scaler\n",
    "    print(f\"  \u2713 Uncertainty scaler trained (OOD detection + volatility scaling)\")\n",
    "    \n",
    "    # === Calibration Check with Uncertainty Scaling ===\n",
    "    print(f\"  Calibration check...\")\n",
    "    \n",
    "    y_pred_test = data['model'].predict(data['scaler'].transform(X_test))\n",
    "    \n",
    "    # Standard quantile interval coverage\n",
    "    q_low = q_models[0.1].predict(X_test_scaled)\n",
    "    q_high = q_models[0.9].predict(X_test_scaled)\n",
    "    q_coverage = np.mean((y_test.values >= q_low) & (y_test.values <= q_high))\n",
    "    \n",
    "    # Standard conformal interval coverage\n",
    "    conf_low = y_pred_test - conformal['quantile']\n",
    "    conf_high = y_pred_test + conformal['quantile']\n",
    "    conf_coverage = np.mean((y_test.values >= conf_low) & (y_test.values <= conf_high))\n",
    "    \n",
    "    # Scaled conformal interval coverage (with uncertainty multipliers)\n",
    "    scaled_coverages = []\n",
    "    for i, (idx, row) in enumerate(X_test.iterrows()):\n",
    "        x_scaled = X_test_scaled[i]\n",
    "        multiplier = calculate_uncertainty_multiplier(x_scaled, unc_scaler)\n",
    "        scaled_interval = conformal['quantile'] * multiplier\n",
    "        in_interval = (y_test.iloc[i] >= y_pred_test[i] - scaled_interval) and \\\n",
    "                      (y_test.iloc[i] <= y_pred_test[i] + scaled_interval)\n",
    "        scaled_coverages.append(in_interval)\n",
    "    scaled_coverage = np.mean(scaled_coverages)\n",
    "    \n",
    "    print(f\"    Quantile 80% interval: actual coverage = {q_coverage:.1%}\")\n",
    "    print(f\"    Conformal 80% interval: actual coverage = {conf_coverage:.1%}\")\n",
    "    print(f\"    Scaled conformal interval: actual coverage = {scaled_coverage:.1%}\")\n",
    "    \n",
    "    # Store calibration results\n",
    "    trained_models[horizon]['calibration'] = {\n",
    "        'quantile_coverage': q_coverage,\n",
    "        'conformal_coverage': conf_coverage,\n",
    "        'scaled_coverage': scaled_coverage,\n",
    "        'conformal_width': conformal['quantile']\n",
    "    }\n",
    "    \n",
    "    if abs(q_coverage - 0.8) > 0.1:\n",
    "        print(f\"    \u26a0\ufe0f Quantile intervals may be miscalibrated\")\n",
    "    if abs(conf_coverage - 0.8) > 0.1:\n",
    "        print(f\"    \u26a0\ufe0f Conformal intervals may need recalibration\")\n",
    "\n",
    "# Copy 4h to 24h if available\n",
    "if '4h' in quantile_models:\n",
    "    quantile_models['24h'] = quantile_models['4h']\n",
    "    print(\"\\n24h: Using 4h quantile models\")\n",
    "\n",
    "if '4h' in conformal_residuals:\n",
    "    conformal_residuals['24h'] = conformal_residuals['4h']\n",
    "\n",
    "if '4h' in uncertainty_scalers:\n",
    "    uncertainty_scalers['24h'] = uncertainty_scalers['4h']\n",
    "\n",
    "print(f\"\\n\u2713 Prediction intervals with uncertainty scaling ready for: {list(quantile_models.keys())}\")\n",
    ""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Direction Prediction - IMPROVED\n",
    "# Changes: Binary up/down, class weights, holdout evaluation, adaptive threshold\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING DIRECTION MODELS (IMPROVED)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "direction_models = {}\n",
    "\n",
    "# Configuration\n",
    "USE_BINARY = True  # Binary (up/down) vs 3-class (down/stable/up)\n",
    "DIRECTION_THRESHOLD = 0.01  # 1% threshold for direction change\n",
    "\n",
    "def create_binary_direction(target, current, threshold=0.01):\n",
    "    \"\"\"Create binary direction labels: 1=up, 0=down/stable\"\"\"\n",
    "    pct_change = (target - current) / (current + 1e-8)\n",
    "    return (pct_change > threshold).astype(int)\n",
    "\n",
    "def create_ternary_direction(target, current, threshold=0.02):\n",
    "    \"\"\"Create 3-class direction labels\"\"\"\n",
    "    pct_change = (target - current) / (current + 1e-8)\n",
    "    direction = pd.Series('stable', index=target.index)\n",
    "    direction[pct_change > threshold] = 'up'\n",
    "    direction[pct_change < -threshold] = 'down'\n",
    "    return direction\n",
    "\n",
    "for horizon in ['1h', '4h']:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{horizon.upper()} DIRECTION MODEL\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Get features and targets\n",
    "    X_h = X_1h if horizon == '1h' else X_4h\n",
    "    features = numeric_features_1h if horizon == '1h' else numeric_features_4h\n",
    "    \n",
    "    # Get raw target for direction calculation\n",
    "    if 'target_1h_raw' in df_train_val.columns:\n",
    "        target_raw = df_train_val[f'target_{horizon}_raw']\n",
    "    else:\n",
    "        target_raw = df_train_val[f'target_{horizon}']\n",
    "    \n",
    "    current = df_train_val['gas']\n",
    "    \n",
    "    # Create direction labels\n",
    "    if USE_BINARY:\n",
    "        y_dir = create_binary_direction(target_raw, current, DIRECTION_THRESHOLD)\n",
    "        print(f\"Binary classification (threshold: {DIRECTION_THRESHOLD*100}%)\")\n",
    "    else:\n",
    "        y_dir = create_ternary_direction(target_raw, current)\n",
    "        print(f\"3-class classification (threshold: {DIRECTION_THRESHOLD*100}%)\")\n",
    "    \n",
    "    mask = y_dir.notna() & target_raw.notna()\n",
    "    X_d = X_h[mask]\n",
    "    y_d = y_dir[mask]\n",
    "    \n",
    "    if len(X_d) < 1000:\n",
    "        print(f\"  \u26a0\ufe0f Insufficient data, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Class distribution\n",
    "    class_counts = y_d.value_counts()\n",
    "    print(f\"Class distribution: {dict(class_counts)}\")\n",
    "    \n",
    "    # Compute class weights\n",
    "    classes = np.unique(y_d)\n",
    "    weights = compute_class_weight('balanced', classes=classes, y=y_d)\n",
    "    class_weight_dict = dict(zip(classes, weights))\n",
    "    print(f\"Class weights: {class_weight_dict}\")\n",
    "    \n",
    "    # Split - use holdout if available\n",
    "    if HAS_HOLDOUT:\n",
    "        X_train, X_test = X_d, df_holdout[features]\n",
    "        y_train = y_d\n",
    "        \n",
    "        # Create holdout labels\n",
    "        if 'target_1h_raw' in df_holdout.columns:\n",
    "            holdout_target = df_holdout[f'target_{horizon}_raw']\n",
    "        else:\n",
    "            holdout_target = df_holdout[f'target_{horizon}']\n",
    "        holdout_current = df_holdout['gas']\n",
    "        \n",
    "        if USE_BINARY:\n",
    "            y_test = create_binary_direction(holdout_target, holdout_current, DIRECTION_THRESHOLD)\n",
    "        else:\n",
    "            y_test = create_ternary_direction(holdout_target, holdout_current)\n",
    "        \n",
    "        test_mask = y_test.notna() & holdout_target.notna()\n",
    "        X_test = X_test[test_mask]\n",
    "        y_test = y_test[test_mask]\n",
    "        print(f\"Using holdout for evaluation ({len(y_test)} samples)\")\n",
    "    else:\n",
    "        split_idx = int(len(X_d) * 0.8)\n",
    "        X_train, X_test = X_d.iloc[:split_idx], X_d.iloc[split_idx:]\n",
    "        y_train, y_test = y_d.iloc[:split_idx], y_d.iloc[split_idx:]\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Try multiple classifiers\n",
    "    classifiers = [\n",
    "        ('LogReg', LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)),\n",
    "        ('RF', RandomForestClassifier(n_estimators=30, max_depth=4, class_weight='balanced', random_state=42, n_jobs=-1)),\n",
    "        ('GBM', GradientBoostingClassifier(n_estimators=30, max_depth=3, learning_rate=0.1, random_state=42)),\n",
    "    ]\n",
    "    \n",
    "    best_clf = None\n",
    "    best_acc = 0\n",
    "    best_name = None\n",
    "    \n",
    "    for name, clf in classifiers:\n",
    "        try:\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            y_pred = clf.predict(X_test_scaled)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "            print(f\"  {name}: Acc={acc:.1%}, F1={f1:.3f}\")\n",
    "            \n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_clf = clf\n",
    "                best_name = name\n",
    "                best_f1 = f1\n",
    "        except Exception as e:\n",
    "            print(f\"  {name}: Failed - {e}\")\n",
    "    \n",
    "    if best_clf is None:\n",
    "        print(f\"  \u26a0\ufe0f All classifiers failed\")\n",
    "        continue\n",
    "    \n",
    "    # Baseline: always predict majority class\n",
    "    majority_class = y_train.mode()[0]\n",
    "    baseline_acc = (y_test == majority_class).mean()\n",
    "    improvement = (best_acc - baseline_acc) / baseline_acc * 100\n",
    "    \n",
    "    print(f\"\\n  >>> Best: {best_name} (Acc: {best_acc:.1%}, vs baseline {baseline_acc:.1%}: {improvement:+.1f}%)\")\n",
    "    \n",
    "    direction_models[horizon] = {\n",
    "        'model': best_clf,\n",
    "        'scaler': scaler,\n",
    "        'accuracy': float(best_acc),\n",
    "        'f1_score': float(best_f1),\n",
    "        'baseline_accuracy': float(baseline_acc),\n",
    "        'improvement_vs_baseline': float(improvement),\n",
    "        'model_name': best_name,\n",
    "        'is_binary': USE_BINARY,\n",
    "        'threshold': DIRECTION_THRESHOLD\n",
    "    }\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DIRECTION MODEL SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "for horizon, data in direction_models.items():\n",
    "    imp = data['improvement_vs_baseline']\n",
    "    status = \"\u2713\" if imp > 5 else \"\u26a0\" if imp > 0 else \"\u2717\"\n",
    "    print(f\"{status} {horizon}: {data['model_name']} | Acc: {data['accuracy']:.1%} | vs baseline: {imp:+.1f}%\")\n",
    ""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# REGIME DETECTION\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING REGIME DETECTION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create regime labels from gas statistics (instead of volatility_regime)\n",
    "# 0 = Normal, 1 = Elevated, 2 = Spike\n",
    "if 'gas_zscore_1h' in df_train_val.columns and 'is_spike' in df_train_val.columns:\n",
    "    # Create regime from z-score: low (<-0.5), normal (-0.5 to 1), elevated (1 to 2), spike (>2)\n",
    "    zscore = df_train_val['gas_zscore_1h']\n",
    "    is_spike = df_train_val['is_spike']\n",
    "    \n",
    "    regime_labels = pd.Series(0, index=df_train_val.index)  # Default: Normal\n",
    "    regime_labels[zscore > 1] = 1  # Elevated\n",
    "    regime_labels[is_spike == 1] = 2  # Spike\n",
    "    \n",
    "    X_r = X_4h.copy()\n",
    "    y_r = regime_labels\n",
    "    \n",
    "    if len(X_r) < 500:\n",
    "        print(\"\u26a0\ufe0f Insufficient data for regime detection\")\n",
    "        regime_clf = None\n",
    "        regime_scaler = None\n",
    "        regime_accuracy = 0\n",
    "    else:\n",
    "        # Train/test split\n",
    "        split_idx = int(len(X_r) * 0.8)\n",
    "        X_train, X_test = X_r.iloc[:split_idx], X_r.iloc[split_idx:]\n",
    "        y_train, y_test = y_r.iloc[:split_idx], y_r.iloc[split_idx:]\n",
    "        \n",
    "        regime_scaler = RobustScaler()\n",
    "        X_train_scaled = regime_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = regime_scaler.transform(X_test)\n",
    "        \n",
    "        # Train classifier (simple, reduced complexity)\n",
    "        regime_clf = RandomForestClassifier(\n",
    "            n_estimators=30, max_depth=4,\n",
    "            min_samples_leaf=20,\n",
    "            random_state=42, n_jobs=-1\n",
    "        )\n",
    "        regime_clf.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = regime_clf.predict(X_test_scaled)\n",
    "        regime_accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Regime classes: Normal (0), Elevated (1), Spike (2)\")\n",
    "        print(f\"Class distribution: {dict(y_r.value_counts().sort_index())}\")\n",
    "        print(f\"Accuracy: {regime_accuracy:.1%}\")\n",
    "        \n",
    "        if regime_accuracy > 0.95:\n",
    "            print(\"\u26a0\ufe0f Warning: Very high accuracy may indicate class imbalance or overfitting\")\n",
    "else:\n",
    "    regime_clf = None\n",
    "    regime_scaler = None\n",
    "    regime_accuracy = 0\n",
    "    print(\"\u26a0\ufe0f Missing gas_zscore_1h or is_spike, skipping regime detection\")\n",
    ""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Spike Detectors\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nprint(\"\\n",
    "\" + \"=\"*60)\nprint(\"TRAINING SPIKE DETECTORS\")\nprint(\"=\"*60)\n\nspike_models = {}\n\nfor horizon, X_h, y_target in [('1h', X_1h, y_1h), ('4h', X_4h, y_4h)]:\n    print(f\"\\n",
    "{horizon} spike detector...\")\n    \n    # Create spike labels (>2 std from mean is a spike)\n    mask = y_target.notna()\n    X_s = X_h[mask]\n    y_s = y_target[mask]\n    current = current_gas[mask]\n    \n    # Define spike threshold\n    price_change = y_s - current\n    threshold = price_change.std() * 2\n    spike_labels = (price_change > threshold).astype(int)\n    \n    spike_rate = spike_labels.mean()\n    print(f\"  Spike rate: {spike_rate:.1%}\")\n    \n    if spike_rate < 0.01 or spike_rate > 0.5:\n        print(f\"  \u26a0\ufe0f Unusual spike rate, skipping\")\n        continue\n    \n    if len(X_s) < 1000:\n        print(f\"  \u26a0\ufe0f Insufficient data, skipping\")\n        continue\n    \n    # Train/test split\n    split_idx = int(len(X_s) * 0.8)\n    X_train, X_test = X_s.iloc[:split_idx], X_s.iloc[split_idx:]\n    y_train, y_test = spike_labels.iloc[:split_idx], spike_labels.iloc[split_idx:]\n    \n    scaler = RobustScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Train with class weights\n    clf = GradientBoostingClassifier(\n        n_estimators=50, max_depth=4,\n        learning_rate=0.1, random_state=42\n    )\n    clf.fit(X_train_scaled, y_train)\n    \n    # Evaluate\n    y_pred = clf.predict(X_test_scaled)\n    acc = accuracy_score(y_test, y_pred)\n    \n    spike_models[horizon] = (clf, scaler)\n    print(f\"  Accuracy: {acc:.1%}\")\n\n# Copy 4h to 24h if available\nif '4h' in spike_models:\n    spike_models['24h'] = spike_models['4h']\n    print(\"\\n",
    "24h: Using 4h spike detector (fallback)\")\n\nprint(f\"\\n",
    "\u2713 Spike detectors trained for: {list(spike_models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# DQN AGENT TRAINING (OPTIONAL)\n",
    "# This trains a reinforcement learning agent for transaction timing\n",
    "# Skip if you just need prediction models\n",
    "\n",
    "TRAIN_DQN = False  # Set to True to train DQN agent\n",
    "\n",
    "if not TRAIN_DQN:\n",
    "    print(\"=\"*60)\n",
    "    print(\"DQN TRAINING SKIPPED (set TRAIN_DQN = True to enable)\")\n",
    "    print(\"=\"*60)\n",
    "    DQN_TRAINED = False"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Training Implementation (runs only if TRAIN_DQN = True)\n",
    "\n",
    "if TRAIN_DQN:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING DQN AGENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        import torch.optim as optim\n",
    "        from collections import deque\n",
    "        import random\n",
    "        \n",
    "        class DQNNetwork(nn.Module):\n",
    "            def __init__(self, state_dim, action_dim):\n",
    "                super().__init__()\n",
    "                self.net = nn.Sequential(\n",
    "                    nn.Linear(state_dim, 64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(64, 32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(32, action_dim)\n",
    "                )\n",
    "            \n",
    "            def forward(self, x):\n",
    "                return self.net(x)\n",
    "        \n",
    "        class DQNAgent:\n",
    "            def __init__(self, state_dim, action_dim):\n",
    "                self.state_dim = state_dim\n",
    "                self.action_dim = action_dim\n",
    "                self.epsilon = 1.0\n",
    "                self.epsilon_min = 0.05\n",
    "                self.epsilon_decay = 0.995\n",
    "                self.gamma = 0.99\n",
    "                self.lr = 0.001\n",
    "                self.memory = deque(maxlen=10000)\n",
    "                self.batch_size = 32\n",
    "                self.training_steps = 0\n",
    "                \n",
    "                self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                self.model = DQNNetwork(state_dim, action_dim).to(self.device)\n",
    "                self.target_model = DQNNetwork(state_dim, action_dim).to(self.device)\n",
    "                self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "                self.update_target()\n",
    "            \n",
    "            def update_target(self):\n",
    "                self.target_model.load_state_dict(self.model.state_dict())\n",
    "            \n",
    "            def act(self, state):\n",
    "                if random.random() < self.epsilon:\n",
    "                    return random.randint(0, self.action_dim - 1)\n",
    "                state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    q_values = self.model(state_t)\n",
    "                return q_values.argmax().item()\n",
    "            \n",
    "            def remember(self, state, action, reward, next_state, done):\n",
    "                self.memory.append((state, action, reward, next_state, done))\n",
    "            \n",
    "            def replay(self):\n",
    "                if len(self.memory) < self.batch_size:\n",
    "                    return\n",
    "                \n",
    "                batch = random.sample(self.memory, self.batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                \n",
    "                states = torch.FloatTensor(states).to(self.device)\n",
    "                actions = torch.LongTensor(actions).to(self.device)\n",
    "                rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "                next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "                dones = torch.FloatTensor(dones).to(self.device)\n",
    "                \n",
    "                current_q = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "                next_q = self.target_model(next_states).max(1)[0].detach()\n",
    "                target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "                \n",
    "                loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                self.training_steps += 1\n",
    "                if self.training_steps % 100 == 0:\n",
    "                    self.update_target()\n",
    "                \n",
    "                self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "            \n",
    "            def save(self, path):\n",
    "                torch.save(self.model.state_dict(), path)\n",
    "        \n",
    "        # Create simple environment\n",
    "        state_dim = min(30, len(X.columns))  # Limit state size\n",
    "        action_dim = 2  # 0 = wait, 1 = execute\n",
    "        \n",
    "        DQN_AGENT = DQNAgent(state_dim, action_dim)\n",
    "        \n",
    "        # Train for a few episodes\n",
    "        n_episodes = 500\n",
    "        print(f\"Training DQN for {n_episodes} episodes...\")\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            # Simple training loop\n",
    "            for i in range(min(100, len(X) - 1)):\n",
    "                state = X.iloc[i, :state_dim].values\n",
    "                action = DQN_AGENT.act(state)\n",
    "                \n",
    "                # Simple reward: negative gas price change if executing\n",
    "                next_gas = current_gas.iloc[i + 1] if i + 1 < len(current_gas) else current_gas.iloc[i]\n",
    "                reward = -(next_gas - current_gas.iloc[i]) if action == 1 else -0.001  # Small wait penalty\n",
    "                \n",
    "                next_state = X.iloc[i + 1, :state_dim].values if i + 1 < len(X) else state\n",
    "                done = (i >= min(99, len(X) - 2))\n",
    "                \n",
    "                DQN_AGENT.remember(state, action, reward, next_state, done)\n",
    "                DQN_AGENT.replay()\n",
    "            \n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f\"  Episode {episode + 1}/{n_episodes}, Epsilon: {DQN_AGENT.epsilon:.3f}\")\n",
    "        \n",
    "        DQN_TRAINED = True\n",
    "        DQN_METRICS = {\n",
    "            'episodes': n_episodes,\n",
    "            'training_steps': DQN_AGENT.training_steps,\n",
    "            'final_epsilon': float(DQN_AGENT.epsilon)\n",
    "        }\n",
    "        print(f\"\\n\u2713 DQN training complete ({DQN_AGENT.training_steps} steps)\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\u26a0\ufe0f PyTorch not available, skipping DQN training\")\n",
    "        DQN_TRAINED = False\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f DQN training failed: {e}\")\n",
    "        DQN_TRAINED = False\n",
    "else:\n",
    "    DQN_TRAINED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models + MODEL COMPARISON REPORT\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json as json_lib\n",
    "\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# === Save prediction models ===\n",
    "for horizon in ['1h', '4h', '24h']:\n",
    "    if horizon not in trained_models:\n",
    "        print(f\"\u26a0\ufe0f No {horizon} model to save\")\n",
    "        continue\n",
    "    \n",
    "    data = trained_models[horizon]\n",
    "    model = data['model']\n",
    "    scaler = data['scaler']\n",
    "    metrics = data['metrics']\n",
    "    features = data.get('features', [])\n",
    "    \n",
    "    model_data = {\n",
    "        'model': model,\n",
    "        'model_name': metrics['name'],\n",
    "        'metrics': {\n",
    "            'mae': float(metrics['mae']),\n",
    "            'improvement': float(metrics['improvement']),\n",
    "            'vs_holdout_baseline': float(metrics['vs_holdout_baseline']) if metrics.get('vs_holdout_baseline') else None,\n",
    "            'passed_baseline': bool(metrics.get('passed_baseline', False)),\n",
    "            'is_fallback': data.get('is_fallback', False)\n",
    "        },\n",
    "        'trained_at': datetime.now().isoformat(),\n",
    "        'feature_names': list(features),\n",
    "        'feature_scaler': scaler,\n",
    "        'scaler_type': 'RobustScaler',\n",
    "        'target_transform': TARGET_TRANSFORM_USED if 'TARGET_TRANSFORM_USED' in dir() else 'none'\n",
    "    }\n",
    "    \n",
    "    if 'cv_mae' in metrics:\n",
    "        model_data['cv_mae'] = float(metrics['cv_mae'])\n",
    "    \n",
    "    if 'conformal_residuals' in dir() and horizon in conformal_residuals:\n",
    "        model_data['conformal_interval'] = float(conformal_residuals[horizon]['quantile'])\n",
    "    \n",
    "    if 'uncertainty_scalers' in dir() and horizon in uncertainty_scalers:\n",
    "        model_data['uncertainty_scaler'] = uncertainty_scalers[horizon]\n",
    "    \n",
    "    joblib.dump(model_data, f'saved_models/model_{horizon}.pkl')\n",
    "    \n",
    "    vs_info = f\"vs holdout: {metrics['vs_holdout_baseline']*100:+.1f}%\" if metrics.get('vs_holdout_baseline') else \"\"\n",
    "    print(f\"\u2713 model_{horizon}.pkl ({metrics['name']}, MAE={metrics['mae']:.4f} {vs_info})\")\n",
    "    \n",
    "    joblib.dump(scaler, f'saved_models/scaler_{horizon}.pkl')\n",
    "\n",
    "# === Save regime-specific models ===\n",
    "if 'regime_specific_models' in dir() and regime_specific_models:\n",
    "    os.makedirs('saved_models/regime_models', exist_ok=True)\n",
    "    for horizon, regime_dict in regime_specific_models.items():\n",
    "        for regime_val, regime_data in regime_dict.items():\n",
    "            regime_model_data = {\n",
    "                'model': regime_data['model'],\n",
    "                'scaler': regime_data['scaler'],\n",
    "                'metrics': regime_data['metrics'],\n",
    "                'regime_name': regime_data['regime_name'],\n",
    "                'regime_val': regime_val,\n",
    "                'n_samples': regime_data['n_samples'],\n",
    "                'trained_at': datetime.now().isoformat()\n",
    "            }\n",
    "            filename = f'saved_models/regime_models/model_{horizon}_{regime_data[\"regime_name\"]}.pkl'\n",
    "            joblib.dump(regime_model_data, filename)\n",
    "            print(f\"  \u2192 {horizon}_{regime_data['regime_name']} regime model\")\n",
    "\n",
    "# === Save feature names ===\n",
    "default_features = trained_models.get('4h', trained_models.get('1h', {})).get('features', [])\n",
    "joblib.dump(list(default_features), 'saved_models/feature_names.pkl')\n",
    "print(f\"\\n\u2713 feature_names.pkl ({len(default_features)} features)\")\n",
    "\n",
    "# === Save other models ===\n",
    "if 'spike_models' in dir() and spike_models:\n",
    "    for horizon, (clf, scaler) in spike_models.items():\n",
    "        joblib.dump({'model': clf, 'scaler': scaler}, f'saved_models/spike_detector_{horizon}.pkl')\n",
    "        print(f\"\u2713 spike_detector_{horizon}.pkl\")\n",
    "\n",
    "if 'regime_clf' in dir() and regime_clf is not None:\n",
    "    joblib.dump({'model': regime_clf, 'scaler': regime_scaler, 'accuracy': regime_accuracy}, \n",
    "                'saved_models/regime_detector.pkl')\n",
    "    print(f\"\u2713 regime_detector.pkl\")\n",
    "\n",
    "if 'quantile_models' in dir() and quantile_models:\n",
    "    for horizon, (q_models, q_scaler) in quantile_models.items():\n",
    "        quantile_data = {'models': q_models, 'scaler': q_scaler, 'quantiles': [0.1, 0.5, 0.9]}\n",
    "        if 'conformal_residuals' in dir() and horizon in conformal_residuals:\n",
    "            quantile_data['conformal'] = {'interval_width': float(conformal_residuals[horizon]['quantile'])}\n",
    "        if 'uncertainty_scalers' in dir() and horizon in uncertainty_scalers:\n",
    "            quantile_data['uncertainty_scaler'] = uncertainty_scalers[horizon]\n",
    "        joblib.dump(quantile_data, f'saved_models/quantile_{horizon}.pkl')\n",
    "        print(f\"\u2713 quantile_{horizon}.pkl\")\n",
    "\n",
    "# === Save training metadata ===\n",
    "def convert_to_python_types(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_python_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_python_types(v) for v in obj]\n",
    "    elif isinstance(obj, (np.bool_, np.integer)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif hasattr(obj, 'item'):\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "metadata = {\n",
    "    'training_timestamp': datetime.now().isoformat(),\n",
    "    'total_samples': len(df_clean),\n",
    "    'training_samples': len(df_train_val),\n",
    "    'holdout_samples': len(df_holdout) if df_holdout is not None else 0,\n",
    "    'date_range': f\"{df_clean.index.min()} to {df_clean.index.max()}\",\n",
    "    'resampling': '30-second intervals',\n",
    "    'selection_method': 'holdout-based',\n",
    "    'configuration': {\n",
    "        'target_transform': TARGET_TRANSFORM_USED if 'TARGET_TRANSFORM_USED' in dir() else 'none',\n",
    "        'use_rolling_window': USE_ROLLING_WINDOW if 'USE_ROLLING_WINDOW' in dir() else False,\n",
    "        'rolling_window_days': ROLLING_WINDOW_DAYS if 'ROLLING_WINDOW_DAYS' in dir() else None,\n",
    "        'auto_adapt_on_shift': AUTO_ADAPT_ON_SHIFT if 'AUTO_ADAPT_ON_SHIFT' in dir() else False,\n",
    "        'distribution_shift_detected': DISTRIBUTION_SHIFT_DETECTED if 'DISTRIBUTION_SHIFT_DETECTED' in dir() else False,\n",
    "        'shift_magnitude': SHIFT_MAGNITUDE if 'SHIFT_MAGNITUDE' in dir() else 0\n",
    "    },\n",
    "    'features': {'count': len(default_features), 'list': list(default_features)},\n",
    "    'baselines': BASELINES,\n",
    "    'models': {},\n",
    "    'regime_models': {},\n",
    "    'direction_models': {}\n",
    "}\n",
    "\n",
    "for horizon, data in trained_models.items():\n",
    "    m = data['metrics']\n",
    "    metadata['models'][horizon] = {\n",
    "        'name': m['name'],\n",
    "        'mae': float(m['mae']),\n",
    "        'improvement_pct': float(m['improvement'] * 100),\n",
    "        'vs_holdout_baseline_pct': float(m['vs_holdout_baseline'] * 100) if m.get('vs_holdout_baseline') else None,\n",
    "        'passed_baseline': bool(m.get('passed_baseline', False)),\n",
    "        'is_fallback': data.get('is_fallback', False)\n",
    "    }\n",
    "    if 'cv_mae' in m:\n",
    "        metadata['models'][horizon]['cv_mae'] = float(m['cv_mae'])\n",
    "    if 'calibration' in data:\n",
    "        metadata['models'][horizon]['calibration'] = data['calibration']\n",
    "\n",
    "if 'regime_specific_models' in dir() and regime_specific_models:\n",
    "    for horizon, regime_dict in regime_specific_models.items():\n",
    "        metadata['regime_models'][horizon] = {}\n",
    "        for regime_val, regime_data in regime_dict.items():\n",
    "            metadata['regime_models'][horizon][regime_data['regime_name']] = {\n",
    "                'mae': float(regime_data['metrics']['mae']),\n",
    "                'n_samples': int(regime_data['n_samples'])\n",
    "            }\n",
    "\n",
    "if 'direction_models' in dir() and direction_models:\n",
    "    for horizon, data in direction_models.items():\n",
    "        metadata['direction_models'][horizon] = {\n",
    "            'accuracy': float(data['accuracy']),\n",
    "            'f1_score': float(data['f1_score']),\n",
    "            'baseline_accuracy': float(data.get('baseline_accuracy', 0)),\n",
    "            'improvement_vs_baseline': float(data.get('improvement_vs_baseline', 0)),\n",
    "            'model_name': data.get('model_name', 'GBM'),\n",
    "            'is_binary': data.get('is_binary', False)\n",
    "        }\n",
    "\n",
    "metadata = convert_to_python_types(metadata)\n",
    "\n",
    "with open('saved_models/training_metadata.json', 'w') as f:\n",
    "    json_lib.dump(metadata, f, indent=2)\n",
    "print(f\"\\n\u2713 training_metadata.json\")\n",
    "\n",
    "# === Save feature importance ===\n",
    "if FEATURE_IMPORTANCE:\n",
    "    sorted_importance = dict(sorted(FEATURE_IMPORTANCE.items(), key=lambda x: x[1], reverse=True))\n",
    "    with open('saved_models/feature_importance.json', 'w') as f:\n",
    "        json_lib.dump(convert_to_python_types(sorted_importance), f, indent=2)\n",
    "    print(f\"\u2713 feature_importance.json\")\n",
    "else:\n",
    "    uniform = {f: 1.0/len(default_features) for f in default_features}\n",
    "    with open('saved_models/feature_importance.json', 'w') as f:\n",
    "        json_lib.dump(uniform, f, indent=2)\n",
    "    print(f\"\u2713 feature_importance.json (uniform)\")\n",
    "\n",
    "# === MODEL COMPARISON REPORT ===\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MODEL COMPARISON REPORT\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "history_file = 'saved_models/training_history.json'\n",
    "if os.path.exists(history_file):\n",
    "    with open(history_file) as f:\n",
    "        history = json_lib.load(f)\n",
    "else:\n",
    "    history = []\n",
    "\n",
    "# Add current run to history\n",
    "current_run = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'models': {},\n",
    "    'config': metadata['configuration']\n",
    "}\n",
    "for horizon, data in trained_models.items():\n",
    "    m = data['metrics']\n",
    "    current_run['models'][horizon] = {\n",
    "        'name': m['name'],\n",
    "        'mae': float(m['mae']),\n",
    "        'vs_holdout_baseline': float(m['vs_holdout_baseline']) if m.get('vs_holdout_baseline') else None\n",
    "    }\n",
    "\n",
    "history.append(current_run)\n",
    "# Keep last 10 runs\n",
    "history = history[-10:]\n",
    "\n",
    "with open(history_file, 'w') as f:\n",
    "    json_lib.dump(history, f, indent=2)\n",
    "\n",
    "# Compare with previous run\n",
    "if len(history) >= 2:\n",
    "    prev_run = history[-2]\n",
    "    print(f\"\\nComparing with previous run ({prev_run['timestamp'][:16]}):\")\n",
    "    print(f\"{'Horizon':<8} {'Prev MAE':<12} {'Curr MAE':<12} {'Change':<12} {'Recommendation'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    recommendations = []\n",
    "    for horizon in ['1h', '4h', '24h']:\n",
    "        if horizon in current_run['models'] and horizon in prev_run['models']:\n",
    "            prev_mae = prev_run['models'][horizon]['mae']\n",
    "            curr_mae = current_run['models'][horizon]['mae']\n",
    "            change = (curr_mae - prev_mae) / prev_mae * 100\n",
    "            \n",
    "            if change < -5:\n",
    "                rec = \"\u2713 DEPLOY (improved)\"\n",
    "                recommendations.append(('deploy', horizon))\n",
    "            elif change > 10:\n",
    "                rec = \"\u2717 KEEP OLD (degraded)\"\n",
    "                recommendations.append(('keep_old', horizon))\n",
    "            else:\n",
    "                rec = \"~ SIMILAR\"\n",
    "                recommendations.append(('similar', horizon))\n",
    "            \n",
    "            print(f\"{horizon:<8} {prev_mae:<12.4f} {curr_mae:<12.4f} {change:+.1f}%{'':<6} {rec}\")\n",
    "    \n",
    "    # Overall recommendation\n",
    "    deploy_count = sum(1 for r, _ in recommendations if r == 'deploy')\n",
    "    keep_old_count = sum(1 for r, _ in recommendations if r == 'keep_old')\n",
    "    \n",
    "    print(f\"\\nOVERALL: \", end=\"\")\n",
    "    if deploy_count > keep_old_count:\n",
    "        print(\"\u2713 RECOMMEND DEPLOYING new models\")\n",
    "    elif keep_old_count > 0:\n",
    "        print(\"\u26a0\ufe0f Consider keeping old models (some degradation)\")\n",
    "    else:\n",
    "        print(\"~ Models similar to previous run\")\n",
    "else:\n",
    "    print(\"\\nFirst run - no previous models to compare\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL MODELS SAVED\")\n",
    "print(f\"{'='*60}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final report\nprint(\"\\n",
    "\" + \"=\"*70)\nprint(\"TRAINING COMPLETE - FINAL REPORT\")\nprint(\"=\"*70)\n\ntotal_days = len(df_clean) / (120 * 24)\n\nprint(f\"\\n",
    "DATA SUMMARY\")\nprint(f\"   Total samples: {len(df_clean):,} ({total_days:.1f} days)\")\nprint(f\"   Training: {len(df_train_val):,} | Holdout: {len(df_holdout) if df_holdout is not None else 0:,}\")\nprint(f\"   Date range: {df_clean.index.min()} to {df_clean.index.max()}\")\nprint(f\"   ETH price: {'Binance 1-min \u2713' if HAS_ETH_PRICE else 'Not available'}\")\nprint(f\"   Features: 1h={len(numeric_features_1h)}, 4h={len(numeric_features_4h)}, 24h={len(numeric_features_24h)}\")\n\nprint(f\"\\n",
    "\" + \"-\"*70)\nprint(f\"{'MODEL PERFORMANCE':^70}\")\nprint(\"-\"*70)\nprint(f\"{'Horizon':<8} {'Model':<15} {'CV MAE':>10} {'Holdout':>10} {'vs Base':>10} {'Status':>12}\")\nprint(\"-\"*70)\n\nfor horizon in ['1h', '4h', '24h']:\n    if horizon in trained_models:\n        data = trained_models[horizon]\n        m = data['metrics']\n        name = m['name'][:14]\n        if data.get('is_fallback'):\n            name = name[:10] + '(fb)'\n        \n        cv_mae = f\"{m['mae']:.4f}\"\n        holdout_mae = f\"{data.get('holdout_mae', 0):.4f}\" if 'holdout_mae' in data else \"N/A\"\n        improvement = f\"{m['improvement']*100:+.1f}%\"\n        status = \"\u2713 PASS\" if m['passed_baseline'] else \"\u2717 FAIL\"\n        \n        print(f\"{horizon:<8} {name:<15} {cv_mae:>10} {holdout_mae:>10} {improvement:>10} {status:>12}\")\n\nprint(\"-\"*70)\n\n# Calibration report\nif any('calibration' in trained_models.get(h, {}) for h in ['1h', '4h']):\n    print(f\"\\n",
    "\" + \"-\"*70)\n    print(f\"{'PREDICTION INTERVAL CALIBRATION':^70}\")\n    print(\"-\"*70)\n    print(f\"{'Horizon':<10} {'Quantile 80%':>15} {'Conformal 80%':>15} {'Width (gwei)':>15}\")\n    print(\"-\"*70)\n    \n    for horizon in ['1h', '4h']:\n        if horizon in trained_models and 'calibration' in trained_models[horizon]:\n            cal = trained_models[horizon]['calibration']\n            q_cov = f\"{cal['quantile_coverage']:.1%}\"\n            c_cov = f\"{cal['conformal_coverage']:.1%}\"\n            width = f\"\u00b1{cal['conformal_width']:.4f}\"\n            print(f\"{horizon:<10} {q_cov:>15} {c_cov:>15} {width:>15}\")\n    \n    print(\"-\"*70)\n\n# Direction models\nif 'direction_models' in dir() and direction_models:\n    print(f\"\\n",
    "\" + \"-\"*70)\n    print(f\"{'DIRECTION PREDICTION':^70}\")\n    print(\"-\"*70)\n    for horizon, data in direction_models.items():\n        print(f\"  {horizon}: Accuracy={data['accuracy']:.1%}, F1={data['f1_score']:.3f}\")\n\n# 24h model status\nprint(f\"\\n",
    "\" + \"-\"*70)\nprint(f\"{'24H MODEL STATUS':^70}\")\nprint(\"-\"*70)\nif '24h' in trained_models:\n    if trained_models['24h'].get('is_fallback'):\n        print(f\"  \u26a0\ufe0f Using 4h model as fallback (need 30+ days of data)\")\n        print(f\"     Current data: {total_days:.1f} days\")\n        print(f\"     Recommendation: Collect {30 - total_days:.0f} more days before training true 24h model\")\n    else:\n        print(f\"  \u2713 True 24h model trained with {total_days:.1f} days of data\")\n\n# Final recommendation\nprint(f\"\\n",
    "\" + \"=\"*70)\nprint(\"RECOMMENDATION\")\nprint(\"=\"*70)\n\nall_passed = all(trained_models.get(h, {}).get('metrics', {}).get('passed_baseline', False) \n                 for h in trained_models if h in trained_models)\n\nif all_passed:\n    print(\"\u2713 All models beat baseline - READY FOR DEPLOYMENT\")\n    print(\"\\n",
    "Next steps:\")\n    print(\"  1. Download saved_models/ folder\")\n    print(\"  2. Copy to backend/models/saved_models/\")\n    print(\"  3. Restart backend\")\nelse:\n    failed = [h for h in trained_models \n              if not trained_models[h]['metrics']['passed_baseline']]\n    print(f\"\u26a0\ufe0f Some models did not pass baseline: {failed}\")\n    print(\"\\n",
    "Recommendations:\")\n    print(\"  - Collect more data\")\n    print(\"  - Review feature engineering\")\n    print(\"  - Only deploy passing models\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualizations - IMPROVED with holdout baseline comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Train vs Holdout Distribution Comparison\n",
    "ax1 = axes[0, 0]\n",
    "train_gas = current_gas.values\n",
    "if HAS_HOLDOUT:\n",
    "    holdout_gas = df_holdout['gas'].values\n",
    "    ax1.hist(train_gas, bins=50, alpha=0.6, color='blue', label=f'Train (mean={train_gas.mean():.2f})', density=True)\n",
    "    ax1.hist(holdout_gas, bins=50, alpha=0.6, color='red', label=f'Holdout (mean={holdout_gas.mean():.2f})', density=True)\n",
    "    ax1.legend()\n",
    "    ax1.set_title('Train vs Holdout Distribution')\n",
    "else:\n",
    "    ax1.hist(train_gas, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "    ax1.axvline(train_gas.mean(), color='red', linestyle='--', label=f'Mean: {train_gas.mean():.2f}')\n",
    "    ax1.legend()\n",
    "    ax1.set_title('Gas Price Distribution')\n",
    "ax1.set_xlabel('Gas Price (gwei)')\n",
    "ax1.set_ylabel('Density' if HAS_HOLDOUT else 'Frequency')\n",
    "\n",
    "# 2. Model vs HOLDOUT Baseline (not train baseline!)\n",
    "ax2 = axes[0, 1]\n",
    "horizons = list(trained_models.keys())\n",
    "maes = [trained_models[h]['metrics']['mae'] for h in horizons]\n",
    "\n",
    "# Use holdout baselines if available, otherwise train baselines\n",
    "baselines = []\n",
    "for h in horizons:\n",
    "    h_key = h.replace('24h', '4h')  # 24h uses 4h baseline\n",
    "    if 'holdout_best' in BASELINES.get(h_key, {}):\n",
    "        baselines.append(BASELINES[h_key]['holdout_best'])\n",
    "    else:\n",
    "        baselines.append(BASELINES.get(h_key, BASELINES['4h'])['best'])\n",
    "\n",
    "x = np.arange(len(horizons))\n",
    "width = 0.35\n",
    "bars1 = ax2.bar(x - width/2, maes, width, label='Model MAE', color='steelblue')\n",
    "bars2 = ax2.bar(x + width/2, baselines, width, label='Holdout Baseline', color='coral')\n",
    "ax2.set_xlabel('Horizon')\n",
    "ax2.set_ylabel('MAE (gwei)')\n",
    "ax2.set_title('Model vs Holdout Baseline Performance')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(horizons)\n",
    "ax2.legend()\n",
    "\n",
    "# Add improvement percentages (vs holdout baseline)\n",
    "for i, (h, m, b) in enumerate(zip(horizons, maes, baselines)):\n",
    "    imp = (b - m) / b * 100\n",
    "    color = 'green' if imp > 0 else 'red'\n",
    "    y_pos = max(m, b) + 0.02 * max(max(maes), max(baselines))\n",
    "    ax2.annotate(f'{imp:+.1f}%', xy=(i, y_pos), ha='center', fontsize=10, fontweight='bold', color=color)\n",
    "\n",
    "# 3. Gas price time series with regime markers\n",
    "ax3 = axes[1, 0]\n",
    "sample_size = min(2000, len(df_clean))\n",
    "sample_df = df_clean.iloc[-sample_size:]\n",
    "sample_gas = sample_df['gas']\n",
    "\n",
    "ax3.plot(sample_gas.index, sample_gas.values, linewidth=0.5, alpha=0.8, color='blue')\n",
    "\n",
    "# Mark holdout period\n",
    "if HAS_HOLDOUT:\n",
    "    holdout_start = df_holdout.index[0]\n",
    "    ax3.axvline(holdout_start, color='red', linestyle='--', linewidth=2, label='Holdout start')\n",
    "    ax3.legend()\n",
    "\n",
    "ax3.set_xlabel('Time')\n",
    "ax3.set_ylabel('Gas Price (gwei)')\n",
    "ax3.set_title(f'Recent Gas Prices (last {sample_size} samples)')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Feature importance (top 10)\n",
    "ax4 = axes[1, 1]\n",
    "if FEATURE_IMPORTANCE and any(v != list(FEATURE_IMPORTANCE.values())[0] for v in FEATURE_IMPORTANCE.values()):\n",
    "    # Non-uniform importance\n",
    "    sorted_imp = sorted(FEATURE_IMPORTANCE.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    features_plot = [f[0][:20] for f in sorted_imp]\n",
    "    importances = [f[1] for f in sorted_imp]\n",
    "    \n",
    "    y_pos = np.arange(len(features_plot))\n",
    "    ax4.barh(y_pos, importances, color='teal')\n",
    "    ax4.set_yticks(y_pos)\n",
    "    ax4.set_yticklabels(features_plot)\n",
    "    ax4.invert_yaxis()\n",
    "    ax4.set_xlabel('Importance')\n",
    "    ax4.set_title('Top 10 Feature Importance (Permutation)')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Feature importance uniform\\n(Huber model)', ha='center', va='center', fontsize=12)\n",
    "    ax4.set_title('Feature Importance')\n",
    "    ax4.set_xlim(0, 1)\n",
    "    ax4.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('saved_models/training_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Saved training_results.png\")\n",
    "\n",
    "# === ADDITIONAL: Distribution shift visualization ===\n",
    "if HAS_HOLDOUT and DISTRIBUTION_SHIFT_DETECTED:\n",
    "    fig2, axes2 = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    for i, horizon in enumerate(['1h', '4h']):\n",
    "        ax = axes2[i]\n",
    "        train_target = df_train_val[f'target_{horizon}'].dropna()\n",
    "        holdout_target = df_holdout[f'target_{horizon}'].dropna()\n",
    "        \n",
    "        ax.hist(train_target, bins=50, alpha=0.6, color='blue', label='Train', density=True)\n",
    "        ax.hist(holdout_target, bins=50, alpha=0.6, color='red', label='Holdout', density=True)\n",
    "        ax.set_xlabel(f'{horizon} Target (gwei)')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_title(f'{horizon} Target Distribution')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('saved_models/distribution_shift.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\u2713 Saved distribution_shift.png\")\n",
    ""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip file for download\n",
    "import shutil\n",
    "\n",
    "shutil.make_archive('gweizy_models', 'zip', 'saved_models')\n",
    "print(\"\\n\u2705 Created gweizy_models.zip\")\n",
    "print(\"\\nDownload this file and extract to: backend/models/saved_models/\")\n",
    "\n",
    "# Auto-download\n",
    "files.download('gweizy_models.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}