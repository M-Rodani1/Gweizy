{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udfaf Gweizy Model Training Notebook\n",
    "\n",
    "Train ML models for gas price prediction on Google Colab.\n",
    "\n",
    "**Steps:**\n",
    "1. Upload your `gas_data.db` file from `backend/gas_data.db`\n",
    "2. Run all cells (Runtime \u2192 Run all)\n",
    "3. Download trained models zip\n",
    "4. Extract and copy to `backend/models/saved_models/`\n",
    "5. Commit and push to deploy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\ufe0f\u20e3 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u2705 Dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy scikit-learn joblib lightgbm sqlalchemy python-dateutil tqdm -q\n",
    "print(\"\u2705 Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download database directly from Railway API",
    "import urllib.request",
    "import os",
    "import time",
    "",
    "if not os.path.exists('gas_data.db'):",
    "    print(\"\ud83d\udce5 Downloading database from Railway API...\")",
    "    print(\"   This may take a minute (file is ~53 MB with 234k records)\")",
    "    start_time = time.time()",
    "    try:",
    "        url = \"https://basegasfeesml-production.up.railway.app/api/database/download\"",
    "        ",
    "        # Show progress",
    "        def show_progress(block_num, block_size, total_size):",
    "            downloaded = block_num * block_size",
    "            percent = (downloaded / total_size * 100) if total_size > 0 else 0",
    "            mb_downloaded = downloaded / (1024 * 1024)",
    "            mb_total = total_size / (1024 * 1024) if total_size > 0 else 0",
    "            elapsed = time.time() - start_time",
    "            speed = mb_downloaded / elapsed if elapsed > 0 else 0",
    "            print(f\"\\r   Progress: {percent:.1f}% ({mb_downloaded:.1f}/{mb_total:.1f} MB) @ {speed:.1f} MB/s\", end='', flush=True)",
    "        ",
    "        urllib.request.urlretrieve(url, \"gas_data.db\", reporthook=show_progress)",
    "        print()  # New line after progress",
    "        ",
    "        file_size_mb = os.path.getsize('gas_data.db') / (1024 * 1024)",
    "        elapsed = time.time() - start_time",
    "        print(f\"\u2705 Database downloaded! Size: {file_size_mb:.2f} MB (took {elapsed:.1f}s)\")",
    "        ",
    "        # Verify it's a valid SQLite database",
    "        try:",
    "            import sqlite3",
    "            conn = sqlite3.connect('gas_data.db')",
    "            cursor = conn.cursor()",
    "            cursor.execute(\"SELECT COUNT(*) FROM gas_prices\")",
    "            count = cursor.fetchone()[0]",
    "            conn.close()",
    "            print(f\"\u2705 Verified: {count:,} records in database\")",
    "        except Exception as e:",
    "            print(f\"\u26a0\ufe0f  Warning: Could not verify database: {e}\")",
    "            ",
    "    except Exception as e:",
    "        print(f\"\\n\u274c Failed to download: {e}\")",
    "        print(\"   Please try:\")",
    "        print(\"   1. Check your internet connection\")",
    "        print(\"   2. Try again in a few moments\")",
    "        print(\"   3. Or upload manually using the cell below\")",
    "        raise",
    "else:",
    "    file_size_mb = os.path.getsize('gas_data.db') / (1024 * 1024)",
    "    print(f\"\u2705 Database already exists! Size: {file_size_mb:.2f} MB\")",
    "    ",
    "    # Show record count",
    "    try:",
    "        import sqlite3",
    "        conn = sqlite3.connect('gas_data.db')",
    "        cursor = conn.cursor()",
    "        cursor.execute(\"SELECT COUNT(*) FROM gas_prices\")",
    "        count = cursor.fetchone()[0]",
    "        conn.close()",
    "        print(f\"   Records: {count:,}\")",
    "    except:",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\ufe0f\u20e3 Upload Database\n",
    "\n",
    "Upload your `gas_data.db` file from `backend/gas_data.db`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Check if database already exists\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Check if database already exists\n",
    "if not os.path.exists('gas_data.db'):\n",
    "    print(\"\ud83d\udcc1 Please upload your gas_data.db file from Railway:\")\n",
    "    print(\"\\n\ud83d\udca1 How to get the database from Railway:\")\n",
    "    print(\"   1. Go to Railway dashboard \u2192 Your service\")\n",
    "    print(\"   2. Open the volume/file browser\")\n",
    "    print(\"   3. Navigate to backend/gas_data.db\")\n",
    "    print(\"   4. Download the file (should be several MB for 234k records)\")\n",
    "    print(\"   5. Upload it here:\")\n",
    "    uploaded = files.upload()\n",
    "    print(f\"\u2705 Uploaded: {list(uploaded.keys())}\")\n",
    "else:\n",
    "    print(\"\u2705 Database already exists!\")\n",
    "\n",
    "# Check file size\n",
    "if os.path.exists('gas_data.db'):\n",
    "    file_size_mb = os.path.getsize('gas_data.db') / (1024 * 1024)\n",
    "    print(f\"\\n\ud83d\udcca Database file size: {file_size_mb:.2f} MB\")\n",
    "    if file_size_mb < 1:\n",
    "        print(\"\u26a0\ufe0f WARNING: File is very small (<1 MB)\")\n",
    "        print(\"   This suggests the database might be empty or outdated\")\n",
    "        print(\"   Expected size: ~5-20 MB for 234,000 records\")\n",
    "    elif file_size_mb < 5:\n",
    "        print(\"\u26a0\ufe0f File size is smaller than expected\")\n",
    "        print(\"   Expected: ~5-20 MB for 234,000 records\")\n",
    "        print(\"   You might have an older database file\")\n",
    "    else:\n",
    "        print(\"\u2705 File size looks reasonable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\ufe0f\u20e3 Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "start_time = time.time()\n",
    "def log(msg):\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"[{elapsed:6.1f}s] {msg}\")\n",
    "\n",
    "# Connect to database\n",
    "engine = create_engine('sqlite:///gas_data.db')\n",
    "\n",
    "# Load data (note: column is 'current_gas' not 'gas_price')\n",
    "query = \"\"\"\n",
    "SELECT timestamp, current_gas, block_number, base_fee, priority_fee\n",
    "FROM gas_prices\n",
    "ORDER BY timestamp DESC\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, engine)\n",
    "# Rename to gas_price for consistency with feature engineering code\n",
    "df = df.rename(columns={'current_gas': 'gas_price'})\n",
    "\n",
    "log(f\"\ud83d\udcca Loaded {len(df):,} records\")\n",
    "log(f\"\ud83d\udcc5 Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "log(f\"\u26fd Gas price range: {df['gas_price'].min():.6f} to {df['gas_price'].max():.6f} gwei\")\n",
    "\n",
    "# Check database size\n",
    "if len(df) < 1000:\n",
    "    log(f\"\u26a0\ufe0f WARNING: Only {len(df):,} records loaded!\")\n",
    "    log(f\"   Expected ~234,000 records from Railway\")\n",
    "    log(f\"   \ud83d\udca1 This suggests:\")\n",
    "    log(f\"      - Wrong database file uploaded\")\n",
    "    log(f\"      - Database file is outdated\")\n",
    "    log(f\"      - Database file path is incorrect\")\n",
    "    log(f\"   \ud83d\udca1 To fix:\")\n",
    "    log(f\"      1. Download the latest gas_data.db from Railway\")\n",
    "    log(f\"      2. Make sure you're uploading from backend/gas_data.db\")\n",
    "    log(f\"      3. Check file size - should be several MB for 234k records\")\n",
    "else:\n",
    "    log(f\"\u2705 Dataset size looks good: {len(df):,} records\")\n",
    "\n",
    "# Show sample\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\ufe0f\u20e3 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"\ud83d\udd27 Starting feature engineering...\")\n",
    "\n",
    "# Helper function for smart NaN handling\n",
    "def smart_fill(series, method='ffill'):\n",
    "    \"\"\"Fill NaN with forward fill, then backward fill, then mean\"\"\"\n",
    "    filled = series.copy()\n",
    "    if method == 'ffill':\n",
    "        filled = filled.ffill().bfill()\n",
    "    else:\n",
    "        filled = filled.bfill().ffill()\n",
    "    if filled.isna().any():\n",
    "        filled = filled.fillna(series.mean() if series.notna().any() else 0)\n",
    "    return filled\n",
    "\n",
    "# Sort by timestamp\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# Sample if too large (use all data on Colab - we have resources!)\n",
    "MAX_RECORDS = 100000  # Can handle more on Colab\n",
    "if len(df) > MAX_RECORDS:\n",
    "    log(f\"\u26a0\ufe0f Sampling {MAX_RECORDS:,} from {len(df):,} records\")\n",
    "    recent = df.tail(MAX_RECORDS // 5)\n",
    "    older = df.head(len(df) - MAX_RECORDS // 5).sample(MAX_RECORDS - len(recent), random_state=42)\n",
    "    df = pd.concat([older, recent]).sort_values('timestamp').reset_index(drop=True)\n",
    "    log(f\"\u2705 Using {len(df):,} records\")\n",
    "\n",
    "# Outlier capping\n",
    "Q1, Q3 = df['gas_price'].quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "lower, upper = Q1 - 3*IQR, Q3 + 3*IQR\n",
    "outliers = ((df['gas_price'] < lower) | (df['gas_price'] > upper)).sum()\n",
    "log(f\"\u26a0\ufe0f Capping {outliers:,} outliers ({outliers/len(df)*100:.1f}%)\")\n",
    "df['gas_price'] = df['gas_price'].clip(lower, upper)\n",
    "\n",
    "# ===================================================================\n",
    "# 1. TIME FEATURES (no NaN)\n",
    "# ===================================================================\n",
    "log(\"   [1/9] Adding time features...\")\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['day_of_month'] = df['timestamp'].dt.day\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "# ===================================================================\n",
    "# 2. BASE FEE FEATURES (use base_fee and priority_fee)\n",
    "# ===================================================================\n",
    "log(\"   [2/9] Adding base fee and priority fee features...\")\n",
    "# Direct features\n",
    "if 'base_fee' in df.columns and 'priority_fee' in df.columns:\n",
    "    # Derived fee features\n",
    "    df['total_fee'] = df['base_fee'] + df['priority_fee']\n",
    "    df['base_fee_ratio'] = df['base_fee'] / (df['gas_price'] + 1e-8)\n",
    "    df['priority_fee_ratio'] = df['priority_fee'] / (df['gas_price'] + 1e-8)\n",
    "    df['base_fee_pct'] = (df['base_fee'] / (df['total_fee'] + 1e-8)) * 100\n",
    "    \n",
    "    # Rolling statistics for fees\n",
    "    for window in [6, 12, 24]:\n",
    "        df[f'base_fee_ma_{window}'] = df['base_fee'].rolling(window, min_periods=1).mean()\n",
    "        df[f'priority_fee_ma_{window}'] = df['priority_fee'].rolling(window, min_periods=1).mean()\n",
    "        df[f'total_fee_ma_{window}'] = df['total_fee'].rolling(window, min_periods=1).mean()\n",
    "else:\n",
    "    log(\"   \u26a0\ufe0f base_fee or priority_fee not found, skipping fee features\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3. LAG FEATURES (with smart NaN handling)\n",
    "# ===================================================================\n",
    "log(\"   [3/9] Adding lag features...\")\n",
    "for lag in [1, 3, 6, 12, 24]:\n",
    "    df[f'gas_lag_{lag}'] = smart_fill(df['gas_price'].shift(lag))\n",
    "\n",
    "# ===================================================================\n",
    "# 4. ROLLING STATISTICS (with min_periods=1)\n",
    "# ===================================================================\n",
    "log(\"   [4/9] Adding rolling statistics...\")\n",
    "rolling_features = []\n",
    "for window in [6, 12, 24, 48]:\n",
    "    rolling_features.append(pd.DataFrame({\n",
    "        f'gas_ma_{window}': df['gas_price'].rolling(window, min_periods=1).mean(),\n",
    "        f'gas_std_{window}': df['gas_price'].rolling(window, min_periods=1).std().fillna(0),\n",
    "        f'gas_min_{window}': df['gas_price'].rolling(window, min_periods=1).min(),\n",
    "        f'gas_max_{window}': df['gas_price'].rolling(window, min_periods=1).max(),\n",
    "        f'gas_range_{window}': df['gas_price'].rolling(window, min_periods=1).max() - \n",
    "                              df['gas_price'].rolling(window, min_periods=1).min()\n",
    "    }))\n",
    "if rolling_features:\n",
    "    df = pd.concat([df] + rolling_features, axis=1)\n",
    "\n",
    "# ===================================================================\n",
    "# 5. PRICE CHANGE & DERIVED FEATURES\n",
    "# ===================================================================\n",
    "log(\"   [5/9] Adding price change features...\")\n",
    "for period in [1, 6, 12, 24]:\n",
    "    df[f'gas_pct_change_{period}'] = (df['gas_price'].pct_change(period) * 100).fillna(0)\n",
    "    df[f'gas_diff_{period}'] = (df['gas_price'].diff(period)).fillna(0)\n",
    "\n",
    "# Volatility (coefficient of variation)\n",
    "for window in [6, 12, 24]:\n",
    "    mean = df['gas_price'].rolling(window, min_periods=1).mean()\n",
    "    std = df['gas_price'].rolling(window, min_periods=1).std()\n",
    "    df[f'volatility_{window}'] = (std / (mean + 1e-8)).fillna(0)\n",
    "\n",
    "# ===================================================================\n",
    "# 6. STATISTICAL FEATURES (percentiles, skewness, kurtosis)\n",
    "# ===================================================================\n",
    "log(\"   [6/9] Adding statistical features...\")\n",
    "stat_features = []\n",
    "for window in [12, 24]:\n",
    "    stat_features.append(pd.DataFrame({\n",
    "        f'q25_{window}': df['gas_price'].rolling(window, min_periods=1).quantile(0.25),\n",
    "        f'q50_{window}': df['gas_price'].rolling(window, min_periods=1).quantile(0.50),\n",
    "        f'q75_{window}': df['gas_price'].rolling(window, min_periods=1).quantile(0.75),\n",
    "        f'skew_{window}': df['gas_price'].rolling(window, min_periods=1).skew().fillna(0),\n",
    "        f'kurt_{window}': df['gas_price'].rolling(window, min_periods=1).kurt().fillna(0)\n",
    "    }))\n",
    "    # Distance from moving average\n",
    "    ma = df['gas_price'].rolling(window, min_periods=1).mean()\n",
    "    stat_features.append(pd.DataFrame({\n",
    "        f'dist_from_ma_{window}': (df['gas_price'] - ma).fillna(0),\n",
    "        f'dist_from_ma_pct_{window}': ((df['gas_price'] - ma) / (ma + 1e-8) * 100).fillna(0)\n",
    "    }))\n",
    "if stat_features:\n",
    "    df = pd.concat([df] + stat_features, axis=1)\n",
    "\n",
    "# ===================================================================\n",
    "# 7. MOMENTUM INDICATORS (RSI, MACD, trend strength)\n",
    "# ===================================================================\n",
    "log(\"   [7/9] Adding momentum indicators...\")\n",
    "\n",
    "# RSI (Relative Strength Index)\n",
    "def calculate_rsi(prices, period=14):\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "    rs = gain / (loss.replace(0, np.nan) + 1e-8)\n",
    "    return (100 - (100 / (1 + rs))).fillna(50)\n",
    "\n",
    "df['rsi_12'] = calculate_rsi(df['gas_price'], 12)\n",
    "df['rsi_24'] = calculate_rsi(df['gas_price'], 24)\n",
    "\n",
    "# MACD-like features (EMA differences)\n",
    "df['ema_6'] = df['gas_price'].ewm(span=6, adjust=False).mean()\n",
    "df['ema_12'] = df['gas_price'].ewm(span=12, adjust=False).mean()\n",
    "df['ema_24'] = df['gas_price'].ewm(span=24, adjust=False).mean()\n",
    "df['macd_6_12'] = (df['ema_6'] - df['ema_12']).fillna(0)\n",
    "df['macd_12_24'] = (df['ema_12'] - df['ema_24']).fillna(0)\n",
    "\n",
    "# Trend strength (linear regression slope)\n",
    "for window in [12, 24]:\n",
    "    def calc_trend(x):\n",
    "        if len(x) < 2:\n",
    "            return 0\n",
    "        try:\n",
    "            return np.polyfit(np.arange(len(x)), x, 1)[0]\n",
    "        except:\n",
    "            return 0\n",
    "    df[f'trend_strength_{window}'] = df['gas_price'].rolling(window, min_periods=2).apply(calc_trend, raw=True).fillna(0)\n",
    "\n",
    "# Momentum\n",
    "for period in [6, 12, 24]:\n",
    "    df[f'momentum_{period}'] = (df['gas_price'] - df['gas_price'].shift(period)).fillna(0)\n",
    "\n",
    "# ===================================================================\n",
    "# 8. INTERACTION FEATURES\n",
    "# ===================================================================\n",
    "log(\"   [8/9] Adding interaction features...\")\n",
    "# Time \u00d7 Price interactions\n",
    "df['hour_x_gas_price'] = df['hour'] * df['gas_price']\n",
    "df['weekend_x_gas_price'] = df['is_weekend'] * df['gas_price']\n",
    "\n",
    "# Business hours indicators\n",
    "df['is_business_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 17) & (df['is_weekend'] == 0)).astype(int)\n",
    "df['is_peak_hours'] = ((df['hour'] >= 14) & (df['hour'] <= 18)).astype(int)\n",
    "df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)\n",
    "\n",
    "# Fee \u00d7 Price interactions (if fees available)\n",
    "if 'base_fee' in df.columns and 'priority_fee' in df.columns:\n",
    "    df['base_fee_x_gas_price'] = df['base_fee'] * df['gas_price']\n",
    "    df['priority_fee_x_gas_price'] = df['priority_fee'] * df['gas_price']\n",
    "    df['total_fee_x_gas_price'] = df['total_fee'] * df['gas_price']\n",
    "\n",
    "# ===================================================================\n",
    "# 9. FINAL NaN CLEANUP (only critical columns)\n",
    "# ===================================================================\n",
    "log(\"   [9/9] Final cleanup...\")\n",
    "initial_len = len(df)\n",
    "critical_cols = ['gas_price']\n",
    "if 'base_fee' in df.columns:\n",
    "    critical_cols.append('base_fee')\n",
    "if 'priority_fee' in df.columns:\n",
    "    critical_cols.append('priority_fee')\n",
    "\n",
    "df = df.dropna(subset=critical_cols)\n",
    "\n",
    "# Fill any remaining NaN in non-critical features with 0 or median\n",
    "for col in df.columns:\n",
    "    if col not in critical_cols + ['timestamp', 'block_number']:\n",
    "        if df[col].dtype in [np.float64, np.float32, np.int64, np.int32]:\n",
    "            if df[col].isna().any():\n",
    "                df[col] = df[col].fillna(df[col].median() if df[col].notna().any() else 0)\n",
    "\n",
    "log(f\"\u2705 Features created: {len(df):,} samples (dropped {initial_len - len(df):,} rows with missing critical data)\")\n",
    "log(f\"   Total features: {len(df.columns)}\")\n",
    "print(f\"\\n\ud83d\udcca Feature columns ({len(df.columns)}): {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# DIAGNOSTIC: NaN Analysis\n",
    "# ===================================================================\n",
    "log(\"\ud83d\udd0d Analyzing NaN values in features...\")\n",
    "\n",
    "nan_counts = df.isna().sum()\n",
    "nan_pct = (nan_counts / len(df) * 100).round(2)\n",
    "\n",
    "# Summary\n",
    "total_nan_features = (nan_counts > 0).sum()\n",
    "log(f\"   Features with NaN: {total_nan_features}/{len(df.columns)}\")\n",
    "log(f\"   Total NaN values: {nan_counts.sum():,}\")\n",
    "\n",
    "# Show features with NaN\n",
    "if total_nan_features > 0:\n",
    "    log(\"\\n   \u26a0\ufe0f Features with NaN values:\")\n",
    "    nan_features = nan_counts[nan_counts > 0].sort_values(ascending=False)\n",
    "    for feat, count in nan_features.items():\n",
    "        pct = nan_pct[feat]\n",
    "        log(f\"      \u2022 {feat}: {count:,} NaN ({pct:.1f}%)\")\n",
    "    \n",
    "    # High NaN features (>10%)\n",
    "    high_nan = nan_pct[nan_pct > 10]\n",
    "    if len(high_nan) > 0:\n",
    "        log(f\"\\n   \u26a0\ufe0f Features with >10% NaN ({len(high_nan)}):\")\n",
    "        for feat, pct in high_nan.sort_values(ascending=False).items():\n",
    "            log(f\"      \u2022 {feat}: {pct:.1f}%\")\n",
    "else:\n",
    "    log(\"   \u2705 No NaN values found in features!\")\n",
    "\n",
    "# Data preservation stats\n",
    "log(f\"\\n   \ud83d\udcca Data Preservation:\")\n",
    "log(f\"      \u2022 Total rows: {len(df):,}\")\n",
    "log(f\"      \u2022 Rows with all features: {(~df.isna().any(axis=1)).sum():,}\")\n",
    "log(f\"      \u2022 Rows with any NaN: {df.isna().any(axis=1).sum():,}\")\n",
    "log(f\"      \u2022 Data completeness: {(~df.isna().any(axis=1)).sum() / len(df) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\ufe0f\u20e3 Create Targets & Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will run AFTER target creation (see cell below)\n",
    "# It's placed here as a placeholder - the actual diagnostic runs after targets are created\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"\ud83c\udfaf Creating prediction targets...\")\n",
    "\n",
    "# Estimate steps per hour from data\n",
    "# Ensure timestamp is datetime\n",
    "if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "time_diffs = df['timestamp'].diff().dropna()\n",
    "if len(time_diffs) == 0:\n",
    "    log(\"\u26a0\ufe0f No time differences found, using default: 1 step per hour\")\n",
    "    steps_per_hour = 1\n",
    "else:\n",
    "    median_interval = time_diffs.median()\n",
    "    if pd.isna(median_interval):\n",
    "        log(\"\u26a0\ufe0f Could not calculate median interval, using default: 1 step per hour\")\n",
    "        steps_per_hour = 1\n",
    "    else:\n",
    "        median_interval_min = median_interval.total_seconds() / 60  # minutes\n",
    "        if median_interval_min <= 0 or not np.isfinite(median_interval_min):\n",
    "            log(\"\u26a0\ufe0f Invalid interval, using default: 1 step per hour\")\n",
    "            steps_per_hour = 1\n",
    "        else:\n",
    "            steps_per_hour = max(1, int(60 / median_interval_min))\n",
    "            log(f\"   Detected {steps_per_hour} steps per hour (interval: {median_interval_min:.1f} min)\")\n",
    "\n",
    "# Future price targets - adjust based on available data\n",
    "max_steps = len(df) - 10  # Leave at least 10 rows for training\n",
    "log(f\"   Available data: {len(df):,} rows, max shift: {max_steps} steps\")\n",
    "\n",
    "horizons = {\n",
    "    '1h': min(steps_per_hour * 1, max_steps),\n",
    "    '4h': min(steps_per_hour * 4, max_steps),\n",
    "    '24h': min(steps_per_hour * 24, max_steps)\n",
    "}\n",
    "\n",
    "# If dataset is small, use smaller horizons\n",
    "if len(df) < 100:\n",
    "    log(f\"   \u26a0\ufe0f Small dataset detected, using reduced horizons\")\n",
    "    horizons = {\n",
    "        '1h': min(steps_per_hour * 1, max(1, max_steps // 4)),\n",
    "        '4h': min(steps_per_hour * 2, max(1, max_steps // 2)),\n",
    "        '24h': min(steps_per_hour * 6, max_steps)\n",
    "    }\n",
    "\n",
    "targets = {}\n",
    "for name, steps in horizons.items():\n",
    "    if steps <= 0:\n",
    "        log(f\"   \u26a0\ufe0f SKIPPING {name}: steps={steps} (too small)\")\n",
    "        continue\n",
    "    \n",
    "    future_price = df['gas_price'].shift(-steps)\n",
    "    pct_change = ((future_price - df['gas_price']) / (df['gas_price'] + 1e-8)) * 100\n",
    "    targets[name] = {\n",
    "        'pct_change': pct_change,\n",
    "        'original': future_price,\n",
    "        'current': df['gas_price'].copy()\n",
    "    }\n",
    "    valid = (~pct_change.isna()).sum()\n",
    "    log(f\"   {name}: {valid:,} valid targets, steps={steps}\")\n",
    "    \n",
    "    # Diagnostic info\n",
    "    if valid == 0:\n",
    "        log(f\"      \u26a0\ufe0f WARNING: No valid targets for {name}!\")\n",
    "        log(f\"      Total rows: {len(df):,}, Shift steps: {steps}\")\n",
    "        log(f\"      Future price NaN: {(future_price.isna()).sum():,}\")\n",
    "        log(f\"      Current price NaN: {(df['gas_price'].isna()).sum():,}\")\n",
    "\n",
    "# Select feature columns\n",
    "exclude_cols = ['timestamp', 'block_number']\n",
    "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "X = df[feature_cols].copy()\n",
    "log(f\"\ud83d\udcca Feature matrix: {X.shape[0]:,} samples, {X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\ufe0f\u20e3 Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('trained_models', exist_ok=True)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for horizon in ['1h', '4h', '24h']:\n",
    "    log(f\"\\n{'='*60}\")\n",
    "    log(f\"\ud83c\udf32 Training model for {horizon} horizon\")\n",
    "    log(f\"{'='*60}\")\n",
    "    \n",
    "    # Check if this horizon was created\n",
    "    if horizon not in targets:\n",
    "        log(f\"   \u26a0\ufe0f SKIPPING: {horizon} horizon not available (likely skipped during target creation)\")\n",
    "        continue\n",
    "    \n",
    "    y_pct = targets[horizon]['pct_change']\n",
    "    y_orig = targets[horizon]['original']\n",
    "    current_prices = targets[horizon]['current']\n",
    "    \n",
    "    # Remove NaN\n",
    "    valid_idx = ~(X.isna().any(axis=1) | y_pct.isna() | y_orig.isna())\n",
    "    X_clean = X[valid_idx]\n",
    "    y_pct_clean = y_pct[valid_idx]\n",
    "    y_orig_clean = y_orig[valid_idx]\n",
    "    current_clean = current_prices[valid_idx]\n",
    "    \n",
    "    log(f\"   Valid samples: {len(X_clean):,}\")\n",
    "    \n",
    "    # Check if we have enough data - MUST check before proceeding\n",
    "    # Reduced minimum for smaller datasets\n",
    "    MIN_SAMPLES = 50  # Minimum samples needed for training (reduced from 100)\n",
    "    if len(X_clean) == 0:\n",
    "        log(f\"   \u26a0\ufe0f SKIPPING: Zero valid samples for {horizon} horizon\")\n",
    "        log(f\"   \ud83d\udca1 This means all rows have NaN in features or targets\")\n",
    "        log(f\"   \ud83d\udca1 Check the diagnostic messages above for details\")\n",
    "        continue\n",
    "    elif len(X_clean) < MIN_SAMPLES:\n",
    "        log(f\"   \u26a0\ufe0f SKIPPING: Only {len(X_clean):,} valid samples (need at least {MIN_SAMPLES})\")\n",
    "        log(f\"   \ud83d\udca1 This might be because:\")\n",
    "        log(f\"      - Not enough historical data for {horizon} horizon\")\n",
    "        log(f\"      - Too many NaN values after feature engineering\")\n",
    "        log(f\"      - Data gaps in timestamp sequence\")\n",
    "        continue\n",
    "    \n",
    "    # Train/test split (80/20, temporal)\n",
    "    split_idx = int(len(X_clean) * 0.8)\n",
    "    X_train, X_test = X_clean.iloc[:split_idx], X_clean.iloc[split_idx:]\n",
    "    y_train, y_test = y_pct_clean.iloc[:split_idx], y_pct_clean.iloc[split_idx:]\n",
    "    y_orig_test = y_orig_clean.iloc[split_idx:]\n",
    "    current_test = current_clean.iloc[split_idx:]\n",
    "    \n",
    "    log(f\"   Train: {len(X_train):,}, Test: {len(X_test):,}\")\n",
    "    \n",
    "    # Additional check after split - MUST check before scaling\n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        log(f\"   \u26a0\ufe0f SKIPPING: Insufficient samples after split (train={len(X_train):,}, test={len(X_test):,})\")\n",
    "        log(f\"   \ud83d\udca1 Need at least 1 sample in both train and test sets\")\n",
    "        continue\n",
    "    \n",
    "    # Final safety check before scaling\n",
    "    if X_train.empty or X_test.empty:\n",
    "        log(f\"   \u26a0\ufe0f SKIPPING: Empty dataframes detected\")\n",
    "        continue\n",
    "    \n",
    "    # Scale features\n",
    "    try:\n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    except ValueError as e:\n",
    "        log(f\"   \u26a0\ufe0f SKIPPING: Error during scaling: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Train RandomForest\n",
    "    log(f\"   Training RandomForest (this may take 1-2 min)...\")\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_pct = model.predict(X_test_scaled)\n",
    "    y_pred_orig = current_test.values * (1 + y_pred_pct / 100)\n",
    "    \n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(y_orig_test, y_pred_orig)\n",
    "    rmse = np.sqrt(mean_squared_error(y_orig_test, y_pred_orig))\n",
    "    r2 = r2_score(y_orig_test, y_pred_orig)\n",
    "    mape = np.mean(np.abs((y_orig_test - y_pred_orig) / (y_orig_test + 1e-8))) * 100\n",
    "    \n",
    "    # Directional accuracy\n",
    "    y_diff_actual = np.diff(y_orig_test.values)\n",
    "    y_diff_pred = np.diff(y_pred_orig)\n",
    "    dir_acc = np.mean(np.sign(y_diff_actual) == np.sign(y_diff_pred))\n",
    "    \n",
    "    log(f\"   \u2705 R\u00b2: {r2:.4f}\")\n",
    "    log(f\"   \u2705 MAE: {mae:.6f} gwei\")\n",
    "    log(f\"   \u2705 MAPE: {mape:.2f}%\")\n",
    "    log(f\"   \u2705 Directional Accuracy: {dir_acc*100:.1f}%\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importances = model.feature_importances_\n",
    "    top_features = sorted(zip(feature_cols, importances), key=lambda x: x[1], reverse=True)[:5]\n",
    "    log(f\"   Top features: {[f[0] for f in top_features]}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_data = {\n",
    "        'model': model,\n",
    "        'model_name': 'RandomForest_PctChange',\n",
    "        'feature_scaler': scaler,\n",
    "        'feature_names': feature_cols,\n",
    "        'predicts_percentage_change': True,\n",
    "        'uses_log_scale': False,\n",
    "        'metrics': {'mae': mae, 'rmse': rmse, 'r2': r2, 'mape': mape, 'directional_accuracy': dir_acc},\n",
    "        'trained_at': datetime.now().isoformat(),\n",
    "        'training_samples': len(X_train),\n",
    "        'feature_importances': dict(zip(feature_cols, importances))\n",
    "    }\n",
    "    \n",
    "    model_path = f'trained_models/model_{horizon}.pkl'\n",
    "    joblib.dump(model_data, model_path)\n",
    "    log(f\"   \ud83d\udcbe Saved to {model_path}\")\n",
    "    \n",
    "    scaler_path = f'trained_models/scaler_{horizon}.pkl'\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    log(f\"   \ud83d\udcbe Saved scaler to {scaler_path}\")\n",
    "    \n",
    "    results[horizon] = model_data['metrics']\n",
    "\n",
    "log(f\"\\n{'='*60}\")\n",
    "log(\"\ud83c\udf89 TRAINING COMPLETE!\")\n",
    "log(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7\ufe0f\u20e3 Summary & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "print(\"\ud83d\udcca Model Performance Summary:\")\n",
    "print(\"=\"*50)\n",
    "if len(results) == 0:\n",
    "    print(\"\\n\u26a0\ufe0f No models were successfully trained!\")\n",
    "    print(\"   This usually means:\")\n",
    "    print(\"   - Not enough historical data\")\n",
    "    print(\"   - Data gaps preventing target creation\")\n",
    "    print(\"   - Too many NaN values after feature engineering\")\n",
    "    print(\"\\n   \ud83d\udca1 Try:\")\n",
    "    print(\"   - Uploading a database with more historical data\")\n",
    "    print(\"   - Checking that timestamps are properly formatted\")\n",
    "    print(\"   - Ensuring gas_price values are valid\")\n",
    "else:\n",
    "    for horizon, metrics in results.items():\n",
    "        print(f\"\\n{horizon}:\")\n",
    "        print(f\"  R\u00b2: {metrics['r2']:.4f}\")\n",
    "        print(f\"  MAE: {metrics['mae']:.6f} gwei\")\n",
    "        print(f\"  MAPE: {metrics['mape']:.2f}%\")\n",
    "        print(f\"  Directional Accuracy: {metrics['directional_accuracy']*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "if os.path.exists('trained_models') and len(os.listdir('trained_models')) > 0:\n",
    "    print(\"\ud83d\udcc1 Generated files:\")\n",
    "    for f in os.listdir('trained_models'):\n",
    "        size = os.path.getsize(f'trained_models/{f}') / 1024 / 1024\n",
    "        print(f\"  \u2022 {f} ({size:.1f} MB)\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No model files generated\")\n",
    "\n",
    "# Create zip if models exist\n",
    "if os.path.exists('trained_models') and len(os.listdir('trained_models')) > 0:\n",
    "    shutil.make_archive('gweizy_models', 'zip', 'trained_models')\n",
    "    print(\"\\n\ud83d\udce6 Created gweizy_models.zip\")\n",
    "    \n",
    "    # Download\n",
    "    files.download('gweizy_models.zip')\n",
    "    print(\"\\n\u2705 Download started!\")\n",
    "    print(\"\\n\ud83d\udccb Next steps:\")\n",
    "    print(\"1. Extract gweizy_models.zip\")\n",
    "    print(\"2. Copy model_*.pkl to backend/models/saved_models/\")\n",
    "    print(\"3. git add, commit, push\")\n",
    "    print(\"4. Railway will auto-deploy with new models!\")\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f Cannot create zip - no models were trained\")\n",
    "    print(\"   Please check your data and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}