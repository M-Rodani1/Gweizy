{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Gweizy Model Training Notebook\n",
        "\n",
        "Train ML models for gas price prediction on Google Colab.\n",
        "\n",
        "**Steps:**\n",
        "1. Upload your `gas_data.db` file from `backend/gas_data.db`\n",
        "2. Run all cells (Runtime ‚Üí Run all)\n",
        "3. Download trained models zip\n",
        "4. Extract and copy to `backend/models/saved_models/`\n",
        "5. Commit and push to deploy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install pandas numpy scikit-learn joblib lightgbm sqlalchemy python-dateutil tqdm -q\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Upload Database\n",
        "\n",
        "Upload your `gas_data.db` file from `backend/gas_data.db`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Check if database already exists\n",
        "if not os.path.exists('gas_data.db'):\n",
        "    print(\"üìÅ Please upload your gas_data.db file:\")\n",
        "    uploaded = files.upload()\n",
        "    print(f\"‚úÖ Uploaded: {list(uploaded.keys())}\")\n",
        "else:\n",
        "    print(\"‚úÖ Database already exists!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Load and Inspect Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sqlalchemy import create_engine\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "start_time = time.time()\n",
        "def log(msg):\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"[{elapsed:6.1f}s] {msg}\")\n",
        "\n",
        "# Connect to database\n",
        "engine = create_engine('sqlite:///gas_data.db')\n",
        "\n",
        "# Load data (note: column is 'current_gas' not 'gas_price')\n",
        "query = \"\"\"\n",
        "SELECT timestamp, current_gas, block_number, base_fee, priority_fee\n",
        "FROM gas_prices\n",
        "ORDER BY timestamp DESC\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_sql(query, engine)\n",
        "# Rename to gas_price for consistency with feature engineering code\n",
        "df = df.rename(columns={'current_gas': 'gas_price'})\n",
        "\n",
        "log(f\"üìä Loaded {len(df):,} records\")\n",
        "log(f\"üìÖ Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
        "log(f\"‚õΩ Gas price range: {df['gas_price'].min():.6f} to {df['gas_price'].max():.6f} gwei\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log(\"üîß Starting feature engineering...\")\n",
        "\n",
        "# Sort by timestamp\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "# Sample if too large (use all data on Colab - we have resources!)\n",
        "MAX_RECORDS = 100000  # Can handle more on Colab\n",
        "if len(df) > MAX_RECORDS:\n",
        "    log(f\"‚ö†Ô∏è Sampling {MAX_RECORDS:,} from {len(df):,} records\")\n",
        "    recent = df.tail(MAX_RECORDS // 5)\n",
        "    older = df.head(len(df) - MAX_RECORDS // 5).sample(MAX_RECORDS - len(recent), random_state=42)\n",
        "    df = pd.concat([older, recent]).sort_values('timestamp').reset_index(drop=True)\n",
        "    log(f\"‚úÖ Using {len(df):,} records\")\n",
        "\n",
        "# Outlier capping\n",
        "Q1, Q3 = df['gas_price'].quantile([0.25, 0.75])\n",
        "IQR = Q3 - Q1\n",
        "lower, upper = Q1 - 3*IQR, Q3 + 3*IQR\n",
        "outliers = ((df['gas_price'] < lower) | (df['gas_price'] > upper)).sum()\n",
        "log(f\"‚ö†Ô∏è Capping {outliers:,} outliers ({outliers/len(df)*100:.1f}%)\")\n",
        "df['gas_price'] = df['gas_price'].clip(lower, upper)\n",
        "\n",
        "# Time features\n",
        "log(\"   Adding time features...\")\n",
        "df['hour'] = df['timestamp'].dt.hour\n",
        "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
        "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
        "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
        "df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
        "\n",
        "# Lag features\n",
        "log(\"   Adding lag features...\")\n",
        "for lag in [1, 2, 3, 6, 12, 24]:\n",
        "    df[f'gas_lag_{lag}'] = df['gas_price'].shift(lag)\n",
        "\n",
        "# Rolling statistics\n",
        "log(\"   Adding rolling statistics...\")\n",
        "for window in [6, 12, 24, 48]:\n",
        "    df[f'gas_ma_{window}'] = df['gas_price'].rolling(window).mean()\n",
        "    df[f'gas_std_{window}'] = df['gas_price'].rolling(window).std()\n",
        "    df[f'gas_min_{window}'] = df['gas_price'].rolling(window).min()\n",
        "    df[f'gas_max_{window}'] = df['gas_price'].rolling(window).max()\n",
        "\n",
        "# Price change features\n",
        "log(\"   Adding price change features...\")\n",
        "df['gas_pct_change_1'] = df['gas_price'].pct_change(1) * 100\n",
        "df['gas_pct_change_6'] = df['gas_price'].pct_change(6) * 100\n",
        "df['gas_pct_change_12'] = df['gas_price'].pct_change(12) * 100\n",
        "df['gas_pct_change_24'] = df['gas_price'].pct_change(24) * 100\n",
        "\n",
        "# Volatility\n",
        "df['volatility_6h'] = df['gas_price'].rolling(6).std() / df['gas_price'].rolling(6).mean()\n",
        "df['volatility_24h'] = df['gas_price'].rolling(24).std() / df['gas_price'].rolling(24).mean()\n",
        "\n",
        "# Momentum\n",
        "df['momentum_6'] = df['gas_price'] - df['gas_price'].shift(6)\n",
        "df['momentum_12'] = df['gas_price'] - df['gas_price'].shift(12)\n",
        "df['momentum_24'] = df['gas_price'] - df['gas_price'].shift(24)\n",
        "\n",
        "# EMA\n",
        "df['ema_6'] = df['gas_price'].ewm(span=6).mean()\n",
        "df['ema_12'] = df['gas_price'].ewm(span=12).mean()\n",
        "df['ema_24'] = df['gas_price'].ewm(span=24).mean()\n",
        "\n",
        "# Drop NaN rows\n",
        "initial_len = len(df)\n",
        "df = df.dropna()\n",
        "log(f\"‚úÖ Features created: {len(df):,} samples, {len(df.columns)} features\")\n",
        "print(f\"\\nüìä Feature columns: {list(df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Create Targets & Prepare Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log(\"üéØ Creating prediction targets...\")\n",
        "\n",
        "# Estimate steps per hour from data\n",
        "# Ensure timestamp is datetime\n",
        "if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "time_diffs = df['timestamp'].diff().dropna()\n",
        "if len(time_diffs) == 0:\n",
        "    log(\"‚ö†Ô∏è No time differences found, using default: 1 step per hour\")\n",
        "    steps_per_hour = 1\n",
        "else:\n",
        "    median_interval = time_diffs.median()\n",
        "    if pd.isna(median_interval):\n",
        "        log(\"‚ö†Ô∏è Could not calculate median interval, using default: 1 step per hour\")\n",
        "        steps_per_hour = 1\n",
        "    else:\n",
        "        median_interval_min = median_interval.total_seconds() / 60  # minutes\n",
        "        if median_interval_min <= 0 or not np.isfinite(median_interval_min):\n",
        "            log(\"‚ö†Ô∏è Invalid interval, using default: 1 step per hour\")\n",
        "            steps_per_hour = 1\n",
        "        else:\n",
        "            steps_per_hour = max(1, int(60 / median_interval_min))\n",
        "            log(f\"   Detected {steps_per_hour} steps per hour (interval: {median_interval_min:.1f} min)\")\n",
        "\n",
        "# Future price targets\n",
        "horizons = {\n",
        "    '1h': steps_per_hour * 1,\n",
        "    '4h': steps_per_hour * 4,\n",
        "    '24h': steps_per_hour * 24\n",
        "}\n",
        "\n",
        "targets = {}\n",
        "for name, steps in horizons.items():\n",
        "    future_price = df['gas_price'].shift(-steps)\n",
        "    pct_change = ((future_price - df['gas_price']) / (df['gas_price'] + 1e-8)) * 100\n",
        "    targets[name] = {\n",
        "        'pct_change': pct_change,\n",
        "        'original': future_price,\n",
        "        'current': df['gas_price'].copy()\n",
        "    }\n",
        "    valid = (~pct_change.isna()).sum()\n",
        "    log(f\"   {name}: {valid:,} valid targets, steps={steps}\")\n",
        "    \n",
        "    # Diagnostic info\n",
        "    if valid == 0:\n",
        "        log(f\"      ‚ö†Ô∏è WARNING: No valid targets for {name}!\")\n",
        "        log(f\"      Total rows: {len(df):,}, Shift steps: {steps}\")\n",
        "        log(f\"      Future price NaN: {(future_price.isna()).sum():,}\")\n",
        "        log(f\"      Current price NaN: {(df['gas_price'].isna()).sum():,}\")\n",
        "\n",
        "# Select feature columns\n",
        "exclude_cols = ['timestamp', 'block_number']\n",
        "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
        "X = df[feature_cols].copy()\n",
        "log(f\"üìä Feature matrix: {X.shape[0]:,} samples, {X.shape[1]} features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ Train Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs('trained_models', exist_ok=True)\n",
        "\n",
        "results = {}\n",
        "\n",
        "for horizon in ['1h', '4h', '24h']:\n",
        "    log(f\"\\n{'='*60}\")\n",
        "    log(f\"üå≤ Training model for {horizon} horizon\")\n",
        "    log(f\"{'='*60}\")\n",
        "    \n",
        "    y_pct = targets[horizon]['pct_change']\n",
        "    y_orig = targets[horizon]['original']\n",
        "    current_prices = targets[horizon]['current']\n",
        "    \n",
        "    # Remove NaN\n",
        "    valid_idx = ~(X.isna().any(axis=1) | y_pct.isna() | y_orig.isna())\n",
        "    X_clean = X[valid_idx]\n",
        "    y_pct_clean = y_pct[valid_idx]\n",
        "    y_orig_clean = y_orig[valid_idx]\n",
        "    current_clean = current_prices[valid_idx]\n",
        "    \n",
        "    log(f\"   Valid samples: {len(X_clean):,}\")\n",
        "    \n",
        "    # Check if we have enough data\n",
        "    MIN_SAMPLES = 100  # Minimum samples needed for training\n",
        "    if len(X_clean) < MIN_SAMPLES:\n",
        "        log(f\"   ‚ö†Ô∏è SKIPPING: Only {len(X_clean):,} valid samples (need at least {MIN_SAMPLES})\")\n",
        "        log(f\"   üí° This might be because:\")\n",
        "        log(f\"      - Not enough historical data for {horizon} horizon\")\n",
        "        log(f\"      - Too many NaN values after feature engineering\")\n",
        "        log(f\"      - Data gaps in timestamp sequence\")\n",
        "        continue\n",
        "    \n",
        "    # Train/test split (80/20, temporal)\n",
        "    split_idx = int(len(X_clean) * 0.8)\n",
        "    X_train, X_test = X_clean.iloc[:split_idx], X_clean.iloc[split_idx:]\n",
        "    y_train, y_test = y_pct_clean.iloc[:split_idx], y_pct_clean.iloc[split_idx:]\n",
        "    y_orig_test = y_orig_clean.iloc[split_idx:]\n",
        "    current_test = current_clean.iloc[split_idx:]\n",
        "    \n",
        "    log(f\"   Train: {len(X_train):,}, Test: {len(X_test):,}\")\n",
        "    \n",
        "    # Additional check after split\n",
        "    if len(X_train) == 0:\n",
        "        log(f\"   ‚ö†Ô∏è SKIPPING: No training samples after split\")\n",
        "        continue\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = RobustScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Train RandomForest\n",
        "    log(f\"   Training RandomForest (this may take 1-2 min)...\")\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=200,\n",
        "        max_depth=20,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Predict\n",
        "    y_pred_pct = model.predict(X_test_scaled)\n",
        "    y_pred_orig = current_test.values * (1 + y_pred_pct / 100)\n",
        "    \n",
        "    # Metrics\n",
        "    mae = mean_absolute_error(y_orig_test, y_pred_orig)\n",
        "    rmse = np.sqrt(mean_squared_error(y_orig_test, y_pred_orig))\n",
        "    r2 = r2_score(y_orig_test, y_pred_orig)\n",
        "    mape = np.mean(np.abs((y_orig_test - y_pred_orig) / (y_orig_test + 1e-8))) * 100\n",
        "    \n",
        "    # Directional accuracy\n",
        "    y_diff_actual = np.diff(y_orig_test.values)\n",
        "    y_diff_pred = np.diff(y_pred_orig)\n",
        "    dir_acc = np.mean(np.sign(y_diff_actual) == np.sign(y_diff_pred))\n",
        "    \n",
        "    log(f\"   ‚úÖ R¬≤: {r2:.4f}\")\n",
        "    log(f\"   ‚úÖ MAE: {mae:.6f} gwei\")\n",
        "    log(f\"   ‚úÖ MAPE: {mape:.2f}%\")\n",
        "    log(f\"   ‚úÖ Directional Accuracy: {dir_acc*100:.1f}%\")\n",
        "    \n",
        "    # Feature importance\n",
        "    importances = model.feature_importances_\n",
        "    top_features = sorted(zip(feature_cols, importances), key=lambda x: x[1], reverse=True)[:5]\n",
        "    log(f\"   Top features: {[f[0] for f in top_features]}\")\n",
        "    \n",
        "    # Save model\n",
        "    model_data = {\n",
        "        'model': model,\n",
        "        'model_name': 'RandomForest_PctChange',\n",
        "        'feature_scaler': scaler,\n",
        "        'feature_names': feature_cols,\n",
        "        'predicts_percentage_change': True,\n",
        "        'uses_log_scale': False,\n",
        "        'metrics': {'mae': mae, 'rmse': rmse, 'r2': r2, 'mape': mape, 'directional_accuracy': dir_acc},\n",
        "        'trained_at': datetime.now().isoformat(),\n",
        "        'training_samples': len(X_train),\n",
        "        'feature_importances': dict(zip(feature_cols, importances))\n",
        "    }\n",
        "    \n",
        "    model_path = f'trained_models/model_{horizon}.pkl'\n",
        "    joblib.dump(model_data, model_path)\n",
        "    log(f\"   üíæ Saved to {model_path}\")\n",
        "    \n",
        "    scaler_path = f'trained_models/scaler_{horizon}.pkl'\n",
        "    joblib.dump(scaler, scaler_path)\n",
        "    log(f\"   üíæ Saved scaler to {scaler_path}\")\n",
        "    \n",
        "    results[horizon] = model_data['metrics']\n",
        "\n",
        "log(f\"\\n{'='*60}\")\n",
        "log(\"üéâ TRAINING COMPLETE!\")\n",
        "log(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7Ô∏è‚É£ Summary & Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "print(\"üìä Model Performance Summary:\")\n",
        "print(\"=\"*50)\n",
        "if len(results) == 0:\n",
        "    print(\"\\n‚ö†Ô∏è No models were successfully trained!\")\n",
        "    print(\"   This usually means:\")\n",
        "    print(\"   - Not enough historical data\")\n",
        "    print(\"   - Data gaps preventing target creation\")\n",
        "    print(\"   - Too many NaN values after feature engineering\")\n",
        "    print(\"\\n   üí° Try:\")\n",
        "    print(\"   - Uploading a database with more historical data\")\n",
        "    print(\"   - Checking that timestamps are properly formatted\")\n",
        "    print(\"   - Ensuring gas_price values are valid\")\n",
        "else:\n",
        "    for horizon, metrics in results.items():\n",
        "        print(f\"\\n{horizon}:\")\n",
        "        print(f\"  R¬≤: {metrics['r2']:.4f}\")\n",
        "        print(f\"  MAE: {metrics['mae']:.6f} gwei\")\n",
        "        print(f\"  MAPE: {metrics['mape']:.2f}%\")\n",
        "        print(f\"  Directional Accuracy: {metrics['directional_accuracy']*100:.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "if os.path.exists('trained_models') and len(os.listdir('trained_models')) > 0:\n",
        "    print(\"üìÅ Generated files:\")\n",
        "    for f in os.listdir('trained_models'):\n",
        "        size = os.path.getsize(f'trained_models/{f}') / 1024 / 1024\n",
        "        print(f\"  ‚Ä¢ {f} ({size:.1f} MB)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No model files generated\")\n",
        "\n",
        "# Create zip if models exist\n",
        "if os.path.exists('trained_models') and len(os.listdir('trained_models')) > 0:\n",
        "    shutil.make_archive('gweizy_models', 'zip', 'trained_models')\n",
        "    print(\"\\nüì¶ Created gweizy_models.zip\")\n",
        "    \n",
        "    # Download\n",
        "    files.download('gweizy_models.zip')\n",
        "    print(\"\\n‚úÖ Download started!\")\n",
        "    print(\"\\nüìã Next steps:\")\n",
        "    print(\"1. Extract gweizy_models.zip\")\n",
        "    print(\"2. Copy model_*.pkl to backend/models/saved_models/\")\n",
        "    print(\"3. git add, commit, push\")\n",
        "    print(\"4. Railway will auto-deploy with new models!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Cannot create zip - no models were trained\")\n",
        "    print(\"   Please check your data and try again\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
